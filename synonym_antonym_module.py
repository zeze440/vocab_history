"""
ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ëª¨ë“ˆ (Synonym & Antonym Extractor)
ë§ˆì§€ë§‰ ë²„ì „ì— ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ëª¨ë“ˆ

ì‚¬ìš©ë²•:
1. ê¸°ì¡´ ì½”ë“œì— ì´ ëª¨ë“ˆì„ ì¶”ê°€
2. AdvancedVocabExtractor í´ë˜ìŠ¤ì— í†µí•©
3. ë‹¨ì–´ì¥ ìƒì„± ì‹œ ë™ì˜ì–´/ë°˜ì˜ì–´ ìë™ ì¶”ì¶œ
"""

import json
import os
import time
from typing import Dict, List, Any, Optional
import openai
import re
from collections import Counter
from nltk.corpus import wordnet
from nltk.stem import PorterStemmer


class SynonymAntonymExtractor:
    """ë…ë¦½ì ì¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸°"""

    def __init__(self, client, cache_file="synonym_antonym_cache.json", verbose=False):
        self.client = client
        self.cache_file = cache_file
        self.verbose = verbose
        self.cache = {}
        self.gpt_calls = 0
        self.max_gpt_calls = 500  # GPT í˜¸ì¶œ ì œí•œ

        # ğŸ”¥ í’ˆì‚¬ ë§¤í•‘ ì¶”ê°€
        self.pos_mapping = {
            "noun": "n",
            "n": "n",
            "NN": "n",
            "NNS": "n",
            "NNP": "n",
            "NNPS": "n",
            "verb": "v",
            "v": "v",
            "VB": "v",
            "VBD": "v",
            "VBG": "v",
            "VBN": "v",
            "VBP": "v",
            "VBZ": "v",
            "adjective": "a",
            "adj": "a",
            "a": "a",
            "JJ": "a",
            "JJR": "a",
            "JJS": "a",
            "adverb": "r",
            "adv": "r",
            "r": "r",
            "RB": "r",
            "RBR": "r",
            "RBS": "r",
        }
        # ê¸°ë³¸ ë‹¨ì–´ ì„¸íŠ¸ (ì œì™¸í•  ë‹¨ì–´ë“¤)
        self.basic_words = {
            "good",
            "bad",
            "big",
            "small",
            "make",
            "take",
            "get",
            "go",
            "come",
            "see",
            "hear",
            "feel",
            "know",
            "think",
            "say",
            "tell",
            "have",
            "be",
            "do",
            "can",
            "will",
            "would",
            "could",
            "should",
            "may",
            "might",
        }

        self._load_cache()

    def _load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    self.cache = json.load(f)
                print(f"âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ìºì‹œ ë¡œë“œ: {len(self.cache)}ê°œ í•­ëª©")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.cache = {}

    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        try:
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
            if self.verbose:
                print(f"âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ìºì‹œ ì €ì¥: {len(self.cache)}ê°œ í•­ëª©")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _normalize_pos(self, pos: str) -> str:
        """í’ˆì‚¬ë¥¼ WordNet í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
        if not pos:
            return ""
        pos_clean = pos.strip().lower()
        return self.pos_mapping.get(pos_clean, pos_clean)

    def _verify_pos_with_wordnet(self, word: str, target_pos: str) -> bool:
        """WordNetìœ¼ë¡œ ë‹¨ì–´ì˜ í’ˆì‚¬ ê²€ì¦"""
        try:
            target_pos_norm = self._normalize_pos(target_pos)
            if not target_pos_norm:
                return True  # í’ˆì‚¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ í†µê³¼

            synsets = wordnet.synsets(word, pos=target_pos_norm)
            return len(synsets) > 0
        except:
            return True  # ì˜¤ë¥˜ ì‹œ í†µê³¼

    def _filter_candidates_by_pos(
        self, candidates: List[str], target_pos: str
    ) -> List[str]:
        """í’ˆì‚¬ê°€ ì¼ì¹˜í•˜ëŠ” í›„ë³´ë“¤ë§Œ í•„í„°ë§"""
        if not candidates or not target_pos:
            return candidates

        filtered = []
        for candidate in candidates:
            if self._verify_pos_with_wordnet(candidate, target_pos):
                filtered.append(candidate)
            elif self.verbose:
                print(f"   âš ï¸ í’ˆì‚¬ ë¶ˆì¼ì¹˜ë¡œ ì œì™¸: {candidate} (ëª©í‘œ: {target_pos})")

        return filtered

    def extract_synonyms_antonyms(
        self, word: str, context: str = "", pos: str = "", meaning: str = ""
    ) -> Dict[str, Any]:
        """
        ë‹¨ì–´ì˜ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ

        Args:
            word: ë¶„ì„í•  ë‹¨ì–´
            context: ë¬¸ë§¥
            pos: í’ˆì‚¬
            meaning: í•œê¸€ ì˜ë¯¸

        Returns:
            ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """

        # ğŸ”¥ í’ˆì‚¬ ì •ê·œí™”
        pos_normalized = self._normalize_pos(pos)

        # ìºì‹œ í‚¤ì— í’ˆì‚¬ ì •ë³´ í¬í•¨
        cache_key = f"{word.lower()}:{context[:50]}:{pos_normalized}"

        # ì•ˆì „í•œ ê¸°ë³¸ê°’
        safe_default = {
            "synonyms": [],
            "antonyms": [],
            "contextual_meaning": meaning or "",
            "confidence": 0.0,
            "source": "default",
            "notes": "ê¸°ë³¸ê°’",
        }

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"{word.lower()}:{context[:50]}:{pos}"

        # ìºì‹œ í™•ì¸
        if cache_key in self.cache:
            cached_result = self.cache[cache_key]
            if isinstance(cached_result, dict):
                return cached_result
            else:
                return safe_default

        # GPT í˜¸ì¶œ ì œí•œ í™•ì¸
        if self.gpt_calls >= self.max_gpt_calls:
            if self.verbose:
                print(f"   âš ï¸ GPT í˜¸ì¶œ í•œë„ ë„ë‹¬, WordNet ì‚¬ìš©: {word}")
            result = self._get_wordnet_synonyms_antonyms(word)
            self.cache[cache_key] = result
            return result

        # GPT ê¸°ë°˜ ì¶”ì¶œ ì‹œë„
        try:
            result = self._extract_with_gpt(word, context, pos_normalized, meaning)
            # ğŸ”¥ ì—¬ê¸°ë¥¼ enhanced ë²„ì „ìœ¼ë¡œ ë³€ê²½
            validated_result = self._validate_gpt_result_enhanced(
                result, word, pos_normalized
            )
            validated_result["source"] = "gpt"
            self.cache[cache_key] = validated_result
            return validated_result
        except Exception as e:
            if self.verbose:
                print(f"   âŒ GPT ì¶”ì¶œ ì‹¤íŒ¨ ({word}): {e}")

            # ğŸ”¥ WordNet fallbackë„ enhanced ë²„ì „ìœ¼ë¡œ ë³€ê²½
            result = self._get_wordnet_synonyms_antonyms_enhanced(word, pos_normalized)
            result["notes"] = f"GPT ì‹¤íŒ¨, WordNet ì‚¬ìš©: {str(e)}"
            self.cache[cache_key] = result
            return result

    def _extract_with_gpt(
        self, word: str, context: str, pos: str, meaning: str
    ) -> Dict[str, Any]:
        """GPTë¥¼ í™œìš©í•œ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ"""

        # ğŸ”¥ í’ˆì‚¬ ì •ë³´ ê°•í™”
        pos_normalized = self._normalize_pos(pos)
        pos_description = {
            "n": "noun (ëª…ì‚¬)",
            "v": "verb (ë™ì‚¬)",
            "a": "adjective (í˜•ìš©ì‚¬)",
            "r": "adverb (ë¶€ì‚¬)",
        }.get(pos_normalized, f"{pos} (ê¸°íƒ€)")

        prompt = f"""
Analyze the English word "{word}" in the given context and provide PRECISE synonyms and antonyms.

Word: "{word}"
Context: "{context}"
Korean meaning: "{meaning}"
Part of speech: {pos_description}

ğŸ¯ CRITICAL FILTERING REQUIREMENTS:
1. ONLY provide words that have the SAME meaning as "{word}" in THIS specific context
2. Do NOT include words from different contexts or meanings of "{word}"
3. ONLY single words (no phrases, no hyphens, no underscores)
4. Maximum 3 synonyms and 2 antonyms
5. Ensure contextual relevance - each word must fit naturally in the given context

Example of BAD filtering:
- "bank" in financial context â†’ Do NOT include "shore, riverbank" (different meaning)
- "light" as adjective â†’ Do NOT include "illumination, lamp" (different part of speech)

Example of GOOD filtering:
- "significant" in academic context â†’ "important, substantial, considerable" âœ“
- "analyze" in research context â†’ "examine, investigate, study" âœ“

For "{word}" in context "{context}":
Provide ONLY words that can replace "{word}" in THIS specific sentence without changing the meaning.

Respond in JSON format:
{{
    "synonyms": ["word1", "word2", "word3"],
    "antonyms": ["word1", "word2"],
    "contextual_meaning": "specific meaning in this context",
    "confidence": 0.8,
    "pos_verified": "{pos_normalized}",
    "notes": "reasoning for each choice"
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹„ìš© ì ˆì•½
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert English linguist specializing in contextual word analysis. Provide accurate synonyms and antonyms based on specific contextual usage.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=300,
                temperature=0.1,
            )

            self.gpt_calls += 1
            content = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            result = json.loads(content)

            # ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬
            validated_result = self._validate_gpt_result(result, word)
            validated_result["source"] = "gpt"

            if self.verbose:
                syns = len(validated_result.get("synonyms", []))
                ants = len(validated_result.get("antonyms", []))
                print(f"   ğŸ“š GPT ì¶”ì¶œ: {word} â†’ ë™ì˜ì–´ {syns}ê°œ, ë°˜ì˜ì–´ {ants}ê°œ")

            return validated_result

        except json.JSONDecodeError as e:
            raise Exception(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        except Exception as e:
            raise Exception(f"GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    def _deduplicate_by_stem(self, words: List[str]) -> List[str]:
        """ì–´ê°„(stem)ì´ ì¤‘ë³µë˜ëŠ” ë‹¨ì–´ëŠ” í•œ ê°œë§Œ ë‚¨ê¸°ê³  ì œê±°"""
        ps = PorterStemmer()
        seen_stems = set()
        deduped = []

        for word in words:
            stem = ps.stem(word.lower())
            if stem not in seen_stems:
                deduped.append(word)
                seen_stems.add(stem)
            elif self.verbose:
                print(f"   âš ï¸ ì–´ê·¼ ì¤‘ë³µ ì œê±°ë¨: {word} (stem: {stem})")

        return deduped

    def _validate_gpt_result(self, result: Dict, word: str) -> Dict[str, Any]:
        """GPT ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬ (ê¸°ë³¸ ë²„ì „)"""

        validated = {
            "synonyms": [],
            "antonyms": [],
            "contextual_meaning": str(result.get("contextual_meaning", "")),
            "confidence": min(max(float(result.get("confidence", 0.5)), 0.0), 1.0),
            "notes": str(result.get("notes", "")),
        }

        # ë™ì˜ì–´ ê²€ì¦
        synonyms_raw = result.get("synonyms", [])
        if isinstance(synonyms_raw, list):
            valid_synonyms = []
            for syn in synonyms_raw:
                if isinstance(syn, str) and self._is_valid_single_word(syn, word):
                    valid_synonyms.append(syn.strip())

            # ğŸ”¥ ì–´ê·¼ ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 3ê°œ ì œí•œ
            validated["synonyms"] = self._enhanced_filter_with_root_dedup(
                valid_synonyms, word, max_count=3
            )

        # ë°˜ì˜ì–´ ê²€ì¦
        antonyms_raw = result.get("antonyms", [])
        if isinstance(antonyms_raw, list):
            valid_antonyms = []
            for ant in antonyms_raw:
                if isinstance(ant, str) and self._is_valid_single_word(ant, word):
                    valid_antonyms.append(ant.strip())

            # ğŸ”¥ ì–´ê·¼ ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 2ê°œ ì œí•œ
            validated["antonyms"] = self._enhanced_filter_with_root_dedup(
                valid_antonyms, word, max_count=2
            )

        return validated

    def _is_valid_single_word(self, candidate: str, original_word: str) -> bool:
        """ë‹¨ì¼ ë‹¨ì–´ ìœ íš¨ì„± ê²€ì‚¬"""
        candidate = candidate.strip()

        # ë‘ ê°œ ì´ìƒ ë‹¨ì–´ ì°¨ë‹¨
        if " " in candidate or "-" in candidate or "_" in candidate:
            return False

        # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ì°¨ë‹¨ (ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ì œì™¸)
        if not re.match(r"^[a-zA-Z']+$", candidate):
            return False

        # ê¸¸ì´ ì œí•œ
        if len(candidate) < 3 or len(candidate) > 12:
            return False

        # ì›ë³¸ê³¼ ë™ì¼í•œ ê²ƒ ì œê±°
        if candidate.lower() == original_word.lower():
            return False

        # ê¸°ë³¸ ë‹¨ì–´ ì œê±°
        if candidate.lower() in self.basic_words:
            return False

        return True

    def _enhanced_filter_with_root_dedup(
        self, candidates: List[str], original_word: str, max_count: int = 3
    ) -> List[str]:
        """ì–´ê·¼ ì¤‘ë³µ ì œê±° + ê°œìˆ˜ ì œí•œ í•„í„°ë§"""

        if not candidates:
            return []

        try:
            from nltk.stem import PorterStemmer

            ps = PorterStemmer()

            seen_stems = set()

            # ì›ë³¸ ë‹¨ì–´ì˜ ì–´ê·¼ ë¨¼ì € ì¶”ê°€
            original_stem = ps.stem(original_word.lower())
            seen_stems.add(original_stem)

            filtered_results = []

            for candidate in candidates:
                candidate_stem = ps.stem(candidate.lower())

                # ì–´ê·¼ì´ ì¤‘ë³µë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
                if candidate_stem not in seen_stems:
                    filtered_results.append(candidate)
                    seen_stems.add(candidate_stem)

                    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
                    if len(filtered_results) >= max_count:
                        break

            if self.verbose and len(candidates) > len(filtered_results):
                removed_count = len(candidates) - len(filtered_results)
                print(
                    f"   ğŸ”„ ì–´ê·¼ ì¤‘ë³µ ì œê±°: {removed_count}ê°œ ì œê±°, {len(filtered_results)}ê°œ ìœ ì§€"
                )

            return filtered_results

        except Exception as e:
            if self.verbose:
                print(f"   âš ï¸ ì–´ê·¼ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì¤‘ë³µ ì œê±° ì‚¬ìš©")

            # NLTK ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì¤‘ë³µ ì œê±° + ê°œìˆ˜ ì œí•œ
            unique_candidates = list(dict.fromkeys(candidates))
            return unique_candidates[:max_count]

    def _get_wordnet_synonyms_antonyms_enhanced(
        self, word: str, pos: str = ""
    ) -> Dict[str, Any]:
        """WordNet ê¸°ë°˜ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ (í–¥ìƒëœ ë²„ì „ - ìµœëŒ€ 3ê°œ ì œí•œ)"""

        try:
            synonyms = []
            antonyms = []

            # WordNetì—ì„œ ë™ì˜ì–´ ì§‘í•© ê°€ì ¸ì˜¤ê¸°
            pos_normalized = self._normalize_pos(pos)
            if pos_normalized:
                synsets = wordnet.synsets(word, pos=pos_normalized)
            else:
                synsets = wordnet.synsets(word)

            for synset in synsets:
                # ë™ì˜ì–´ ìˆ˜ì§‘
                for lemma in synset.lemmas():
                    if lemma.name().lower() != word.lower():
                        synonym = lemma.name().replace("_", " ")
                        # ğŸ”¥ ì¦‰ì‹œ í•„í„°ë§ ì ìš©
                        if self._is_valid_single_word(synonym, word):
                            synonyms.append(synonym)

                # ë°˜ì˜ì–´ ìˆ˜ì§‘
                for lemma in synset.lemmas():
                    for antonym in lemma.antonyms():
                        antonym_word = antonym.name().replace("_", " ")
                        # ğŸ”¥ ì¦‰ì‹œ í•„í„°ë§ ì ìš©
                        if self._is_valid_single_word(antonym_word, word):
                            antonyms.append(antonym_word)

            # ğŸ”¥ ì–´ê·¼ ì¤‘ë³µ ì œê±° ë° ê°œìˆ˜ ì œí•œ
            filtered_synonyms = self._enhanced_filter_with_root_dedup(
                synonyms, word, max_count=3
            )
            filtered_antonyms = self._enhanced_filter_with_root_dedup(
                antonyms, word, max_count=2
            )

            return {
                "synonyms": filtered_synonyms,
                "antonyms": filtered_antonyms,
                "contextual_meaning": "",
                "confidence": 0.6,
                "source": "wordnet_enhanced",
                "notes": "WordNet ê¸°ë°˜ ì¶”ì¶œ (ìµœëŒ€ 3ê°œ ì œí•œ, ì–´ê·¼ ì¤‘ë³µ ì œê±°)",
            }

        except Exception as e:
            if self.verbose:
                print(f"   âš ï¸ WordNet ì¶”ì¶œ ì‹¤íŒ¨ ({word}): {e}")

            return {
                "synonyms": [],
                "antonyms": [],
                "contextual_meaning": "",
                "confidence": 0.0,
                "source": "failed",
                "notes": f"WordNet ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}",
            }

    def batch_extract(
        self, word_list: List[Dict[str, str]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ë‹¨ì–´ì˜ ë™ì˜ì–´/ë°˜ì˜ì–´ë¥¼ ë°°ì¹˜ë¡œ ì¶”ì¶œ

        Args:
            word_list: [{"word": "ë‹¨ì–´", "context": "ë¬¸ë§¥", "pos": "í’ˆì‚¬", "meaning": "ì˜ë¯¸"}, ...]

        Returns:
            {ë‹¨ì–´: ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ë³´} ë”•ì…”ë„ˆë¦¬
        """
        results = {}

        print(f"ğŸ” ë™ì˜ì–´/ë°˜ì˜ì–´ ë°°ì¹˜ ì¶”ì¶œ ì‹œì‘: {len(word_list)}ê°œ ë‹¨ì–´")

        for i, word_info in enumerate(word_list):
            word = word_info.get("word", "")
            context = word_info.get("context", "")
            pos = word_info.get("pos", "")
            meaning = word_info.get("meaning", "")

            if not word:
                continue

            try:
                result = self.extract_synonyms_antonyms(word, context, pos, meaning)
                results[word] = result

                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 10 == 0:
                    print(f"   ğŸ“Š ì§„í–‰ë¥ : {i + 1}/{len(word_list)} ì™„ë£Œ")

                # ìºì‹œ ì£¼ê¸°ì  ì €ì¥ (50ê°œë§ˆë‹¤)
                if (i + 1) % 50 == 0:
                    self._save_cache()

            except Exception as e:
                if self.verbose:
                    print(f"   âŒ {word} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                results[word] = {
                    "synonyms": [],
                    "antonyms": [],
                    "contextual_meaning": "",
                    "confidence": 0.0,
                    "source": "error",
                    "notes": f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                }

        # ìµœì¢… ìºì‹œ ì €ì¥
        self._save_cache()

        # í†µê³„ ì¶œë ¥
        successful = len([r for r in results.values() if r.get("confidence", 0) > 0])
        with_synonyms = len(
            [r for r in results.values() if len(r.get("synonyms", [])) > 0]
        )
        with_antonyms = len(
            [r for r in results.values() if len(r.get("antonyms", [])) > 0]
        )

        print(f"âœ… ë°°ì¹˜ ì¶”ì¶œ ì™„ë£Œ:")
        print(f"   ğŸ“Š ì„±ê³µ: {successful}/{len(word_list)}ê°œ")
        print(f"   ğŸ“Š ë™ì˜ì–´ í¬í•¨: {with_synonyms}ê°œ")
        print(f"   ğŸ“Š ë°˜ì˜ì–´ í¬í•¨: {with_antonyms}ê°œ")
        print(f"   ğŸ“Š GPT í˜¸ì¶œ: {self.gpt_calls}íšŒ")

        return results

    def get_statistics(self) -> Dict[str, Any]:
        """ì¶”ì¶œ í†µê³„ ë°˜í™˜"""
        total_items = len(self.cache)

        if total_items == 0:
            return {"total_items": 0}

        with_synonyms = 0
        with_antonyms = 0
        total_synonyms = 0
        total_antonyms = 0
        gpt_count = 0
        wordnet_count = 0

        for result in self.cache.values():
            if isinstance(result, dict):
                synonyms = result.get("synonyms", [])
                antonyms = result.get("antonyms", [])

                if synonyms:
                    with_synonyms += 1
                    total_synonyms += len(synonyms)

                if antonyms:
                    with_antonyms += 1
                    total_antonyms += len(antonyms)

                source = result.get("source", "unknown")
                if source == "gpt":
                    gpt_count += 1
                elif source == "wordnet":
                    wordnet_count += 1

        return {
            "total_items": total_items,
            "with_synonyms": with_synonyms,
            "with_antonyms": with_antonyms,
            "synonym_coverage": (
                (with_synonyms / total_items * 100) if total_items > 0 else 0
            ),
            "antonym_coverage": (
                (with_antonyms / total_items * 100) if total_items > 0 else 0
            ),
            "avg_synonyms": (
                (total_synonyms / with_synonyms) if with_synonyms > 0 else 0
            ),
            "avg_antonyms": (
                (total_antonyms / with_antonyms) if with_antonyms > 0 else 0
            ),
            "gpt_count": gpt_count,
            "wordnet_count": wordnet_count,
            "gpt_calls_used": self.gpt_calls,
        }


# ======= ê¸°ì¡´ AdvancedVocabExtractorì— í†µí•©í•˜ëŠ” ì½”ë“œ =======


def integrate_synonym_extractor_to_vocab_extractor():
    """
    ê¸°ì¡´ AdvancedVocabExtractor í´ë˜ìŠ¤ì— ë™ì˜ì–´/ë°˜ì˜ì–´ ê¸°ëŠ¥ì„ í†µí•©í•˜ëŠ” ë°©ë²•

    ë‹¤ìŒ ì½”ë“œë¥¼ AdvancedVocabExtractor í´ë˜ìŠ¤ì— ì¶”ê°€í•˜ì„¸ìš”:
    """

    integration_code = '''
# AdvancedVocabExtractor í´ë˜ìŠ¤ì˜ __init__ ë©”ì„œë“œì— ì¶”ê°€:
def __init__(self, ...):
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # ğŸ”¥ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
    try:
        self.synonym_extractor = SynonymAntonymExtractor(
            client=client, 
            cache_file="synonym_antonym_cache.json",
            verbose=self.verbose
        )
        print("âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        self.synonym_extractor = None

# AdvancedVocabExtractor í´ë˜ìŠ¤ì— ìƒˆ ë©”ì„œë“œ ì¶”ê°€:
def add_synonyms_antonyms_to_results(self, results):
    """ê²°ê³¼ì— ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ë³´ ì¶”ê°€"""
    if not hasattr(self, 'synonym_extractor') or not self.synonym_extractor:
        print("âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return results
    
    print("ğŸ” ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ì¤‘...")
    
    # ë‹¨ì–´ ì •ë³´ ì¤€ë¹„
    word_list = []
    for result in results:
        word_info = {
            "word": result.get("original", result.get("ë‹¨ì–´", "")),
            "context": result.get("context", result.get("ë¬¸ë§¥", "")),
            "pos": result.get("pos", result.get("í’ˆì‚¬", "")),
            "meaning": result.get("korean_meaning", result.get("ëœ»(í•œê¸€)", ""))
        }
        word_list.append(word_info)
    
    # ë°°ì¹˜ ì¶”ì¶œ
    synonym_results = self.synonym_extractor.batch_extract(word_list)
    
    # ê²°ê³¼ì— ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ë³´ ì¶”ê°€
    for result in results:
        word = result.get("original", result.get("ë‹¨ì–´", ""))
        if word in synonym_results:
            syn_data = synonym_results[word]
            result["synonyms"] = ", ".join(syn_data.get("synonyms", []))
            result["antonyms"] = ", ".join(syn_data.get("antonyms", []))
            result["ë™ì˜ì–´"] = result["synonyms"]  # í•œê¸€ ì»¬ëŸ¼ëª…ë„ ì§€ì›
            result["ë°˜ì˜ì–´"] = result["antonyms"]
            result["synonym_confidence"] = syn_data.get("confidence", 0.0)
    
    return results

# process_text ë©”ì„œë“œ ìˆ˜ì • (ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì¶”ê°€):
def process_text(self, ...):
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # ğŸ”¥ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ (ìƒˆë¡œ ì¶”ê°€)
    if rows:  # ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ
        try:
            rows = self.add_synonyms_antonyms_to_results(rows)
            print(f"âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì™„ë£Œ: {len(rows)}ê°œ í•­ëª©")
        except Exception as e:
            print(f"âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    return rows
'''

    return integration_code


# ======= ë…ë¦½ ì‹¤í–‰ ì˜ˆì œ =======


def example_usage():
    """ì‚¬ìš© ì˜ˆì œ"""

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì‹¤ì œ ì‚¬ìš© ì‹œ í•„ìš”)
    # client = openai.OpenAI()

    # ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ìƒì„±
    # extractor = SynonymAntonymExtractor(client, verbose=True)

    # ë‹¨ì¼ ë‹¨ì–´ ì¶”ì¶œ
    # result = extractor.extract_synonyms_antonyms(
    #     word="significant",
    #     context="This is a significant achievement in the field of science.",
    #     pos="adjective",
    #     meaning="ì¤‘ìš”í•œ, ì˜ë¯¸ ìˆëŠ”"
    # )
    # print("ë™ì˜ì–´:", result["synonyms"])
    # print("ë°˜ì˜ì–´:", result["antonyms"])

    # ë°°ì¹˜ ì¶”ì¶œ
    # word_list = [
    #     {"word": "significant", "context": "significant impact", "pos": "adj", "meaning": "ì¤‘ìš”í•œ"},
    #     {"word": "analyze", "context": "analyze the data", "pos": "verb", "meaning": "ë¶„ì„í•˜ë‹¤"},
    #     {"word": "complex", "context": "complex problem", "pos": "adj", "meaning": "ë³µì¡í•œ"}
    # ]
    # results = extractor.batch_extract(word_list)

    # í†µê³„ í™•ì¸
    # stats = extractor.get_statistics()
    # print("ì¶”ì¶œ í†µê³„:", stats)

    pass


if __name__ == "__main__":
    print("ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ëª¨ë“ˆ")
    print("ì´ ëª¨ë“ˆì„ ê¸°ì¡´ vocab_extractorì— í†µí•©í•˜ì„¸ìš”.")
    print("\ní†µí•© ë°©ë²•:")
    print("1. ì´ íŒŒì¼ì„ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì €ì¥")
    print(
        "2. vocab_extractor.pyì—ì„œ 'from synonym_antonym_module import SynonymAntonymExtractor' import"
    )
    print("3. AdvancedVocabExtractor í´ë˜ìŠ¤ì— ìœ„ì˜ í†µí•© ì½”ë“œ ì¶”ê°€")
