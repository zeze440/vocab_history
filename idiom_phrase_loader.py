import pandas as pd
import requests
import json
import logging
from typing import Dict, List, Optional, Union
import time
from requests.exceptions import RequestException
import os
import pickle
import hashlib
from datetime import datetime, timedelta

# í™˜ê²½ ë³€ìˆ˜ë¡œ ë¡œê·¸ ë ˆë²¨ ì œì–´
if not os.getenv("DEBUG_OPENAI"):
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("idiom_loader")


class DatasetMetadata:
    """ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, name: str, url: str, source: str, load_date: str = None):
        self.name = name
        self.url = url
        self.source = source
        self.load_date = load_date or time.strftime("%Y-%m-%d %H:%M:%S")
        self.record_count = 0

    def to_dict(self) -> Dict:
        return {
            "name": self.name,
            "url": self.url,
            "source": self.source,
            "load_date": self.load_date,
            "record_count": self.record_count,
        }


class DataCache:
    """ë°ì´í„° ìºì‹±ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, cache_dir: str = ".cache", cache_validity: int = None):
        """
        ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™”
        Args:
            cache_dir: ìºì‹œ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
            cache_validity: ìºì‹œ ìœ íš¨ ê¸°ê°„(ì¼)
        """
        self.cache_dir = cache_dir
        self.cache_validity = cache_validity  # ìºì‹œ ìœ íš¨ ê¸°ê°„(ì¼)

        # ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
            logger.info(f"ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {cache_dir}")

    def _get_cache_path(self, url: str) -> str:
        """URLì— í•´ë‹¹í•˜ëŠ” ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        # URLì˜ í•´ì‹œê°’ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return os.path.join(self.cache_dir, f"{url_hash}.pkl")

    def get_cache_summary(self) -> Dict:
        """ìºì‹œ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        summary = {"total_files": 0, "total_records": 0, "cache_details": []}

        if not os.path.exists(self.cache_dir):
            return summary

        for filename in os.listdir(self.cache_dir):
            if filename.endswith(".pkl"):
                cache_path = os.path.join(self.cache_dir, filename)
                try:
                    with open(cache_path, "rb") as f:
                        cached_data = pickle.load(f)
                        record_count = len(cached_data)
                        mod_time = datetime.fromtimestamp(os.path.getmtime(cache_path))

                        summary["total_files"] += 1
                        summary["total_records"] += record_count
                        summary["cache_details"].append(
                            {
                                "file": filename,
                                "records": record_count,
                                "modified": mod_time.strftime("%Y-%m-%d %H:%M:%S"),
                            }
                        )
                except Exception as e:
                    logger.warning(f"ìºì‹œ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {filename}: {e}")

        return summary

    def get(self, url: str) -> Optional[pd.DataFrame]:
        """
        ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

        Args:
            url: ë°ì´í„° ì†ŒìŠ¤ URL

        Returns:
            ìºì‹œëœ ë°ì´í„°í”„ë ˆì„ ë˜ëŠ” None(ìºì‹œ ì—†ìŒ)
        """
        cache_path = self._get_cache_path(url)

        if not os.path.exists(cache_path):
            logger.debug(f"ìºì‹œ ì—†ìŒ: {url}")
            return None

        try:
            # ìºì‹œ íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ í™•ì¸
            mod_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
            now = datetime.now()

            # ìºì‹œ ìœ íš¨ê¸°ê°„ í™•ì¸
            if now - mod_time > timedelta(days=self.cache_validity):
                logger.info(
                    f"ìºì‹œ ë§Œë£Œë¨: {url} (ìƒì„±ì¼: {mod_time.strftime('%Y-%m-%d')})"
                )
                return None

            # ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ
            with open(cache_path, "rb") as f:
                cached_data = pickle.load(f)
                logger.info(f"ìºì‹œì—ì„œ ë¡œë“œë¨: {url} (ë ˆì½”ë“œ ìˆ˜: {len(cached_data)})")
                return cached_data

        except Exception as e:
            logger.warning(f"ìºì‹œ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
            return None

    def save(self, url: str, df: pd.DataFrame) -> bool:
        """
        ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥

        Args:
            url: ë°ì´í„° ì†ŒìŠ¤ URL
            df: ì €ì¥í•  ë°ì´í„°í”„ë ˆì„

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        cache_path = self._get_cache_path(url)

        try:
            with open(cache_path, "wb") as f:
                pickle.dump(df, f)
            logger.info(f"ìºì‹œì— ì €ì¥ë¨: {url} (ë ˆì½”ë“œ ìˆ˜: {len(df)})")
            return True
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return False


# ì „ì—­ ìºì‹œ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
cache = DataCache(cache_validity=9999)


def safe_request(
    url: str, stream: bool = False, timeout: int = 30, max_retries: int = 3
) -> requests.Response:
    """ì•ˆì „í•˜ê²Œ HTTP ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, stream=stream, timeout=timeout)
            response.raise_for_status()  # 4XX, 5XX ì˜¤ë¥˜ ê²€ì¶œ
            return response
        except RequestException as e:
            retries += 1
            wait_time = 2**retries  # ì§€ìˆ˜ ë°±ì˜¤í”„
            logger.warning(
                f"ìš”ì²­ ì‹¤íŒ¨ ({retries}/{max_retries}): {str(e)}. {wait_time}ì´ˆ í›„ ì¬ì‹œë„..."
            )
            time.sleep(wait_time)

    raise RequestException(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {url}")


def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> pd.DataFrame:
    """ë°ì´í„°í”„ë ˆì„ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ê³  í•„ìš”í•œ ì—´ì´ ì—†ìœ¼ë©´ ì¶”ê°€"""
    for col in required_columns:
        if col not in df.columns:
            df[col] = ""
    return df


def load_tatoeba_phrases(
    limit: int = 100000,  # ê¸°ë³¸ê°’ì„ 10ë§Œìœ¼ë¡œ ì¦ê°€
    chunk_size: int = 1000,
    use_cache: bool = False,
) -> pd.DataFrame:
    """Tatoebaì—ì„œ ì˜ì–´ ë¬¸ì¥ ë°ì´í„° ë¡œë“œ (ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)"""
    metadata = DatasetMetadata(
        name="Tatoeba Phrases",
        url=f"https://downloads.tatoeba.org/exports/sentences.csv?limit={limit}",
        source="tatoeba",
    )

    logger.info(f"{metadata.name} ë°ì´í„° ë¡œë“œ ì¤‘... (ìµœëŒ€ {limit}ê°œ, ìºì‹œ ì‚¬ìš© ì•ˆí•¨)")

    try:
        # ì‹¤ì œ Tatoeba sentences.csv ë‹¤ìš´ë¡œë“œ URL
        url = "https://downloads.tatoeba.org/exports/sentences.csv"
        res = safe_request(url, stream=True)

        phrases = []
        processed_count = 0  # ì²˜ë¦¬ëœ ë¼ì¸ ìˆ˜
        valid_phrases = 0  # ìœ íš¨í•œ êµ¬ë¬¸ ìˆ˜
        skip_header = True  # í—¤ë” ìŠ¤í‚µ í”Œë˜ê·¸

        print(f"ğŸ“¥ Tatoeba ë°ì´í„° ì²˜ë¦¬ ì¤‘... (ìµœëŒ€ {limit:,}ê°œ)")

        for line in res.iter_lines():
            # í—¤ë” ìŠ¤í‚µ (ì²« ë²ˆì§¸ ë¼ì¸)
            if skip_header:
                skip_header = False
                continue

            # ì›í•˜ëŠ” ê°œìˆ˜ë§Œí¼ ìˆ˜ì§‘í–ˆìœ¼ë©´ ì¤‘ë‹¨
            if valid_phrases >= limit:
                break

            try:
                # TSV íŒŒì‹± (íƒ­ìœ¼ë¡œ êµ¬ë¶„)
                row = line.decode("utf-8").split("\t")

                # ìµœì†Œ 3ê°œ ì»¬ëŸ¼ í•„ìš”: ID, ì–¸ì–´ì½”ë“œ, ë¬¸ì¥
                if len(row) >= 3 and row[1] == "eng":  # ì˜ì–´ë§Œ í•„í„°ë§
                    sentence = row[2].strip()

                    # ë¹ˆ ë¬¸ì¥ ìŠ¤í‚µ
                    if not sentence:
                        continue

                    words = sentence.split()

                    # ë¬¸ì¥ ê¸¸ì´ í•„í„°ë§ (ì›í•˜ëŠ” ì¡°ê±´ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥)
                    if 2 <= len(words) <= 15:  # 2-15ë‹¨ì–´ë¡œ ë²”ìœ„ í™•ëŒ€
                        phrases.append(
                            {
                                "phrase": sentence,
                                "definition": "",
                                "example": "",
                                "source": metadata.source,
                            }
                        )
                        valid_phrases += 1

            except (UnicodeDecodeError, IndexError) as e:
                # ë””ì½”ë”© ì˜¤ë¥˜ë‚˜ ì»¬ëŸ¼ ë¶€ì¡± ë¬´ì‹œ
                continue
            except Exception as e:
                # ê¸°íƒ€ ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                continue

            processed_count += 1

            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10,000ê°œë§ˆë‹¤)
            if processed_count % 10000 == 0:
                print(
                    f"  ğŸ“Š ì²˜ë¦¬ë¨: {processed_count:,}ì¤„, ìœ íš¨ êµ¬ë¬¸: {valid_phrases:,}ê°œ"
                )

        print(
            f"âœ… Tatoeba ì²˜ë¦¬ ì™„ë£Œ: ì´ {processed_count:,}ì¤„ ì²˜ë¦¬, {valid_phrases:,}ê°œ êµ¬ë¬¸ ìˆ˜ì§‘"
        )

        df = pd.DataFrame(phrases)
        metadata.record_count = len(df)

        logger.info(
            f"{metadata.name} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {metadata.record_count}ê°œ ë ˆì½”ë“œ"
        )

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        df.attrs["metadata"] = metadata.to_dict()

        # ìºì‹œì— ì €ì¥
        if use_cache:
            cache.save(metadata.url, df)

        return df

    except Exception as e:
        logger.error(f"{metadata.name} ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return pd.DataFrame(columns=["phrase", "definition", "example", "source"])


def load_all_idioms_and_phrases(
    use_cache: bool = True,
    save_to_csv: Optional[str] = None,
    tatoeba_limit: int = 30000,
) -> pd.DataFrame:
    master_cache_file = os.path.join(cache.cache_dir, "master_idioms_combined.pkl")

    # âœ… ë¡œì»¬ CSV íŒŒì¼ ë¨¼ì € ë¡œë“œ
    local_csv_path = "all_merged_idioms.csv"
    custom_idioms = []

    if os.path.exists(local_csv_path):
        try:
            local_df = pd.read_csv(local_csv_path)
            if "idiom" in local_df.columns:
                custom_idioms = (
                    local_df["idiom"].dropna().str.strip().str.lower().tolist()
                )
                print(f"ğŸ“– {local_csv_path}ì—ì„œ {len(custom_idioms)}ê°œ ìˆ™ì–´ ë¡œë“œë¨")
            else:
                print(f"âš ï¸ {local_csv_path}ì— 'idiom' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"âŒ {local_csv_path} ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ìºì‹œ í™•ì¸ (ë¡œì»¬ ìˆ™ì–´ í¬í•¨ëœ ì™„ì „í•œ ìºì‹œ)
    if use_cache and os.path.exists(master_cache_file):
        try:
            with open(master_cache_file, "rb") as f:
                combined_df = pickle.load(f)

            # ë¡œì»¬ ìˆ™ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if len(combined_df) > 15000:  # Tatoeba(10K) + ë¡œì»¬(24K) ì •ë„ë©´ í¬í•¨ëœ ê²ƒ
                logger.info(
                    f"âœ… ìºì‹œì—ì„œ ì™„ì „í•œ ìˆ™ì–´ ë°ì´í„° ë¡œë“œ: {len(combined_df)}ê°œ"
                )
                return combined_df
            else:
                logger.info("ìºì‹œê°€ ë¶ˆì™„ì „í•¨. ìƒˆë¡œ ìƒì„±...")
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {e}")

    # ìƒˆë¡œ ìƒì„±
    logger.info("ì˜ì–´ ìˆ™ì–´ ë° êµ¬ë¬¸ ë°ì´í„°ë¥¼ ìƒˆë¡œ ë¡œë“œí•©ë‹ˆë‹¤...")
    tatoeba_df = load_tatoeba_phrases(limit=tatoeba_limit, use_cache=False)
    combined_df = pd.concat([tatoeba_df], ignore_index=True)

    # ë¡œì»¬ ìˆ™ì–´ ë³‘í•©
    if custom_idioms:
        df_local = pd.DataFrame(
            {"phrase": custom_idioms, "definition": "", "example": "", "source": "user"}
        )
        combined_df = pd.concat([combined_df, df_local], ignore_index=True)

    combined_df.drop_duplicates(subset=["phrase"], keep="first", inplace=True)

    # ìºì‹œ ì €ì¥
    if use_cache:
        try:
            with open(master_cache_file, "wb") as f:
                pickle.dump(combined_df, f)
            logger.info(f"âœ… í†µí•© ìºì‹œ ì €ì¥ ì™„ë£Œ: {master_cache_file}")
        except Exception as e:
            logger.error(f"í†µí•© ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    return combined_df


# ë©”ì¸ í•¨ìˆ˜ (ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰ë  ë•Œ ì‚¬ìš©)
def main():
    import argparse

    parser = argparse.ArgumentParser(description="ì˜ì–´ ìˆ™ì–´ ë° êµ¬ë¬¸ ë°ì´í„° ë¡œë”")
    parser.add_argument("--output", "-o", type=str, help="ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument(
        "--limit",
        "-l",
        type=int,
        default=30000,
        help="Tatoebaì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜",
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê¹… í™œì„±í™”")
    parser.add_argument("--no-cache", action="store_true", help="ìºì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ")
    parser.add_argument(
        "--clear-cache", action="store_true", help="ìºì‹œ ì´ˆê¸°í™” í›„ ì‹¤í–‰"
    )
    parser.add_argument(
        "--cache-dir", type=str, default=".cache", help="ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ"
    )
    parser.add_argument(
        "--cache-validity", type=int, default=100, help="ìºì‹œ ìœ íš¨ ê¸°ê°„(ì¼)"
    )

    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(logging.DEBUG)

    # ìºì‹œ ì„¤ì •
    global cache
    cache = DataCache(cache_dir=args.cache_dir, cache_validity=args.cache_validity)

    # ìºì‹œ ì´ˆê¸°í™” ì˜µì…˜ ì²˜ë¦¬
    if args.clear_cache:
        import shutil

        try:
            shutil.rmtree(args.cache_dir)
            os.makedirs(args.cache_dir)
            logger.info(f"ìºì‹œ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”: {args.cache_dir}")
        except Exception as e:
            logger.error(f"ìºì‹œ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")

    df = load_all_idioms_and_phrases(
        use_cache=not args.no_cache, save_to_csv=args.output, tatoeba_limit=args.limit
    )

    print(f"\nì´ {len(df)}ê°œì˜ ì˜ì–´ ìˆ™ì–´ì™€ êµ¬ë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    print(f"ì†ŒìŠ¤ë³„ ë¶„í¬:")
    for source, count in df["source"].value_counts().items():
        print(f"- {source}: {count}ê°œ")


if __name__ == "__main__":
    main()
