import pandas as pd
import re
import nltk
from nltk import ngrams, word_tokenize
from collections import Counter, defaultdict
import spacy
from typing import List, Dict, Tuple
import argparse
import json

# NLTK ë‹¤ìš´ë¡œë“œ
nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)

# SpaCy ë¡œë“œ
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("âŒ SpaCy ì˜ì–´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("python -m spacy download en_core_web_sm")
    exit(1)


class PatternExtractor:
    """ì§€ë¬¸DBì—ì„œ êµìœ¡ìš© íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        # êµìœ¡ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì‹œì‘ ë‹¨ì–´ë“¤ (íŒ¨í„´ ì•ë¶€ë¶„ì— ì˜¤ë©´ ê°€ì )
        self.educational_starters = {
            "according",
            "as",
            "in",
            "on",
            "with",
            "by",
            "due",
            "because",
            "since",
            "while",
            "during",
            "after",
            "before",
            "until",
            "unless",
            "although",
            "though",
            "however",
            "therefore",
            "moreover",
            "furthermore",
            "nevertheless",
            "nonetheless",
            "consequently",
            "hence",
            "thus",
            "instead",
            "rather",
            "besides",
            "additionally",
            "similarly",
            "likewise",
        }

        # ì „ì¹˜ì‚¬ ë¦¬ìŠ¤íŠ¸ (ì „ì¹˜ì‚¬êµ¬ íŒ¨í„´ ì¸ì‹ìš©)
        self.prepositions = {
            "about",
            "above",
            "across",
            "after",
            "against",
            "along",
            "among",
            "around",
            "at",
            "before",
            "behind",
            "below",
            "beneath",
            "beside",
            "between",
            "beyond",
            "by",
            "down",
            "during",
            "except",
            "for",
            "from",
            "in",
            "inside",
            "into",
            "like",
            "near",
            "of",
            "off",
            "on",
            "outside",
            "over",
            "through",
            "throughout",
            "till",
            "to",
            "toward",
            "under",
            "until",
            "up",
            "upon",
            "with",
            "within",
            "without",
        }

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not isinstance(text, str):
            return ""

        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ë‹¨, í•˜ì´í”ˆê³¼ ì–´í¬ìŠ¤íŠ¸ë¡œí”¼ëŠ” ë³´ì¡´)
        text = re.sub(r"[^\w\s\'-]", " ", text)

        # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
        text = re.sub(r"\s+", " ", text)

        return text.strip().lower()

    def extract_ngrams(
        self, texts: List[str], n: int, min_freq: int = 2
    ) -> Dict[str, int]:
        """N-gram ì¶”ì¶œ"""
        ngram_counts = Counter()

        for text in texts:
            if not isinstance(text, str) or len(text.strip()) == 0:
                continue

            clean_text = self.clean_text(text)
            if len(clean_text) < 5:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                continue

            # í† í°í™”
            try:
                tokens = word_tokenize(clean_text)
                tokens = [
                    token for token in tokens if len(token) > 1 and token.isalpha()
                ]

                if len(tokens) < n:
                    continue

                # N-gram ìƒì„±
                for ngram in ngrams(tokens, n):
                    ngram_str = " ".join(ngram)

                    # ê¸°ë³¸ í•„í„°ë§
                    if self.is_valid_ngram(ngram, n):
                        ngram_counts[ngram_str] += 1

            except Exception as e:
                print(f"âš ï¸ í† í°í™” ì˜¤ë¥˜: {e}")
                continue

        # ìµœì†Œ ë¹ˆë„ í•„í„°ë§
        return {
            ngram: count for ngram, count in ngram_counts.items() if count >= min_freq
        }

    def is_valid_ngram(self, ngram: Tuple[str, ...], n: int) -> bool:
        """N-gramì´ êµìœ¡ì ìœ¼ë¡œ ìœ íš¨í•œì§€ íŒë‹¨"""
        ngram_list = list(ngram)

        # 2. ì²« ë²ˆì§¸ë‚˜ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ê´€ì‚¬ì¸ ê²½ìš° ì œì™¸
        if ngram_list[0] in ["a", "an", "the"] or ngram_list[-1] in ["a", "an", "the"]:
            return False

        # 4. ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
        if all(word.isdigit() for word in ngram_list):
            return False

        return True

    def calculate_educational_score(self, ngram: str, frequency: int, n: int) -> float:
        """êµìœ¡ì  ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = frequency * 1.0  # ê¸°ë³¸ ë¹ˆë„ ì ìˆ˜

        words = ngram.split()

        # 2. ì „ì¹˜ì‚¬êµ¬ íŒ¨í„´ ë³´ë„ˆìŠ¤
        if any(word in self.prepositions for word in words):
            score += 1.5

        # 3. ê¸¸ì´ë³„ ê°€ì¤‘ì¹˜ (3-4gramì´ ê°€ì¥ ìœ ìš©)
        length_weights = {2: 1.0, 3: 1.5, 4: 2.0, 5: 1.2}
        score *= length_weights.get(n, 1.0)

        # 4. íŠ¹ì • íŒ¨í„´ ë³´ë„ˆìŠ¤
        pattern_bonuses = {
            r"\bas\s+\w+\s+as\b": 3.0,  # as ... as
            r"\bin\s+order\s+to\b": 3.0,  # in order to
            r"\bas\s+a\s+result\b": 2.5,  # as a result
            r"\bin\s+spite\s+of\b": 2.5,  # in spite of
            r"\bdue\s+to\b": 2.0,  # due to
            r"\baccording\s+to\b": 2.0,  # according to
            r"\bnot\s+only\b": 2.5,  # not only
            r"\bwould\s+rather\b": 2.0,  # would rather
        }

        for pattern, bonus in pattern_bonuses.items():
            if re.search(pattern, ngram, re.IGNORECASE):
                score += bonus
                break

        return score

    def filter_by_pos_patterns(self, ngrams_dict: Dict[str, int]) -> Dict[str, Dict]:
        """í’ˆì‚¬ íŒ¨í„´ì„ ê³ ë ¤í•œ í•„í„°ë§"""
        filtered_results = {}

        for ngram, frequency in ngrams_dict.items():
            try:
                # SpaCyë¡œ í’ˆì‚¬ ë¶„ì„
                doc = nlp(ngram)
                pos_pattern = [token.pos_ for token in doc]

                # êµìœ¡ì ìœ¼ë¡œ ì¤‘ìš”í•œ í’ˆì‚¬ íŒ¨í„´ë“¤
                important_patterns = [
                    ["ADP", "NOUN"],  # ì „ì¹˜ì‚¬ + ëª…ì‚¬
                    ["ADV", "ADJ"],  # ë¶€ì‚¬ + í˜•ìš©ì‚¬
                    ["VERB", "ADP"],  # ë™ì‚¬ + ì „ì¹˜ì‚¬ (phrasal verb)
                    ["ADP", "NOUN", "ADP"],  # ì „ì¹˜ì‚¬ + ëª…ì‚¬ + ì „ì¹˜ì‚¬
                    ["CONJ", "ADP"],  # ì ‘ì†ì‚¬ + ì „ì¹˜ì‚¬
                    ["ADV", "CONJ"],  # ë¶€ì‚¬ + ì ‘ì†ì‚¬
                ]

                # íŒ¨í„´ ë§¤ì¹­ í™•ì¸
                is_important = any(
                    pos_pattern == pattern
                    or (
                        len(pos_pattern) >= len(pattern)
                        and pos_pattern[: len(pattern)] == pattern
                    )
                    for pattern in important_patterns
                )

                if is_important or frequency >= 5:  # ê³ ë¹ˆë„ëŠ” í’ˆì‚¬ ìƒê´€ì—†ì´ í¬í•¨
                    filtered_results[ngram] = {
                        "frequency": frequency,
                        "pos_pattern": pos_pattern,
                        "is_important_pattern": is_important,
                    }

            except Exception as e:
                # SpaCy ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ë¹ˆë„ë§Œìœ¼ë¡œ íŒë‹¨
                if frequency >= 3:
                    filtered_results[ngram] = {
                        "frequency": frequency,
                        "pos_pattern": [],
                        "is_important_pattern": False,
                    }

        return filtered_results

    def extract_comprehensive_patterns(self, texts: List[str]) -> Dict[int, Dict]:
        """ì¢…í•©ì ì¸ íŒ¨í„´ ì¶”ì¶œ"""
        print("ğŸ“Š N-gram íŒ¨í„´ ì¶”ì¶œ ì‹œì‘...")

        all_patterns = {}

        # 2-gramë¶€í„° 5-gramê¹Œì§€ ì¶”ì¶œ
        for n in range(2, 6):
            print(f"ğŸ” {n}-gram ì¶”ì¶œ ì¤‘...")

            min_freq = max(1, 5 - n)  # ê¸¸ìˆ˜ë¡ ë‚®ì€ ìµœì†Œë¹ˆë„
            ngrams_dict = self.extract_ngrams(texts, n, min_freq)

            print(f"   â†’ {len(ngrams_dict)}ê°œ {n}-gram ë°œê²¬")

            # í’ˆì‚¬ íŒ¨í„´ í•„í„°ë§
            filtered_ngrams = self.filter_by_pos_patterns(ngrams_dict)

            # êµìœ¡ì  ì ìˆ˜ ê³„ì‚°
            scored_patterns = {}
            for ngram, info in filtered_ngrams.items():
                score = self.calculate_educational_score(ngram, info["frequency"], n)
                scored_patterns[ngram] = {
                    **info,
                    "educational_score": score,
                    "length": n,
                }

            # ìƒìœ„ íŒ¨í„´ë§Œ ì„ ë³„ (n-gramë³„ë¡œ ìµœëŒ€ 50ê°œ)
            top_patterns = dict(
                sorted(
                    scored_patterns.items(),
                    key=lambda x: x[1]["educational_score"],
                    reverse=True,
                )[:200]
            )

            all_patterns[n] = top_patterns
            print(f"   âœ… {len(top_patterns)}ê°œ ìƒìœ„ {n}-gram ì„ ë³„")

        return all_patterns

    def export_patterns(self, patterns: Dict[int, Dict], output_file: str):
        """íŒ¨í„´ì„ Excel íŒŒì¼ë¡œ ì €ì¥"""
        all_data = []

        for n, ngrams in patterns.items():
            for ngram, info in ngrams.items():
                all_data.append(
                    {
                        "pattern": ngram,
                        "length": n,
                        "frequency": info["frequency"],
                        "educational_score": round(info["educational_score"], 2),
                        "pos_pattern": " -> ".join(info.get("pos_pattern", [])),
                        "is_important": info.get("is_important_pattern", False),
                        "category": self.categorize_pattern(ngram),
                    }
                )

        # DataFrame ìƒì„± ë° ì •ë ¬
        df = pd.DataFrame(all_data)
        df = df.sort_values(
            ["educational_score", "frequency"], ascending=[False, False]
        )

        # Excel ì €ì¥
        with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
            # ì „ì²´ íŒ¨í„´
            df.to_excel(writer, sheet_name="ì „ì²´íŒ¨í„´", index=False)

            # ê¸¸ì´ë³„ ì‹œíŠ¸
            for n in range(2, 6):
                n_gram_df = df[df["length"] == n]
                if not n_gram_df.empty:
                    n_gram_df.to_excel(writer, sheet_name=f"{n}gram", index=False)

            # ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸
            categories = df["category"].unique()
            for category in categories:
                cat_df = df[df["category"] == category]
                if not cat_df.empty and len(category) < 30:  # ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
                    cat_df.to_excel(writer, sheet_name=category, index=False)

        print(f"âœ… íŒ¨í„´ ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“Š ì´ {len(df)}ê°œ íŒ¨í„´ ì¶”ì¶œë¨")

    def categorize_pattern(self, pattern: str) -> str:
        """íŒ¨í„´ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        pattern_lower = pattern.lower()

        # ì—°ê²°ì–´êµ¬
        if any(
            word in pattern_lower
            for word in [
                "as a result",
                "in addition",
                "however",
                "therefore",
                "moreover",
                "furthermore",
            ]
        ):
            return "ì—°ê²°ì–´êµ¬"

        # ì „ì¹˜ì‚¬êµ¬
        elif any(
            pattern_lower.startswith(prep)
            for prep in ["in", "on", "at", "by", "with", "for", "of"]
        ):
            return "ì „ì¹˜ì‚¬êµ¬"

        # ì‹œê°„í‘œí˜„
        elif any(
            word in pattern_lower
            for word in ["when", "while", "during", "before", "after", "until", "since"]
        ):
            return "ì‹œê°„í‘œí˜„"

        # ì¡°ê±´í‘œí˜„
        elif any(
            word in pattern_lower
            for word in ["if", "unless", "provided", "suppose", "in case"]
        ):
            return "ì¡°ê±´í‘œí˜„"

        # ë™ì‚¬êµ¬
        elif len(pattern.split()) == 2 and any(
            word in pattern_lower for word in ["up", "out", "off", "on", "down", "away"]
        ):
            return "ë™ì‚¬êµ¬"

        # í•™ìˆ í‘œí˜„
        elif any(
            word in pattern_lower
            for word in ["according to", "due to", "because of", "in terms of"]
        ):
            return "í•™ìˆ í‘œí˜„"

        else:
            return "ê¸°íƒ€í‘œí˜„"


def main():
    parser = argparse.ArgumentParser(description="ì§€ë¬¸DBì—ì„œ êµìœ¡ìš© íŒ¨í„´ ì¶”ì¶œ")
    parser.add_argument("--input", "-i", required=True, help="ì…ë ¥ CSV íŒŒì¼ (ì§€ë¬¸DB)")
    parser.add_argument(
        "--output", "-o", default="extracted_patterns.xlsx", help="ì¶œë ¥ Excel íŒŒì¼"
    )
    parser.add_argument(
        "--text-column", "-t", default="content", help="í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆëŠ” ì»¬ëŸ¼ëª…"
    )
    parser.add_argument("--encoding", "-e", default="cp949", help="CSV íŒŒì¼ ì¸ì½”ë”©")

    args = parser.parse_args()

    try:
        # CSV íŒŒì¼ ì½ê¸°
        print(f"ğŸ“‚ {args.input} íŒŒì¼ ì½ëŠ” ì¤‘...")
        try:
            df = pd.read_csv(args.input, encoding=args.encoding)
        except UnicodeDecodeError:
            df = pd.read_csv(args.input, encoding="utf-8")

        print(f"âœ… {len(df)}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
        print(f"ğŸ“Š ì»¬ëŸ¼ëª…: {list(df.columns)}")

        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
        text_column = args.text_column
        if text_column not in df.columns:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
            text_column = df.columns[0]
            print(f"âš ï¸ '{args.text_column}' ì»¬ëŸ¼ì´ ì—†ì–´ì„œ '{text_column}' ì»¬ëŸ¼ ì‚¬ìš©")

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = df[text_column].dropna().astype(str).tolist()
        print(f"ğŸ“ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")

        # íŒ¨í„´ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        extractor = PatternExtractor()

        # íŒ¨í„´ ì¶”ì¶œ ì‹¤í–‰
        patterns = extractor.extract_comprehensive_patterns(texts)

        # ê²°ê³¼ ì €ì¥
        extractor.export_patterns(patterns, args.output)

        # ìš”ì•½ í†µê³„ ì¶œë ¥
        total_patterns = sum(len(ngrams) for ngrams in patterns.values())
        print(f"\nğŸ“ˆ ì¶”ì¶œ ê²°ê³¼ ìš”ì•½:")
        for n, ngrams in patterns.items():
            print(f"  â€¢ {n}-gram: {len(ngrams)}ê°œ")
        print(f"  â€¢ ì´ íŒ¨í„´: {total_patterns}ê°œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
