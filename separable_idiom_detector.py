# separable_idiom_detector.py - ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ ì‹œìŠ¤í…œ

import re
import json
from typing import Set, Dict, List
from utils import load_json_safe, save_json_safe


class SeparableIdiomDetector:
    """ì‚¬ìš©ì DBì˜ ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ê¸° (OpenAI í™œìš©)"""

    def __init__(self, client, verbose=False):
        self.client = client
        self.verbose = verbose
        self.separable_cache = {}  # GPT í˜¸ì¶œ ê²°ê³¼ ìºì‹±
        self.gpt_calls = 0

        # ë¶„ë¦¬í˜• ìˆ™ì–´ ë°ì´í„°ë² ì´ìŠ¤
        self.user_separable_idioms = {}  # {ì›ë³¸ìˆ™ì–´: ë¶„ë¦¬í˜•ì •ë³´}
        self.separable_patterns = {}  # {ì›ë³¸ìˆ™ì–´: [íŒ¨í„´ë“¤]}

        print("ğŸ”§ ì‚¬ìš©ì DB ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_user_idioms_with_gpt(self, user_idioms: Set[str]) -> Dict[str, Dict]:
        """GPTë¡œ ì‚¬ìš©ì DB ìˆ™ì–´ë“¤ì˜ ë¶„ë¦¬í˜• ê°€ëŠ¥ì„± ë¶„ì„"""
        print(f"ğŸ¤– GPTë¡œ ì‚¬ìš©ì ìˆ™ì–´ ë¶„ë¦¬í˜• ë¶„ì„ ì‹œì‘: {len(user_idioms)}ê°œ")

        results = {}

        # ë„ì–´ì“°ê¸°ê°€ ìˆëŠ” ìˆ™ì–´ë“¤ë§Œ ë¶„ì„
        potential_idioms = [
            idiom for idiom in user_idioms if " " in idiom and len(idiom.split()) == 2
        ]

        print(f"   ğŸ“‹ ë¶„ì„ ëŒ€ìƒ 2ë‹¨ì–´ ìˆ™ì–´: {len(potential_idioms)}ê°œ")

        for idiom in potential_idioms:
            if idiom in self.separable_cache:
                results[idiom] = self.separable_cache[idiom]
                continue

            try:
                analysis = self._gpt_analyze_separable_idiom(idiom)
                results[idiom] = analysis
                self.separable_cache[idiom] = analysis

                if self.verbose and analysis.get("is_separable", False):
                    display = analysis.get("display_form", idiom)
                    print(f"      âœ… ë¶„ë¦¬í˜• í™•ì¸: {idiom} â†’ {display}")

            except Exception as e:
                if self.verbose:
                    print(f"      âŒ GPT ë¶„ì„ ì‹¤íŒ¨ ({idiom}): {e}")
                continue

        # ë¶„ë¦¬í˜• ìˆ™ì–´ë§Œ í•„í„°ë§
        separable_count = len(
            [r for r in results.values() if r.get("is_separable", False)]
        )
        print(f"   ğŸ¯ ë¶„ë¦¬í˜• ìˆ™ì–´ ë°œê²¬: {separable_count}ê°œ")

        return results

    def _gpt_analyze_separable_idiom(self, idiom: str) -> Dict:
        """GPTë¡œ ê°œë³„ ìˆ™ì–´ì˜ ë¶„ë¦¬í˜• ì—¬ë¶€ ë¶„ì„"""
        words = idiom.split()
        if len(words) != 2:
            return {"is_separable": False, "reason": "Not a two-word phrase"}

        verb, particle = words[0], words[1]

        prompt = f"""
Analyze the English phrasal verb "{idiom}":

Please determine if this is a separable phrasal verb.

Characteristics of separable phrasal verbs:
1. Structure: verb + particle (up, down, on, off, out, in, away, back, etc.)
2. When there's an object, it can be placed between the verb and particle
   Example: pick up â†’ pick something up, pick it up
3. Pronoun objects (it, him, her, them) MUST be placed between verb and particle
   Example: pick it up (âœ“), pick up it (âœ—)

Analyze "{idiom}":
- Is "{verb}" a verb?
- Is "{particle}" a particle?
- Can an object be placed between them?
- Is this actually used as a separable phrasal verb in real English?

Please respond in JSON format:
{{
    "is_separable": true/false,
    "verb": "{verb}",
    "particle": "{particle}", 
    "display_form": "if separable, show as 'verb ~ particle' format",
    "examples": ["examples of separable usage"],
    "reason": "reasoning for the decision",
    "confidence": 0.0-1.0
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in English phrasal verbs. You can accurately identify separable phrasal verbs and their usage patterns.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=300,
                temperature=0.1,
            )

            self.gpt_calls += 1
            content = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            result = json.loads(content)

            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„
            if not isinstance(result.get("is_separable"), bool):
                result["is_separable"] = False

            if result["is_separable"] and not result.get("display_form"):
                result["display_form"] = f"{verb} ~ {particle}"

            return result

        except Exception as e:
            if self.verbose:
                print(f"GPT ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "is_separable": False,
                "reason": f"GPT ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                "confidence": 0.0,
            }

    def build_separable_patterns(self, separable_analysis: Dict[str, Dict]):
        """ë¶„ë¦¬í˜• ìˆ™ì–´ë“¤ì˜ ê°ì§€ íŒ¨í„´ ìƒì„±"""
        print(f"ğŸ”§ ë¶„ë¦¬í˜• ìˆ™ì–´ íŒ¨í„´ ìƒì„± ì¤‘...")

        for idiom, analysis in separable_analysis.items():
            if not analysis.get("is_separable", False):
                continue

            words = idiom.split()
            if len(words) != 2:
                continue

            verb, particle = words[0], words[1]
            display_form = analysis.get("display_form", f"{verb} ~ {particle}")

            # ë‹¤ì–‘í•œ ë¶„ë¦¬í˜• íŒ¨í„´ë“¤ ìƒì„±
            patterns = self._generate_detection_patterns(verb, particle)

            self.user_separable_idioms[idiom] = {
                "display_form": display_form,
                "verb": verb,
                "particle": particle,
                "confidence": analysis.get("confidence", 0.9),
                "examples": analysis.get("examples", []),
                "reason": analysis.get("reason", ""),
            }

            self.separable_patterns[idiom] = patterns

            if self.verbose:
                print(f"   âœ… {idiom} â†’ {display_form} ({len(patterns)}ê°œ íŒ¨í„´)")

        print(f"âœ… ë¶„ë¦¬í˜• íŒ¨í„´ ìƒì„± ì™„ë£Œ: {len(self.user_separable_idioms)}ê°œ ìˆ™ì–´")

    def _generate_detection_patterns(self, verb: str, particle: str) -> List[Dict]:
        """ê°œë³„ ë¶„ë¦¬í˜• ìˆ™ì–´ì˜ ê°ì§€ íŒ¨í„´ë“¤ ìƒì„±"""
        patterns = []

        # 1. ì—°ì†í˜• (ê¸°ë³¸í˜•)
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+{re.escape(particle)}\b",
                "type": "continuous",
                "description": "ì—°ì†í˜•",
                "is_separated": False,
                "priority": 1,
            }
        )

        # 2. ë¶„ë¦¬í˜• - ì¼ë°˜ ëª…ì‚¬/ëª…ì‚¬êµ¬
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+(?:the\s+|a\s+|an\s+|this\s+|that\s+|his\s+|her\s+|my\s+|your\s+|our\s+|their\s+)?(?:\w+\s+)*\w+\s+{re.escape(particle)}\b",
                "type": "separated_noun",
                "description": "ë¶„ë¦¬í˜•(ëª…ì‚¬)",
                "is_separated": True,
                "priority": 2,
            }
        )

        # 3. ë¶„ë¦¬í˜• - ëŒ€ëª…ì‚¬ (ë°˜ë“œì‹œ ë¶„ë¦¬)
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+(?:it|him|her|them|this|that|these|those)\s+{re.escape(particle)}\b",
                "type": "separated_pronoun",
                "description": "ë¶„ë¦¬í˜•(ëŒ€ëª…ì‚¬)",
                "is_separated": True,
                "priority": 3,
            }
        )

        # 4. ë¶„ë¦¬í˜• - ê¸´ ëª…ì‚¬êµ¬
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+(?:the\s+)?(?:\w+\s+){{2,}}\w+\s+{re.escape(particle)}\b",
                "type": "separated_long_noun",
                "description": "ë¶„ë¦¬í˜•(ê¸´ëª…ì‚¬êµ¬)",
                "is_separated": True,
                "priority": 2,
            }
        )

        return patterns

    def detect_separable_idioms_in_text(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©ì DB ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€"""
        if not self.user_separable_idioms:
            return []

        results = []
        found_positions = set()

        if self.verbose:
            print(f"ğŸ” ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ ì¤‘: {len(self.user_separable_idioms)}ê°œ ëŒ€ìƒ")

        # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ ì •ë ¬ (ë¶„ë¦¬í˜•ì„ ë¨¼ì € ì°¾ì•„ì„œ ì—°ì†í˜•ê³¼ ì¤‘ë³µ ë°©ì§€)
        for idiom, info in self.user_separable_idioms.items():
            patterns = self.separable_patterns.get(idiom, [])

            # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ íŒ¨í„´ ì •ë ¬ (ë¶„ë¦¬í˜• ë¨¼ì €)
            sorted_patterns = sorted(
                patterns, key=lambda x: x["priority"], reverse=True
            )

            for pattern_info in sorted_patterns:
                pattern = pattern_info["pattern"]

                matches = re.finditer(pattern, text, re.IGNORECASE)

                for match in matches:
                    start, end = match.span()

                    # ì¤‘ë³µ ìœ„ì¹˜ í™•ì¸
                    if any(abs(start - pos[0]) <= 5 for pos in found_positions):
                        continue

                    matched_text = match.group().strip()

                    # ê²°ê³¼ ìƒì„±
                    result = {
                        "original": matched_text,
                        "base_form": idiom,
                        "display_form": info["display_form"],
                        "pattern_type": pattern_info["type"],
                        "description": pattern_info["description"],
                        "is_separated": pattern_info["is_separated"],
                        "confidence": info["confidence"],
                        "start": start,
                        "end": end,
                        "separable_info": {
                            "verb": info["verb"],
                            "particle": info["particle"],
                            "detection_pattern": pattern,
                            "gpt_reason": info.get("reason", ""),
                            "examples": info.get("examples", []),
                        },
                    }

                    results.append(result)
                    found_positions.add((start, end))

                    if self.verbose:
                        sep_mark = "ğŸ”§" if result["is_separated"] else "ğŸ“"
                        print(
                            f"      {sep_mark} ë°œê²¬: '{matched_text}' â†’ {info['display_form']} ({pattern_info['description']})"
                        )

                    # ê°™ì€ ìˆ™ì–´ì˜ ë‹¤ë¥¸ íŒ¨í„´ì€ ìŠ¤í‚µ (ì²« ë²ˆì§¸ ë§¤ì¹˜ë§Œ)
                    break

        return results

    def integrate_with_extractor(self, extractor_instance):
        """ê¸°ì¡´ AdvancedVocabExtractorì™€ í†µí•©"""
        print(f"ğŸ”— ê¸°ì¡´ extractorì™€ ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ê¸° í†µí•©...")

        # ê¸°ì¡´ ì‚¬ìš©ì ìˆ™ì–´ë“¤ ë¶„ì„
        if hasattr(extractor_instance, "user_idioms"):
            separable_analysis = self.analyze_user_idioms_with_gpt(
                extractor_instance.user_idioms
            )
            self.build_separable_patterns(separable_analysis)

            # extractorì— ë¶„ë¦¬í˜• ì •ë³´ ì¶”ê°€
            extractor_instance.separable_detector = self
            extractor_instance.user_separable_idioms = self.user_separable_idioms

            print(
                f"âœ… í†µí•© ì™„ë£Œ: {len(self.user_separable_idioms)}ê°œ ë¶„ë¦¬í˜• ìˆ™ì–´ í™œì„±í™”"
            )
        else:
            print("âš ï¸ extractorì— user_idiomsê°€ ì—†ìŠµë‹ˆë‹¤")

    def save_separable_analysis(self, output_file: str = "separable_analysis.json"):
        """ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        analysis_data = {
            "separable_idioms": self.user_separable_idioms,
            "total_count": len(self.user_separable_idioms),
            "gpt_calls": self.gpt_calls,
            "cache": self.separable_cache,
        }

        if save_json_safe(analysis_data, output_file):
            print(f"âœ… ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        else:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {output_file}")

    def load_separable_analysis(self, input_file: str = "separable_analysis.json"):
        """ê¸°ì¡´ ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        analysis_data = load_json_safe(input_file)

        if not analysis_data:
            print(f"ğŸ“‚ {input_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
            return False

        try:
            self.user_separable_idioms = analysis_data.get("separable_idioms", {})
            self.separable_cache = analysis_data.get("cache", {})

            # íŒ¨í„´ ì¬ìƒì„±
            for idiom, info in self.user_separable_idioms.items():
                verb = info.get("verb", "")
                particle = info.get("particle", "")
                if verb and particle:
                    patterns = self._generate_detection_patterns(verb, particle)
                    self.separable_patterns[idiom] = patterns

            print(f"âœ… ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ë¡œë“œ: {len(self.user_separable_idioms)}ê°œ")
            return True

        except Exception as e:
            print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


class AdvancedIdiomChecker:
    """ê³ ê¸‰ ìˆ™ì–´ ê²€ì¦ê¸° - ë¶„ë¦¬í˜•ê³¼ ë¬¸ë²• íŒ¨í„´ êµ¬ë¶„"""

    def __init__(self, nlp_model):
        self.nlp = nlp_model

        # ì—°ì†í˜• ê°€ëŠ¥ êµ¬ë™ì‚¬ (ë¶™ì—¬ì„œ ì¨ë„ OK)
        self.optional_separable = {
            "pick up",
            "turn on",
            "turn off",
            "look up",
            "put on",
            "take off",
            "bring up",
            "call up",
            "give up",
            "set up",
            "clean up",
            "fill up",
            "write down",
            "sit down",
            "stand up",
            "wake up",
            "get up",
        }

        # ë°˜ë“œì‹œ ë¶„ë¦¬ë˜ì–´ì•¼ í•˜ëŠ” êµ¬ë™ì‚¬ (ëª©ì ì–´ í•„ìˆ˜)
        self.mandatory_separable = {
            "pick": ["up", "out", "off"],
            "turn": ["down", "up"],
            "put": ["away", "back"],
            "take": ["apart", "down"],
            "figure": ["out"],
            "work": ["out"],
            "point": ["out"],
            "carry": ["out"],
            "bring": ["about"],
        }

        # ë¬¸ë²• íŒ¨í„´ ìˆ™ì–´ë“¤ (íŠ¹ì • í’ˆì‚¬ í•„ìˆ˜)
        self.grammar_patterns = {
            # V-ing íŒ¨í„´
            r"\bspend\s+(?:time|money|hours?|days?|years?)\s+(\w+ing)\b": "spend time V-ing",
            r"\bis\s+worth\s+(\w+ing)\b": "be worth V-ing",
            r"\bkeep\s+(?:on\s+)?(\w+ing)\b": "keep V-ing",
            r"\bavoid\s+(\w+ing)\b": "avoid V-ing",
            r"\benjoy\s+(\w+ing)\b": "enjoy V-ing",
            r"\bfinish\s+(\w+ing)\b": "finish V-ing",
            # N + V-ing íŒ¨í„´
            r"\bprevent\s+(\w+)\s+from\s+(\w+ing)\b": "prevent N from V-ing",
            r"\bstop\s+(\w+)\s+from\s+(\w+ing)\b": "stop N from V-ing",
            # ê¸°íƒ€ íŒ¨í„´
            r"\bit\s+takes\s+(\w+)\s+to\s+(\w+)": "it takes N to V",
            r"\bthere\s+is\s+no\s+point\s+in\s+(\w+ing)\b": "there is no point in V-ing",
        }

        # ì•Œë ¤ì§„ ì¼ë°˜ ìˆ™ì–´ íŒ¨í„´ë“¤
        self.known_phrasal_patterns = {
            r"\bas\s+\w+\s+as\b",
            r"\bin\s+order\s+to\b",
            r"\bas\s+a\s+result\b",
            r"\bon\s+the\s+other\s+hand\b",
            r"\bfor\s+instance\b",
            r"\bin\s+spite\s+of\b",
            r"\bbecause\s+of\b",
            r"\binstead\s+of\b",
            r"\baccording\s+to\b",
        }

    def analyze_phrasal_verb_pattern(self, text, verb_token, particle_token):
        """êµ¬ë™ì‚¬ íŒ¨í„´ ë¶„ì„ - ì—°ì†í˜• vs ë¶„ë¦¬í˜• vs ë¬¸ë²•íŒ¨í„´"""
        verb = verb_token.lemma_.lower()
        particle = particle_token.text.lower()
        base_phrasal = f"{verb} {particle}"

        # 1. ì—°ì†í˜• ê°€ëŠ¥í•œ êµ¬ë™ì‚¬ì¸ì§€ í™•ì¸
        if base_phrasal in self.optional_separable:
            return {
                "pattern_type": "optional_separable",
                "base_form": base_phrasal,
                "display_form": base_phrasal,  # ê·¸ëƒ¥ ì—°ì†í˜•ìœ¼ë¡œ í‘œì‹œ
                "is_separated": False,
            }

        # 2. ë°˜ë“œì‹œ ë¶„ë¦¬ë˜ì–´ì•¼ í•˜ëŠ” êµ¬ë™ì‚¬ì¸ì§€ í™•ì¸
        if (
            verb in self.mandatory_separable
            and particle in self.mandatory_separable[verb]
        ):
            return {
                "pattern_type": "mandatory_separable",
                "base_form": base_phrasal,
                "display_form": f"{verb} ~ {particle}",  # ~ ë¡œ í‘œì‹œ
                "is_separated": True,
            }

        # 3. ì¼ë°˜ êµ¬ë™ì‚¬ (ì‹¤ì œ ë¶„ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        verb_idx = verb_token.i
        particle_idx = particle_token.i

        # ë™ì‚¬ì™€ ì…ì ì‚¬ì´ì— ë‹¤ë¥¸ í† í°ì´ ìˆëŠ”ì§€ í™•ì¸
        if abs(verb_idx - particle_idx) > 1:
            # ì‹¤ì œë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŒ
            return {
                "pattern_type": "actually_separated",
                "base_form": base_phrasal,
                "display_form": f"{verb} ~ {particle}",
                "is_separated": True,
            }
        else:
            # ì—°ì†ìœ¼ë¡œ ë¶™ì–´ìˆìŒ
            return {
                "pattern_type": "continuous",
                "base_form": base_phrasal,
                "display_form": base_phrasal,
                "is_separated": False,
            }

    def analyze_grammar_pattern(self, text):
        """ë¬¸ë²• íŒ¨í„´ ë¶„ì„"""
        results = []

        for pattern_regex, pattern_name in self.grammar_patterns.items():
            matches = re.finditer(pattern_regex, text, re.IGNORECASE)
            for match in matches:
                start, end = match.span()
                original_text = match.group()

                # ë§¤ì¹­ëœ ê·¸ë£¹ë“¤ ë¶„ì„
                groups = match.groups()

                # V-ing íŒ¨í„´ ê²€ì¦
                if "V-ing" in pattern_name and groups:
                    # ë§ˆì§€ë§‰ ê·¸ë£¹ì´ ì‹¤ì œë¡œ ë™ëª…ì‚¬ì¸ì§€ í™•ì¸
                    last_word = groups[-1]
                    if last_word.endswith("ing"):
                        results.append(
                            {
                                "original": original_text,
                                "pattern_type": "grammar_pattern",
                                "display_form": pattern_name,
                                "base_form": pattern_name,
                                "start": start,
                                "end": end,
                                "is_separated": False,
                            }
                        )

                # ê¸°íƒ€ íŒ¨í„´
                else:
                    results.append(
                        {
                            "original": original_text,
                            "pattern_type": "grammar_pattern",
                            "display_form": pattern_name,
                            "base_form": pattern_name,
                            "start": start,
                            "end": end,
                            "is_separated": False,
                        }
                    )

        return results
