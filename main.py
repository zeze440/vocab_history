# main.py - ë©”ì¸ ì‹¤í–‰ íŒŒì¼

import argparse
import sys
import pandas as pd
from config import DEFAULT_SETTINGS, FilePaths
from main_extractor import AdvancedVocabExtractor
from utils import safe_read_csv, find_text_column
import os
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")
    api_key = os.getenv('OPENAI_API_KEY')
    if api_key:
        print(f"âœ… API Key í™•ì¸: {api_key[:10]}...")
    else:
        print("âŒ API Key ë¡œë“œ ì‹¤íŒ¨")
except Exception as e:
    print(f"âš ï¸ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° v4.0")
    parser.add_argument(
        "--input", "-i", default=FilePaths.DEFAULT_INPUT, help="ì…ë ¥ CSV íŒŒì¼"
    )
    parser.add_argument(
        "--output", "-o", default=FilePaths.DEFAULT_OUTPUT, help="ì¶œë ¥ ì—‘ì…€ íŒŒì¼"
    )
    parser.add_argument(
        "--threshold", "-t", type=float, default=2.5, help="ë‚œì´ë„ ì„ê³„ê°’"
    )
    parser.add_argument("--cache", "-c", default="gpt_cache.json", help="GPT ìºì‹œ íŒŒì¼")
    parser.add_argument(
        "--max-tokens", "-mt", type=int, default=200000, help="ìµœëŒ€ í† í° ì‚¬ìš©ëŸ‰"
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥")
    parser.add_argument("--user-words", help="ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ (CSV/XLSX)")
    parser.add_argument(
        "--data-dir", default=FilePaths.DATA_DIR, help="ìˆ™ì–´ ë°ì´í„° ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--no-quality-check", action="store_true", help="í’ˆì§ˆ ê²€ì‚¬ ê±´ë„ˆë›°ê¸°"
    )

    args = parser.parse_args()

    settings = {
        "DIFFICULTY_THRESHOLD": args.threshold,
        "CACHE_FILE": args.cache,
        "MAX_TOKENS": args.max_tokens,
        "USE_CACHE": True,
        "USE_INTEGRATED_CONTEXTUAL": True,
        "USE_INTEGRATED_DIFFICULTY": True,
        "data_dir": args.data_dir,
    }

    print(f"ğŸš€ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° v4.0")
    print(f"   â€¢ ì‚¬ìš©ì DB ìˆ™ì–´ ìš°ì„  ì¸ì‹: âœ…")
    print(f"   â€¢ ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  í¬í•¨: âœ…")
    print(f"   â€¢ ë¬¸ë²• íŒ¨í„´ ë¶„ì„: âœ… (V-ing)")
    print(f"   â€¢ ê³ ê¸‰ êµ¬ë™ì‚¬ ë¶„ì„: âœ… (ì—°ì†í˜•/ë¶„ë¦¬í˜• ìë™ êµ¬ë¶„)")
    print(f"   â€¢ ë¶„ë¦¬í˜• í‘œì‹œ ê°œì„ : âœ… (pick ~ up, spend time V-ing)")
    print(f"   â€¢ í†µí•© ì»¨í…ìŠ¤íŠ¸ ì˜ë¯¸: âœ…")
    print(f"   â€¢ í†µí•© ë‚œì´ë„ ë¶„ì„: âœ…")

    # ğŸ”¥ data_loaderë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¡œë“œ
    from data_loader import load_texts_data

    try:
        extractor = AdvancedVocabExtractor(
            user_words_file=(
                args.user_words if args.user_words else FilePaths.USER_WORDS_FILE
            ),
            settings=settings,
            csv_file=args.input,
            verbose=args.verbose,
        )

        if not hasattr(extractor, "passage_db") or not extractor.passage_db:
            print(f"âŒ ì§€ë¬¸ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            return

        texts_with_metadata = extractor.passage_db  # ë©”íƒ€ë°ì´í„° í¬í•¨ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        print(f"âœ… ë©”íƒ€ë°ì´í„° í¬í•¨ í…ìŠ¤íŠ¸ {len(texts_with_metadata)}ê°œ ë¡œë“œ ì™„ë£Œ")

        print(
            f"ğŸ“š ì´ {len(texts_with_metadata)}ê°œ í…ìŠ¤íŠ¸ì—ì„œ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ì‹œì‘"
        )

        df = extractor.generate_vocabulary_workbook(
            texts_with_metadata,  # âœ… ë©”íƒ€ë°ì´í„° í¬í•¨ëœ í…ìŠ¤íŠ¸ ì „ë‹¬
            output_file=args.output,
            enable_quality_check=not args.no_quality_check,
        )
        # ê²°ê³¼ ì¶œë ¥
        if df is not None and len(df) > 0:
            print(f"\nğŸ‰ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ì™„ë£Œ!")
            print(f"   ğŸ“ íŒŒì¼: {args.output}")
            print(f"   ğŸ“Š ì´ í•­ëª©: {len(df)}ê°œ")
            print(
                f"   âœ¨ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ì ìš© (ë¶„ë¦¬í˜•: pick ~ up, ë¬¸ë²•íŒ¨í„´: spend time V-ing)"
            )

            # ì‚¬ìš©ì DB ë§¤ì¹­ í†µê³„ ì¶œë ¥
            if "ì‚¬ìš©ìDBë§¤ì¹­" in df.columns:
                user_matched = df["ì‚¬ìš©ìDBë§¤ì¹­"].sum()
                total_items = len(df)
                match_ratio = (
                    (user_matched / total_items * 100) if total_items > 0 else 0
                )
                print(f"\nğŸ‘¤ ì‚¬ìš©ì DB ë§¤ì¹­ ê²°ê³¼:")
                print(f"   â€¢ ë§¤ì¹­ëœ í•­ëª©: {user_matched}ê°œ")
                print(f"   â€¢ ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨: {match_ratio:.1f}%")

            # íŒ¨í„´ë³„ í†µê³„ ì¶œë ¥
            if "ë§¤ì¹­ë°©ì‹" in df.columns:
                pattern_stats = df["ë§¤ì¹­ë°©ì‹"].value_counts()
                print(f"\nğŸ“Š íŒ¨í„´ë³„ ì¶”ì¶œ í†µê³„:")
                for pattern, count in pattern_stats.items():
                    if pattern:
                        print(f"   â€¢ {pattern}: {count}ê°œ")

        else:
            print("âš ï¸ ë‹¨ì–´ì¥ ìƒì„±ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ì¶”ì¶œëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    except FileNotFoundError:
        print(f"âŒ '{args.input}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback

            traceback.print_exc()


def show_help():
    """ë„ì›€ë§ ì¶œë ¥"""
    print(
        """
ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° v4.0 ì‚¬ìš©ë²•:

ê¸°ë³¸ ì‚¬ìš©ë²•:
python main.py --input ì§€ë¬¸DB.csv --output vocabulary.xlsx

ğŸ”¥ ì£¼ìš” ê°œì„ ì‚¬í•­ (v4.0):
âœ… ê³ ê¸‰ êµ¬ë™ì‚¬ íŒ¨í„´ ë¶„ì„
â€¢ ì—°ì†í˜• ê°€ëŠ¥: pick up (ê·¸ëŒ€ë¡œ í‘œì‹œ)
â€¢ ë¶„ë¦¬ í•„ìˆ˜: pick ~ up (~ í‘œì‹œ)
â€¢ ì‹¤ì œ ë¶„ë¦¬: pick something up â†’ pick ~ up

âœ… ë¬¸ë²• íŒ¨í„´ ìë™ ì¸ì‹
â€¢ V-ing íŒ¨í„´: spend time reading â†’ spend time V-ing
â€¢ N V-ing íŒ¨í„´: prevent him from going â†’ prevent N from V-ing

âœ… ì‚¬ìš©ì DB ìš°ì„  ì²˜ë¦¬
â€¢ ì‚¬ìš©ì DB ìˆ™ì–´ ìµœìš°ì„  ì¸ì‹
â€¢ ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  í¬í•¨
â€¢ ë§¤ì¹­ ë°©ì‹ë³„ ìƒì„¸ í†µê³„

âœ… íŒ¨í„´ë³„ ì •ë°€ ë¶„ì„
â€¢ ìœ„ì¹˜ ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€
â€¢ ë¬¸ë²•ì  ê²€ì¦ ê°•í™”
â€¢ ì‹ ë¢°ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„

íŒ¨í„´ í‘œì‹œ ì˜ˆì‹œ:
- pick up â†’ pick up (ì—°ì†í˜• ê°€ëŠ¥)
- pick something up â†’ pick ~ up (ë¶„ë¦¬ í•„ìˆ˜)
- spend time reading â†’ spend time V-ing (ë¬¸ë²• íŒ¨í„´)

ì£¼ìš” ì˜µì…˜:
--input: ì…ë ¥ CSV íŒŒì¼ (ê¸°ë³¸: ì§€ë¬¸DB.csv)
--output: ì¶œë ¥ Excel íŒŒì¼ (ê¸°ë³¸: vocabulary_advanced.xlsx)
--user-words: ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ (ê¸°ë³¸: ë‹¨ì–´DB.csv)
--data-dir: ì°¸ì¡° ìˆ™ì–´ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data)
--verbose: ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
--no-quality-check: í’ˆì§ˆ ê²€ì‚¬ ê±´ë„ˆë›°ê¸°

ì‚¬ìš© ì˜ˆì‹œ:
python main.py --input ì§€ë¬¸DB.csv --output my_vocab.xlsx --verbose
"""
    )


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "help":
        show_help()
        sys.exit(0)

    try:
        print("ğŸš€ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° ì‹œì‘...")
        print("ğŸ”¥ v4.0 - ë¶„ë¦¬í˜•/ë¬¸ë²•íŒ¨í„´ ê³ ê¸‰ ë¶„ì„ ë²„ì „")
        main()
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
