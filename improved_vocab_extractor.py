import os
import json
import time
import pickle
import logging
import openai
import re
import glob
import requests
import numpy as np
import pandas as pd
from safe_data_utils import (
    safe_get_column_value,
    safe_string_operation,
    safe_numeric_operation,
)
from synonym_antonym_module import SynonymAntonymExtractor
from missing_methods import MissingMethodsMixin

from bs4 import BeautifulSoup
from collections import Counter, defaultdict
from nltk.corpus import wordnet, stopwords
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from tqdm import tqdm
from typing import List, Dict, Any, Set, Union
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
from contextlib import contextmanager
from datetime import datetime

import hashlib


# ğŸ”¥ 3. ì–´ê·¼ ì¤‘ë³µ ì œê±° ì „ì—­ í•¨ìˆ˜ ì¶”ê°€ (íŒŒì¼ ë§¨ ìœ„ìª½ì—)
def enhanced_filter_synonyms_antonyms(candidates, original_word, max_count=3):
    """ë™ì˜ì–´/ë°˜ì˜ì–´ í–¥ìƒëœ í•„í„°ë§ (ì–´ê·¼ ì¤‘ë³µ ì œê±° + ê°œìˆ˜ ì œí•œ)"""

    if not candidates:
        return []

    # 1ë‹¨ê³„: ê¸°ë³¸ í•„í„°ë§
    basic_filtered = []
    for candidate in candidates:
        candidate = str(candidate).strip()

        # ğŸ”¥ ë‘ ê°œ ì´ìƒ ë‹¨ì–´ ê°•ë ¥ ì°¨ë‹¨
        if " " in candidate or "-" in candidate or "_" in candidate:
            continue

        # ğŸ”¥ ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ì°¨ë‹¨ (ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ì œì™¸)
        if not re.match(r"^[a-zA-Z']+$", candidate):
            continue

        # ğŸ”¥ ê¸¸ì´ ì œí•œ
        if len(candidate) < 3 or len(candidate) > 12:
            continue

        # ğŸ”¥ ì›ë³¸ê³¼ ë™ì¼í•œ ê²ƒ ì œê±°
        if candidate.lower() == original_word.lower():
            continue

        basic_filtered.append(candidate)

    # 2ë‹¨ê³„: ì–´ê·¼ ì¤‘ë³µ ì œê±°
    try:
        from nltk.stem import PorterStemmer

        ps = PorterStemmer()

        seen_stems = set()
        original_stem = ps.stem(original_word.lower())
        seen_stems.add(original_stem)  # ì›ë³¸ ì–´ê·¼ ë¨¼ì € ì¶”ê°€

        unique_filtered = []
        for candidate in basic_filtered:
            stem = ps.stem(candidate.lower())
            if stem not in seen_stems:
                unique_filtered.append(candidate)
                seen_stems.add(stem)

        # 3ë‹¨ê³„: ê°œìˆ˜ ì œí•œ
        return unique_filtered[:max_count]

    except Exception:
        # NLTK ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì¤‘ë³µ ì œê±° + ê°œìˆ˜ ì œí•œ
        unique_basic = list(dict.fromkeys(basic_filtered))
        return unique_basic[:max_count]


# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
spacy = None
nlp = None
openai = None
client = None
nltk = None
lemmatizer = None

# ğŸ”¥ ê¸°ë³¸ ë‹¨ì–´ ì„¸íŠ¸ë“¤ ì •ì˜
BASIC_VERBS = {
    # ê°ê°/ì§€ê° ë™ì‚¬
    "feel",
    "see",
    "hear",
    "smell",
    "taste",
    "touch",
    "look",
    "watch",
    "listen",
    # ìƒíƒœ ë™ì‚¬
    "be",
    "am",
    "is",
    "are",
    "was",
    "were",
    "seem",
    "appear",
    "become",
    "stay",
    "remain",
    # ê¸°ë³¸ í–‰ë™ ë™ì‚¬
    "go",
    "come",
    "get",
    "give",
    "take",
    "make",
    "do",
    "have",
    "say",
    "tell",
    "speak",
    "eat",
    "drink",
    "sleep",
    "walk",
    "run",
    "sit",
    "stand",
    "lie",
    "live",
    "die",
    # ê°ì • ë™ì‚¬ (êµìœ¡ì  ê°€ì¹˜ ë¬´ê´€í•˜ê²Œ ì œì™¸)
    "love",
    "like",
    "hate",
    "want",
    "need",
    "hope",
    "wish",
    "fear",
    "worry",
    "care",
    # ì‚¬ê³  ë™ì‚¬
    "think",
    "know",
    "believe",
    "understand",
    "remember",
    "forget",
    "learn",
    "study",
    # ê¸°ë³¸ ì‘ì—… ë™ì‚¬
    "work",
    "play",
    "help",
    "start",
    "stop",
    "finish",
    "open",
    "close",
    "turn",
    "move",
}

BASIC_ADJECTIVES = {
    "good",
    "bad",
    "big",
    "small",
    "old",
    "new",
    "young",
    "hot",
    "cold",
    "warm",
    "happy",
    "sad",
    "angry",
    "tired",
    "hungry",
    "thirsty",
    "easy",
    "hard",
    "difficult",
    "important",
    "interesting",
    "beautiful",
    "ugly",
    "clean",
    "dirty",
    "fast",
    "slow",
}

BASIC_NOUNS = {
    "man",
    "woman",
    "child",
    "boy",
    "girl",
    "people",
    "person",
    "family",
    "friend",
    "house",
    "home",
    "school",
    "work",
    "job",
    "money",
    "time",
    "day",
    "night",
    "year",
}


def safe_import_packages():
    """í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤ì„ ì•ˆì „í•˜ê²Œ import"""
    global spacy, openai, nltk, nlp, lemmatizer, client

    try:
        import spacy

        nlp = spacy.load("en_core_web_sm")
    except ImportError:
        print(
            "âŒ spacyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install spacy' ì‹¤í–‰ í›„ 'python -m spacy download en_core_web_sm' ì‹¤í–‰í•˜ì„¸ìš”."
        )
        return False
    except OSError:
        print(
            "âŒ spacy ì˜ì–´ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 'python -m spacy download en_core_web_sm' ì‹¤í–‰í•˜ì„¸ìš”."
        )
        return False

    try:
        import openai

        # OpenAI API í‚¤ ê²€ì¦
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ìŒ ì¤‘ í•œ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”:")
            print("   1. export OPENAI_API_KEY='your-api-key-here'")
            print("   2. .env íŒŒì¼ì— OPENAI_API_KEY=your-api-key-here ì¶”ê°€")
            print("   3. ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •")
            return False

        try:
            client = openai.OpenAI(api_key=api_key)
            # API í‚¤ ìœ íš¨ì„± ê°„ë‹¨ í…ŒìŠ¤íŠ¸
            test_response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5,
            )
            print("âœ… OpenAI API ì—°ê²° ë° ì¸ì¦ ì„±ê³µ")
        except openai.AuthenticationError:
            print("âŒ OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return False
        except openai.RateLimitError:
            print("âš ï¸ OpenAI API ì‚¬ìš©ëŸ‰ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return False
        except openai.APIError as e:
            print(f"âŒ OpenAI API ì˜¤ë¥˜: {e}")
            return False
        except Exception as e:
            print(f"âŒ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    except ImportError:
        print("âŒ openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install openai' ì‹¤í–‰í•˜ì„¸ìš”.")
        return False

    try:
        import nltk
        from nltk.stem import WordNetLemmatizer

        lemmatizer = WordNetLemmatizer()

        # NLTK ë°ì´í„° í™•ì¸
        required_nltk_data = [
            ("tokenizers/punkt", "punkt"),
            ("corpora/wordnet", "wordnet"),
            ("taggers/averaged_perceptron_tagger", "averaged_perceptron_tagger"),
            ("corpora/stopwords", "stopwords"),
        ]

        for data_path, download_name in required_nltk_data:
            try:
                nltk.data.find(data_path)
            except LookupError:
                print(f"ğŸ“¦ NLTK {download_name} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
                try:
                    nltk.download(download_name, quiet=True)
                except Exception as e:
                    print(f"âš ï¸ NLTK {download_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    print("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")

    except ImportError:
        print("âŒ nltkê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install nltk' ì‹¤í–‰í•˜ì„¸ìš”.")
        return False

    return True


def safe_gpt_call(
    client,
    prompt_messages,
    model="gpt-4o",
    max_tokens=300,
    temperature=0.1,
    max_retries=3,
):
    """ì•ˆì „í•œ GPT API í˜¸ì¶œ with ì¬ì‹œë„ ë¡œì§"""

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=prompt_messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            return response, None

        except openai.AuthenticationError as e:
            error_msg = "API í‚¤ ì¸ì¦ ì‹¤íŒ¨"
            print(f"âŒ {error_msg}: {e}")
            return None, error_msg

        except openai.RateLimitError as e:
            wait_time = min(2**attempt, 60)  # ì§€ìˆ˜ ë°±ì˜¤í”„, ìµœëŒ€ 60ì´ˆ
            print(
                f"âš ï¸ API ì‚¬ìš©ëŸ‰ í•œë„ ë„ë‹¬. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (ì‹œë„ {attempt + 1}/{max_retries})"
            )
            time.sleep(wait_time)
            continue

        except openai.APIError as e:
            if attempt < max_retries - 1:
                wait_time = 2**attempt
                print(
                    f"âš ï¸ API ì˜¤ë¥˜ ë°œìƒ. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (ì‹œë„ {attempt + 1}/{max_retries}): {e}"
                )
                time.sleep(wait_time)
                continue
            else:
                error_msg = f"API ì˜¤ë¥˜ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼): {e}"
                print(f"âŒ {error_msg}")
                return None, error_msg

        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2**attempt
                print(
                    f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (ì‹œë„ {attempt + 1}/{max_retries}): {e}"
                )
                time.sleep(wait_time)
                continue
            else:
                error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼): {e}"
                print(f"âŒ {error_msg}")
                return None, error_msg

    return None, "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼"


# íŒ¨í‚¤ì§€ import ì‹¤í–‰
if not safe_import_packages():
    print("âŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ import ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit(1)


# ğŸ”¥ ê³ ë“±í•™ìƒ ìˆ˜ì¤€ ì‰¬ìš´ ë‹¨ì–´ íŒë³„ í•¨ìˆ˜
def enhanced_is_too_easy_for_highschool(word, pos, easy_words, child_vocab, freq_tiers):
    """ê³ ë“±í•™ìƒ ìˆ˜ì¤€ì—ì„œ ë„ˆë¬´ ì‰¬ìš´ ë‹¨ì–´ì¸ì§€ íŒë³„ - GPT í•„í„°ì™€ í•¨ê»˜ ì‚¬ìš©"""
    word_lower = word.lower()

    # 1. ê¸°ë³¸ ë‹¨ì–´ ê°•ë ¥ í•„í„°ë§ ì¶”ê°€ (ë§¨ ì•ì—)
    if (
        word_lower in BASIC_VERBS
        or word_lower in BASIC_ADJECTIVES
        or word_lower in BASIC_NOUNS
    ):
        return True

    # 2. ì™¸ë¶€ DB ì²´í¬ ì¶”ê°€
    if is_basic_by_external_db(word_lower):
        return True

    # 3. ì•„ë™ ì–´íœ˜ ì²´í¬
    if child_vocab and word_lower in child_vocab:
        return True

    # 4. ë§¤ìš° ì§§ì€ ë‹¨ì–´
    if len(word_lower) <= 2:
        return True

    # 5. ê¸°ë³¸ ë¬¸ë²• ìš”ì†Œ
    if pos in {"DET", "ADP", "CONJ", "PRON", "AUX", "INTJ"}:
        return True

    return False


# ğŸ”¥ contextual_meaning.py ì§ì ‘ í†µí•©
def integrated_get_best_korean_definition(
    word,
    phrase_db=None,
    is_phrase=False,
    max_tokens=10000,
    client=None,
    gpt_cache=None,
    gpt_call_count=0,
    GPT_CALL_LIMIT=100,
    token_usage=None,
    custom_prompt=None,
    sentence="",  # ğŸ”¥ ì¶”ê°€
):
    """contextual_meaning.pyì˜ get_best_korean_definition í†µí•© ë²„ì „"""

    # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
    if gpt_cache is None:
        gpt_cache = {}
    if token_usage is None:
        token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    word_lower = word.lower()

    # 1. phrase_dbì—ì„œ ë¨¼ì € ì°¾ê¸°
    if phrase_db is not None:
        found = phrase_db[phrase_db["phrase"].str.lower() == word_lower]
        if not found.empty:
            kor = found.iloc[0].get("definition", "")
            if kor and re.search("[ê°€-í£]", kor):
                return kor.strip(), gpt_cache, gpt_call_count, token_usage

    # 2. í† í° ì œí•œ í™•ì¸
    if token_usage["total_tokens"] >= max_tokens:
        print(f"      âŒ í† í° ì œí•œ ë„ë‹¬")
        return word, gpt_cache, gpt_call_count, token_usage

    # 3. GPT í˜¸ì¶œ
    if gpt_call_count >= GPT_CALL_LIMIT:
        print(f"      âŒ GPT í˜¸ì¶œ ì œí•œ ë„ë‹¬")
        return word, gpt_cache, gpt_call_count, token_usage

    # ìºì‹œ í™•ì¸
    cache_key = (word, is_phrase)
    if cache_key in gpt_cache:
        return gpt_cache[cache_key], gpt_cache, gpt_call_count, token_usage

    # GPT í˜¸ì¶œ
    if custom_prompt:
        prompt = custom_prompt
    else:
        # ë¬¸ë§¥ ì •ë³´ê°€ ìˆìœ¼ë©´ ë¬¸ë§¥ íŠ¹í™” í”„ë¡¬í”„íŠ¸
        if sentence and len(sentence.strip()) > 10:
            prompt = f"""Analyze the word "{word}" in this specific context and provide the Korean meaning that fits THIS usage.

Context sentence: "{sentence}"

Provide the Korean meaning for how "{word}" is used in THIS specific context:
- Focus on the meaning in this particular sentence
- Not the general dictionary meaning
- Korean translation should match this specific usage

Korean meaning for this context:"""
        else:
            # ê¸°ì¡´ ì‚¬ì „ì  ì˜ë¯¸ í”„ë¡¬í”„íŠ¸ ìœ ì§€
            prompt = f"""Provide the accurate Korean meaning of the following English {'idiom' if is_phrase else 'word'}.
            
    English: "{word}"
    Korean Translation:"""

        system_message = "You are an expert English-Korean translator. Return only Korean meanings without examples."

    # GPT í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt},
            ],
            temperature=0.1,
            max_tokens=50,
        )
        answer = response.choices[0].message.content.strip().replace('"', "")
        gpt_call_count += 1

        # í† í° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
        if hasattr(response, "usage"):
            usage = response.usage
            token_usage["prompt_tokens"] += usage.prompt_tokens
            token_usage["completion_tokens"] += usage.completion_tokens
            token_usage["total_tokens"] += usage.total_tokens

        gpt_cache[cache_key] = answer
        return answer, gpt_cache, gpt_call_count, token_usage

    except Exception as e:
        print("GPT í˜¸ì¶œ ì‹¤íŒ¨:", e)
        return word, gpt_cache, gpt_call_count, token_usage


# ğŸ”¥ vocab_difficulty.py í•µì‹¬ í•¨ìˆ˜ë“¤ ì§ì ‘ í†µí•©
def integrated_extract_info(word, pos=None):
    """vocab_difficulty.pyì˜ extract_info í†µí•© ë²„ì „"""
    definition = ""
    synonyms = []
    antonyms = []

    # í’ˆì‚¬ íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì¶”ë¡ 
    if not pos:
        try:
            from nltk import pos_tag

            tagged = pos_tag([word])
            pos = integrated_get_wordnet_pos(tagged[0][1]) if tagged else None
        except:
            pos = None

    # WordNetì—ì„œ ì •ë³´ ì¶”ì¶œ
    synsets = wordnet.synsets(word, pos=pos) if pos else wordnet.synsets(word)

    if synsets:
        # ì²« ë²ˆì§¸ ë™ì˜ì–´ ì§‘í•©ì˜ ì •ì˜ ì‚¬ìš©
        definition = synsets[0].definition()

        # ë™ì˜ì–´ ìˆ˜ì§‘
        for synset in synsets:
            for lemma in synset.lemmas():
                if lemma.name().lower() != word.lower():
                    synonyms.append(lemma.name().replace("_", " "))

                # ë°˜ì˜ì–´ ìˆ˜ì§‘
                for antonym in lemma.antonyms():
                    antonyms.append(antonym.name().replace("_", " "))

    # ì¤‘ë³µ ì œê±°
    synonyms = list(set(synonyms))[:5]
    antonyms = list(set(antonyms))[:5]

    return definition, synonyms, antonyms


def integrated_get_wordnet_pos(tag):
    """vocab_difficulty.pyì˜ get_wordnet_pos í†µí•© ë²„ì „"""
    if tag.startswith("J"):
        return wordnet.ADJ
    elif tag.startswith("V"):
        return wordnet.VERB
    elif tag.startswith("N"):
        return wordnet.NOUN
    elif tag.startswith("R"):
        return wordnet.ADV
    else:
        return None


def integrated_calculate_phonetic_complexity(word):
    """vocab_difficulty.pyì˜ calculate_phonetic_complexity í†µí•© ë²„ì „"""
    vowels = set("aeiou")
    consonants = set("bcdfghjklmnpqrstvwxyz")
    complexity = 0
    consonant_clusters = 0
    prev_consonant = False

    for char in word.lower():
        if char in consonants:
            if prev_consonant:
                consonant_clusters += 1
            prev_consonant = True
        else:
            prev_consonant = False

    complexity += consonant_clusters * 0.5

    rare_combinations = ["ph", "th", "ch", "sh", "gh", "ck", "ng", "qu"]
    for combo in rare_combinations:
        if combo in word.lower():
            complexity += 0.3

    return complexity


def integrated_get_word_difficulty_score(word, nlp_model=None):
    """vocab_difficulty.pyì˜ get_word_difficulty_score í†µí•© ë²„ì „"""
    try:
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        morphological_complexity = 0

        if nlp_model:
            doc = nlp_model(word)
            for token in doc:
                if token.lemma_ != token.text:
                    morphological_complexity += 1
                if token.pos_ in ["NOUN", "VERB"]:
                    morphological_complexity += 0.5
                elif token.pos_ in ["ADJ", "ADV"]:
                    morphological_complexity += 1

        # WordNet ê¸°ë°˜ ì˜ë¯¸ ë³µì¡ë„
        synsets = wordnet.synsets(word)
        semantic_complexity = len(synsets) * 0.1

        # ìŒì„±ì  ë³µì¡ë„
        phonetic_complexity = integrated_calculate_phonetic_complexity(word)

        # ì´ ì ìˆ˜ ê³„ì‚°
        total_score = (
            morphological_complexity * 0.4
            + semantic_complexity * 0.3
            + phonetic_complexity * 0.3
        )

        return total_score
    except:
        return 0.5  # ê¸°ë³¸ê°’


def integrated_is_difficult_word(
    word,
    easy_words,
    children_vocab,
    frequency_tiers=None,
    nlp_model=None,
    threshold=2.8,
):
    """vocab_difficulty.pyì˜ is_difficult_word í†µí•© ë²„ì „"""
    min_length = 4

    if not word or len(word) < min_length:
        return False

    if not word.isalpha():
        return False

    # ì‰¬ìš´ ë‹¨ì–´ ì²´í¬
    if word.lower() in easy_words:
        return False

    if children_vocab and word.lower() in children_vocab:
        return False

    # WordNet ê¸°ë°˜ ê¸°ë³¸ íŒë³„
    synsets = wordnet.synsets(word)
    if not synsets:
        return True  # ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ëŠ” ì–´ë ¤ìš´ ê²ƒìœ¼ë¡œ ê°„ì£¼

    # ë‚œì´ë„ ì ìˆ˜ ê¸°ë°˜ íŒë³„
    difficulty_score = integrated_get_word_difficulty_score(word, nlp_model)
    return difficulty_score > 0.6


def load_custom_idioms_from_data_directory(data_dir: str = "data") -> list:
    """data ë””ë ‰í† ë¦¬ì—ì„œ ìˆ™ì–´ ë¡œë“œ"""
    idioms = set()
    loading_summary = []

    print(f"ğŸ“ data ë””ë ‰í† ë¦¬ ìˆ™ì–´ ë¡œë”© ì‹œì‘: {data_dir}")

    if not os.path.exists(data_dir):
        print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_dir}")
        return []

    file_patterns = ["*.csv", "*.txt", "*.xlsx", "*.xls"]
    all_files = []
    for pattern in file_patterns:
        all_files.extend(glob.glob(os.path.join(data_dir, pattern)))

    print(f"ğŸ” ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(all_files)}ê°œ")

    for file_path in all_files:
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(filename)[1].lower()
        file_idioms_count = 0

        try:
            print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}")

            if file_ext == ".csv":
                df = pd.read_csv(file_path, encoding="utf-8")
                possible_columns = [
                    "phrase",
                    "idiom",
                    "expression",
                    "text",
                    "content",
                    "ì›í˜•",
                ]
                target_column = next(
                    (
                        col
                        for col in df.columns
                        if col.lower() in [c.lower() for c in possible_columns]
                    ),
                    df.columns[0] if len(df.columns) > 0 else None,
                )

                if target_column and target_column in df.columns:
                    phrases = (
                        df[target_column].dropna().astype(str).str.strip().str.lower()
                    )
                    phrases = phrases[phrases != ""].unique()
                    valid_phrases = [
                        phrase
                        for phrase in phrases
                        if 2 <= len(phrase.split()) <= 8
                        and len(phrase) <= 100
                        and phrase.replace(" ", "").replace("-", "").isalpha()
                    ]
                    idioms.update(valid_phrases)
                    file_idioms_count = len(valid_phrases)
                    print(
                        f"   âœ… CSV: {file_idioms_count}ê°œ ì¶”ì¶œ (ì»¬ëŸ¼: {target_column})"
                    )

            elif file_ext == ".txt":
                encodings = ["utf-8", "cp949", "euc-kr", "latin1"]
                for encoding in encodings:
                    try:
                        with open(file_path, "r", encoding=encoding) as f:
                            lines = f.readlines()
                        phrases = [
                            line.strip().lower()
                            for line in lines
                            if line.strip()
                            and not line.startswith(("#", "//"))
                            and 2 <= len(line.split()) <= 8
                            and len(line) <= 100
                        ]
                        idioms.update(phrases)
                        file_idioms_count = len(phrases)
                        print(
                            f"   âœ… TXT: {file_idioms_count}ê°œ ì¶”ì¶œ (ì¸ì½”ë”©: {encoding})"
                        )
                        break
                    except UnicodeDecodeError:
                        continue

            elif file_ext in [".xlsx", ".xls"]:
                excel_file = pd.ExcelFile(file_path)
                total_phrases = []
                for sheet_name in excel_file.sheet_names:
                    try:
                        df = pd.read_excel(file_path, sheet_name=sheet_name)
                        possible_columns = [
                            "phrase",
                            "idiom",
                            "expression",
                            "text",
                            "content",
                            "ì›í˜•",
                        ]
                        target_column = next(
                            (
                                col
                                for col in df.columns
                                if col.lower() in [c.lower() for c in possible_columns]
                            ),
                            df.columns[0] if len(df.columns) > 0 else None,
                        )
                        if target_column and target_column in df.columns:
                            phrases = (
                                df[target_column]
                                .dropna()
                                .astype(str)
                                .str.strip()
                                .str.lower()
                            )
                            phrases = phrases[phrases != ""].unique()
                            valid_phrases = [
                                phrase
                                for phrase in phrases
                                if 2 <= len(phrase.split()) <= 8
                                and len(phrase) <= 100
                                and phrase.replace(" ", "").replace("-", "").isalpha()
                            ]
                            total_phrases.extend(valid_phrases)
                            print(f"      ğŸ“‹ {sheet_name}: {len(valid_phrases)}ê°œ")
                    except Exception as e:
                        print(f"      âŒ {sheet_name} ì‹œíŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                idioms.update(total_phrases)
                file_idioms_count = len(total_phrases)
                print(f"   âœ… Excel: ì´ {file_idioms_count}ê°œ ì¶”ì¶œ")

            loading_summary.append(
                {
                    "file": filename,
                    "type": file_ext,
                    "loaded_count": file_idioms_count,
                    "status": "success" if file_idioms_count > 0 else "empty",
                }
            )

        except Exception as e:
            print(f"   âŒ {filename} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            loading_summary.append(
                {
                    "file": filename,
                    "type": file_ext,
                    "loaded_count": 0,
                    "status": "failed",
                    "error": str(e),
                }
            )

    final_idioms = sorted(set(idioms))
    print(f"\nğŸ“Š data ë””ë ‰í† ë¦¬ ë¡œë”© ì™„ë£Œ: ì´ {len(final_idioms)}ê°œ ìˆ™ì–´")
    return final_idioms


class SafeCacheManager:
    """Windows í˜¸í™˜ ê°„ë‹¨í•œ ìºì‹œ ê´€ë¦¬ì"""

    def __init__(self, cache_dir: str = "cache", app_name: str = "vocab_extractor"):
        self.cache_dir = cache_dir
        self.app_name = app_name
        os.makedirs(cache_dir, exist_ok=True)

    def get_cache_filename(self, cache_type: str, identifier: str = "") -> str:
        """ìºì‹œ íŒŒì¼ëª… ìƒì„±"""
        if identifier:
            import hashlib

            hash_id = hashlib.md5(identifier.encode()).hexdigest()[:8]
            filename = f"{self.app_name}_{cache_type}_{hash_id}.json"
        else:
            filename = f"{self.app_name}_{cache_type}.json"
        return os.path.join(self.cache_dir, filename)

    def load_cache(self, cache_type: str, identifier: str = "") -> Dict[str, Any]:
        """ìºì‹œ ë¡œë“œ"""
        cache_file = self.get_cache_filename(cache_type, identifier)
        try:
            if os.path.exists(cache_file):
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)

                # ê°„ë‹¨í•œ ìœ íš¨ì„± ê²€ì‚¬
                if isinstance(cache_data, dict) and "data" in cache_data:
                    print(f"âœ… ìºì‹œ ë¡œë“œ ì„±ê³µ: {cache_type}")
                    return cache_data["data"]
                elif isinstance(cache_data, dict):
                    return cache_data
            return {}
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ({cache_type}): {e}")
            return {}

    def save_cache(self, cache_type: str, data: Dict[str, Any], identifier: str = ""):
        """ìºì‹œ ì €ì¥"""
        cache_file = self.get_cache_filename(cache_type, identifier)
        try:
            cache_data = {
                "metadata": {
                    "created_at": time.time(),
                    "app_name": self.app_name,
                    "cache_type": cache_type,
                },
                "data": data,
            }

            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… ìºì‹œ ì €ì¥ ì„±ê³µ: {cache_type}")

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ ({cache_type}): {e}")

    def clear_cache(self, cache_type: str = None):
        """ìºì‹œ ì‚­ì œ"""
        try:
            if cache_type:
                cache_file = self.get_cache_filename(cache_type)
                if os.path.exists(cache_file):
                    os.remove(cache_file)
                    print(f"âœ… ìºì‹œ ì‚­ì œ: {cache_type}")
            else:
                for filename in os.listdir(self.cache_dir):
                    if filename.startswith(self.app_name):
                        os.remove(os.path.join(self.cache_dir, filename))
                print(f"âœ… ëª¨ë“  ìºì‹œ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")


# í†µí•© ìºì‹œ ê´€ë¦¬ì (ë‘ íŒŒì¼ì—ì„œ ê³µí†µ ì‚¬ìš©)
class UnifiedCacheManager:
    """improved_vocab_extractor.pyì™€ quality_checker.pyì—ì„œ ê³µí†µ ì‚¬ìš©í•  ìºì‹œ ê´€ë¦¬ì"""

    def __init__(self):
        self.cache_manager = SafeCacheManager(
            cache_dir="cache", app_name="vocab_system"
        )

        # ìºì‹œ íƒ€ì… ì •ì˜
        self.CACHE_TYPES = {
            "gpt_difficulty": "gpt_difficulty",
            "gpt_quality": "gpt_quality",
            "gpt_contextual": "gpt_contextual",
            "separable_analysis": "separable_analysis",
            "user_words": "user_words",
            "easy_words": "easy_words",
        }

    def load_gpt_cache(
        self, cache_type: str, user_identifier: str = ""
    ) -> Dict[str, Any]:
        """GPT ìºì‹œ ë¡œë“œ (íƒ€ì…ë³„)"""
        return self.cache_manager.load_cache(cache_type, user_identifier)

    def save_gpt_cache(
        self, cache_type: str, data: Dict[str, Any], user_identifier: str = ""
    ):
        """GPT ìºì‹œ ì €ì¥ (íƒ€ì…ë³„)"""
        self.cache_manager.save_cache(cache_type, data, user_identifier)

    def merge_caches(
        self, cache_type: str, new_data: Dict[str, Any], user_identifier: str = ""
    ):
        """ê¸°ì¡´ ìºì‹œì™€ ìƒˆ ë°ì´í„° ë³‘í•©"""
        try:
            existing_cache = self.load_gpt_cache(cache_type, user_identifier)

            # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ìƒˆ ë°ì´í„°ê°€ ìš°ì„ )
            merged_data = {**existing_cache, **new_data}

            self.save_gpt_cache(cache_type, merged_data, user_identifier)

            print(
                f"âœ… ìºì‹œ ë³‘í•© ì™„ë£Œ: {cache_type} (ê¸°ì¡´ {len(existing_cache)}ê°œ + ì‹ ê·œ {len(new_data)}ê°œ = ì´ {len(merged_data)}ê°œ)"
            )

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë³‘í•© ì‹¤íŒ¨ ({cache_type}): {e}")

    def cleanup_old_caches(self, days_old: int = 7):
        """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
        try:
            from datetime import datetime, timedelta

            cutoff_date = datetime.now() - timedelta(days=days_old)
            removed_count = 0

            for filename in os.listdir(self.cache_manager.cache_dir):
                if filename.startswith(
                    self.cache_manager.app_name
                ) and filename.endswith(".json"):
                    filepath = os.path.join(self.cache_manager.cache_dir, filename)
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(filepath))

                    if file_mtime < cutoff_date:
                        os.remove(filepath)
                        removed_count += 1

            if removed_count > 0:
                print(f"âœ… ì˜¤ë˜ëœ ìºì‹œ {removed_count}ê°œ ì‚­ì œ ì™„ë£Œ ({days_old}ì¼ ì´ìƒ)")

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_unified_stats(self) -> Dict[str, Any]:
        """í†µí•© ìºì‹œ í†µê³„"""
        return self.cache_manager.get_cache_stats()


# ì „ì—­ ìºì‹œ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
_global_cache_manager = None


def get_cache_manager() -> UnifiedCacheManager:
    """ì „ì—­ ìºì‹œ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _global_cache_manager
    if _global_cache_manager is None:
        _global_cache_manager = UnifiedCacheManager()
    return _global_cache_manager


class SeparableIdiomDetector:
    """ì‚¬ìš©ì DBì˜ ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ê¸° (OpenAI í™œìš©)"""

    def __init__(self, client, verbose=False):
        self.client = client
        self.verbose = verbose
        self.separable_cache = {}  # GPT í˜¸ì¶œ ê²°ê³¼ ìºì‹±
        self.gpt_calls = 0

        # ë¶„ë¦¬í˜• ìˆ™ì–´ ë°ì´í„°ë² ì´ìŠ¤
        self.user_separable_idioms = {}  # {ì›ë³¸ìˆ™ì–´: ë¶„ë¦¬í˜•ì •ë³´}
        self.separable_patterns = {}  # {ì›ë³¸ìˆ™ì–´: [íŒ¨í„´ë“¤]}

        print("ğŸ”§ ì‚¬ìš©ì DB ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_user_idioms_with_gpt(self, user_idioms: Set[str]) -> Dict[str, Dict]:
        """GPTë¡œ ì‚¬ìš©ì DB ìˆ™ì–´ë“¤ì˜ ë¶„ë¦¬í˜• ê°€ëŠ¥ì„± ë¶„ì„"""

        print(f"ğŸ¤– GPTë¡œ ì‚¬ìš©ì ìˆ™ì–´ ë¶„ë¦¬í˜• ë¶„ì„ ì‹œì‘: {len(user_idioms)}ê°œ")

        results = {}

        # ë„ì–´ì“°ê¸°ê°€ ìˆëŠ” ìˆ™ì–´ë“¤ë§Œ ë¶„ì„
        potential_idioms = [
            idiom for idiom in user_idioms if " " in idiom and len(idiom.split()) == 2
        ]

        print(f"   ğŸ“‹ ë¶„ì„ ëŒ€ìƒ 2ë‹¨ì–´ ìˆ™ì–´: {len(potential_idioms)}ê°œ")

        for idiom in potential_idioms:
            if idiom in self.separable_cache:
                results[idiom] = self.separable_cache[idiom]
                continue

            try:
                analysis = self._gpt_analyze_separable_idiom(idiom)
                results[idiom] = analysis
                self.separable_cache[idiom] = analysis

                if self.verbose and analysis.get("is_separable", False):
                    display = analysis.get("display_form", idiom)
                    print(f"      âœ… ë¶„ë¦¬í˜• í™•ì¸: {idiom} â†’ {display}")

            except Exception as e:
                if self.verbose:
                    print(f"      âŒ GPT ë¶„ì„ ì‹¤íŒ¨ ({idiom}): {e}")
                continue

        # ë¶„ë¦¬í˜• ìˆ™ì–´ë§Œ í•„í„°ë§
        separable_count = len(
            [r for r in results.values() if r.get("is_separable", False)]
        )
        print(f"   ğŸ¯ ë¶„ë¦¬í˜• ìˆ™ì–´ ë°œê²¬: {separable_count}ê°œ")

        return results

    def _gpt_analyze_separable_idiom(self, idiom: str) -> Dict:
        """GPTë¡œ ê°œë³„ ìˆ™ì–´ì˜ ë¶„ë¦¬í˜• ì—¬ë¶€ ë¶„ì„"""

        words = idiom.split()
        if len(words) != 2:
            return {"is_separable": False, "reason": "Not a two-word phrase"}

        verb, particle = words[0], words[1]

        prompt = f"""
Analyze the English phrasal verb "{idiom}":

Please determine if this is a separable phrasal verb.

Characteristics of separable phrasal verbs:
1. Structure: verb + particle (up, down, on, off, out, in, away, back, etc.)
2. When there's an object, it can be placed between the verb and particle
   Example: pick up â†’ pick something up, pick it up
3. Pronoun objects (it, him, her, them) MUST be placed between verb and particle
   Example: pick it up (âœ“), pick up it (âœ—)

Analyze "{idiom}":
- Is "{verb}" a verb?
- Is "{particle}" a particle?
- Can an object be placed between them?
- Is this actually used as a separable phrasal verb in real English?

Please respond in JSON format:
{{
    "is_separable": true/false,
    "verb": "{verb}",
    "particle": "{particle}", 
    "display_form": "if separable, show as 'verb ~ particle' format",
    "examples": ["examples of separable usage"],
    "reason": "reasoning for the decision",
    "confidence": 0.0-1.0
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in English phrasal verbs. You can accurately identify separable phrasal verbs and their usage patterns.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=300,
                temperature=0.1,
            )

            self.gpt_calls += 1
            content = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            result = json.loads(content)

            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„
            if not isinstance(result.get("is_separable"), bool):
                result["is_separable"] = False

            if result["is_separable"] and not result.get("display_form"):
                result["display_form"] = f"{verb} ~ {particle}"

            return result

        except Exception as e:
            if self.verbose:
                print(f"GPT ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "is_separable": False,
                "reason": f"GPT ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                "confidence": 0.0,
            }

    def build_separable_patterns(self, separable_analysis: Dict[str, Dict]):
        """ë¶„ë¦¬í˜• ìˆ™ì–´ë“¤ì˜ ê°ì§€ íŒ¨í„´ ìƒì„±"""

        print(f"ğŸ”§ ë¶„ë¦¬í˜• ìˆ™ì–´ íŒ¨í„´ ìƒì„± ì¤‘...")

        for idiom, analysis in separable_analysis.items():
            if not analysis.get("is_separable", False):
                continue

            words = idiom.split()
            if len(words) != 2:
                continue

            verb, particle = words[0], words[1]
            display_form = analysis.get("display_form", f"{verb} ~ {particle}")

            # ë‹¤ì–‘í•œ ë¶„ë¦¬í˜• íŒ¨í„´ë“¤ ìƒì„±
            patterns = self._generate_detection_patterns(verb, particle)

            self.user_separable_idioms[idiom] = {
                "display_form": display_form,
                "verb": verb,
                "particle": particle,
                "confidence": analysis.get("confidence", 0.9),
                "examples": analysis.get("examples", []),
                "reason": analysis.get("reason", ""),
            }

            self.separable_patterns[idiom] = patterns

            if self.verbose:
                print(f"   âœ… {idiom} â†’ {display_form} ({len(patterns)}ê°œ íŒ¨í„´)")

        print(f"âœ… ë¶„ë¦¬í˜• íŒ¨í„´ ìƒì„± ì™„ë£Œ: {len(self.user_separable_idioms)}ê°œ ìˆ™ì–´")

    def _generate_detection_patterns(self, verb: str, particle: str) -> List[Dict]:
        """ê°œë³„ ë¶„ë¦¬í˜• ìˆ™ì–´ì˜ ê°ì§€ íŒ¨í„´ë“¤ ìƒì„±"""

        patterns = []

        # 1. ì—°ì†í˜• (ê¸°ë³¸í˜•)
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+{re.escape(particle)}\b",
                "type": "continuous",
                "description": "ì—°ì†í˜•",
                "is_separated": False,
                "priority": 1,
            }
        )

        # 2. ë¶„ë¦¬í˜• - ì¼ë°˜ ëª…ì‚¬/ëª…ì‚¬êµ¬
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+(?:the\s+|a\s+|an\s+|this\s+|that\s+|his\s+|her\s+|my\s+|your\s+|our\s+|their\s+)?(?:\w+\s+)*\w+\s+{re.escape(particle)}\b",
                "type": "separated_noun",
                "description": "ë¶„ë¦¬í˜•(ëª…ì‚¬)",
                "is_separated": True,
                "priority": 2,
            }
        )

        # 3. ë¶„ë¦¬í˜• - ëŒ€ëª…ì‚¬ (ë°˜ë“œì‹œ ë¶„ë¦¬)
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+(?:it|him|her|them|this|that|these|those)\s+{re.escape(particle)}\b",
                "type": "separated_pronoun",
                "description": "ë¶„ë¦¬í˜•(ëŒ€ëª…ì‚¬)",
                "is_separated": True,
                "priority": 3,
            }
        )

        # 4. ë¶„ë¦¬í˜• - ê¸´ ëª…ì‚¬êµ¬
        patterns.append(
            {
                "pattern": rf"\b{re.escape(verb)}\s+(?:the\s+)?(?:\w+\s+){{2,}}\w+\s+{re.escape(particle)}\b",
                "type": "separated_long_noun",
                "description": "ë¶„ë¦¬í˜•(ê¸´ëª…ì‚¬êµ¬)",
                "is_separated": True,
                "priority": 2,
            }
        )

        return patterns

    def detect_separable_idioms_in_text(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©ì DB ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€"""

        if not self.user_separable_idioms:
            return []

        results = []
        found_positions = set()

        if self.verbose:
            print(f"ğŸ” ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ ì¤‘: {len(self.user_separable_idioms)}ê°œ ëŒ€ìƒ")

        # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ ì •ë ¬ (ë¶„ë¦¬í˜•ì„ ë¨¼ì € ì°¾ì•„ì„œ ì—°ì†í˜•ê³¼ ì¤‘ë³µ ë°©ì§€)
        for idiom, info in self.user_separable_idioms.items():
            patterns = self.separable_patterns.get(idiom, [])

            # ìš°ì„ ìˆœìœ„ìˆœìœ¼ë¡œ íŒ¨í„´ ì •ë ¬ (ë¶„ë¦¬í˜• ë¨¼ì €)
            sorted_patterns = sorted(
                patterns, key=lambda x: x["priority"], reverse=True
            )

            for pattern_info in sorted_patterns:
                pattern = pattern_info["pattern"]

                matches = re.finditer(pattern, text, re.IGNORECASE)

                for match in matches:
                    start, end = match.span()

                    # ì¤‘ë³µ ìœ„ì¹˜ í™•ì¸
                    if any(abs(start - pos[0]) <= 5 for pos in found_positions):
                        continue

                    matched_text = match.group().strip()

                    # ê²°ê³¼ ìƒì„±
                    result = {
                        "original": matched_text,
                        "base_form": idiom,
                        "display_form": info["display_form"],
                        "pattern_type": pattern_info["type"],
                        "description": pattern_info["description"],
                        "is_separated": pattern_info["is_separated"],
                        "confidence": info["confidence"],
                        "start": start,
                        "end": end,
                        "separable_info": {
                            "verb": info["verb"],
                            "particle": info["particle"],
                            "detection_pattern": pattern,
                            "gpt_reason": info.get("reason", ""),
                            "examples": info.get("examples", []),
                        },
                    }

                    results.append(result)
                    found_positions.add((start, end))

                    if self.verbose:
                        sep_mark = "ğŸ”§" if result["is_separated"] else "ğŸ“"
                        print(
                            f"      {sep_mark} ë°œê²¬: '{matched_text}' â†’ {info['display_form']} ({pattern_info['description']})"
                        )

                    # ê°™ì€ ìˆ™ì–´ì˜ ë‹¤ë¥¸ íŒ¨í„´ì€ ìŠ¤í‚µ (ì²« ë²ˆì§¸ ë§¤ì¹˜ë§Œ)
                    break

        return results

    def integrate_with_extractor(self, extractor_instance):
        """ê¸°ì¡´ AdvancedVocabExtractorì™€ í†µí•©"""

        print(f"ğŸ”— ê¸°ì¡´ extractorì™€ ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ê¸° í†µí•©...")

        # ê¸°ì¡´ ì‚¬ìš©ì ìˆ™ì–´ë“¤ ë¶„ì„
        if hasattr(extractor_instance, "user_idioms"):
            separable_analysis = self.analyze_user_idioms_with_gpt(
                extractor_instance.user_idioms
            )
            self.build_separable_patterns(separable_analysis)

            # extractorì— ë¶„ë¦¬í˜• ì •ë³´ ì¶”ê°€
            extractor_instance.separable_detector = self
            extractor_instance.user_separable_idioms = self.user_separable_idioms

            print(
                f"âœ… í†µí•© ì™„ë£Œ: {len(self.user_separable_idioms)}ê°œ ë¶„ë¦¬í˜• ìˆ™ì–´ í™œì„±í™”"
            )
        else:
            print("âš ï¸ extractorì— user_idiomsê°€ ì—†ìŠµë‹ˆë‹¤")

    def save_separable_analysis(self, output_file: str = "separable_analysis.json"):
        """ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ì €ì¥"""

        analysis_data = {
            "separable_idioms": self.user_separable_idioms,
            "total_count": len(self.user_separable_idioms),
            "gpt_calls": self.gpt_calls,
            "cache": self.separable_cache,
        }

        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_separable_analysis(self, input_file: str = "separable_analysis.json"):
        """ê¸°ì¡´ ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""

        try:
            with open(input_file, "r", encoding="utf-8") as f:
                analysis_data = json.load(f)

            self.user_separable_idioms = analysis_data.get("separable_idioms", {})
            self.separable_cache = analysis_data.get("cache", {})

            # íŒ¨í„´ ì¬ìƒì„±
            for idiom, info in self.user_separable_idioms.items():
                verb = info.get("verb", "")
                particle = info.get("particle", "")
                if verb and particle:
                    patterns = self._generate_detection_patterns(verb, particle)
                    self.separable_patterns[idiom] = patterns

            print(f"âœ… ë¶„ë¦¬í˜• ë¶„ì„ ê²°ê³¼ ë¡œë“œ: {len(self.user_separable_idioms)}ê°œ")
            return True

        except FileNotFoundError:
            print(f"ğŸ“‚ {input_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


class ExternalVocabDatabase:
    def __init__(self):
        self.oxford_3000 = self._load_oxford_3000()
        self.coca_2000 = self._load_coca_frequent()
        self.gsl_2000 = self._load_general_service_list()

    def _load_oxford_3000(self):
        """Oxford 3000 ê¸°ë³¸ ì–´íœ˜ ë¡œë“œ"""
        try:
            # ë¡œì»¬ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¸íŠ¸ ë°˜í™˜
            oxford_file = "data/oxford_3000.txt"
            if os.path.exists(oxford_file):
                with open(oxford_file, "r", encoding="utf-8") as f:
                    return set(line.strip().lower() for line in f if line.strip())
            else:
                # ê¸°ë³¸ Oxford ê¸°ì´ˆ ë‹¨ì–´ë“¤
                return {
                    "about",
                    "above",
                    "across",
                    "act",
                    "active",
                    "activity",
                    "add",
                    "afraid",
                    "after",
                    "again",
                    "against",
                    "age",
                    "ago",
                    "agree",
                    "air",
                    "all",
                    "alone",
                    "along",
                    "already",
                    "although",
                    "always",
                    "among",
                    "angry",
                    "animal",
                    "answer",
                    "any",
                    "anyone",
                    "anything",
                    "appear",
                    "area",
                    "argue",
                    "arm",
                    "army",
                    "around",
                    "arrive",
                    "art",
                    "article",
                    "ask",
                    "attack",
                    "aunt",
                    "autumn",
                    "away",
                }
        except:
            return set()

    def _load_coca_frequent(self):
        """COCA ìµœê³ ë¹ˆë„ 2000ê°œ ë‹¨ì–´ ë¡œë“œ"""
        try:
            coca_file = "data/coca_2000.txt"
            if os.path.exists(coca_file):
                with open(coca_file, "r", encoding="utf-8") as f:
                    return set(line.strip().lower() for line in f if line.strip())
            else:
                # COCA ê¸°ë³¸ ê³ ë¹ˆë„ ë‹¨ì–´ë“¤
                return {
                    "the",
                    "be",
                    "to",
                    "of",
                    "and",
                    "a",
                    "in",
                    "that",
                    "have",
                    "i",
                    "it",
                    "for",
                    "not",
                    "on",
                    "with",
                    "he",
                    "as",
                    "you",
                    "do",
                    "at",
                    "this",
                    "but",
                    "his",
                    "by",
                    "from",
                    "they",
                    "she",
                    "or",
                    "an",
                    "will",
                    "my",
                    "one",
                    "all",
                    "would",
                    "there",
                    "their",
                    "what",
                    "so",
                    "up",
                    "out",
                    "if",
                    "about",
                    "who",
                    "get",
                    "which",
                    "go",
                    "me",
                    "when",
                    "make",
                    "can",
                    "like",
                    "time",
                    "no",
                    "just",
                    "him",
                    "know",
                    "take",
                    "people",
                    "into",
                    "year",
                    "your",
                    "good",
                    "some",
                    "could",
                    "them",
                    "see",
                    "other",
                    "than",
                    "then",
                    "now",
                    "look",
                    "only",
                    "come",
                    "its",
                    "over",
                    "think",
                    "also",
                    "back",
                    "after",
                    "use",
                    "two",
                    "how",
                    "our",
                    "work",
                    "first",
                    "well",
                    "way",
                    "even",
                    "new",
                    "want",
                    "because",
                }
        except:
            return set()

    def _load_general_service_list(self):
        """General Service List ê¸°ë³¸ 2000ë‹¨ì–´ ë¡œë“œ"""
        try:
            gsl_file = "data/gsl_2000.txt"
            if os.path.exists(gsl_file):
                with open(gsl_file, "r", encoding="utf-8") as f:
                    return set(line.strip().lower() for line in f if line.strip())
            else:
                # GSL ê¸°ë³¸ ë‹¨ì–´ë“¤ (ì¼ë¶€)
                return (
                    BASIC_VERBS.union(BASIC_ADJECTIVES)
                    .union(BASIC_NOUNS)
                    .union(
                        {
                            "the",
                            "be",
                            "to",
                            "of",
                            "and",
                            "a",
                            "in",
                            "that",
                            "have",
                            "i",
                            "it",
                            "for",
                            "not",
                            "on",
                            "with",
                            "he",
                            "as",
                            "you",
                            "do",
                            "at",
                            "this",
                            "but",
                            "his",
                            "by",
                        }
                    )
                )
        except:
            return set()

    def is_basic_word(self, word):
        """ì™¸ë¶€ DB ê¸°ë°˜ ê¸°ë³¸ ë‹¨ì–´ íŒë³„"""
        word_lower = word.lower()
        return (
            word_lower in self.oxford_3000
            or word_lower in self.coca_2000
            or word_lower in self.gsl_2000
        )


def is_basic_by_external_db(word):
    """ì™¸ë¶€ ì–´íœ˜ DB í™œìš©í•œ ê¸°ë³¸ ë‹¨ì–´ íŒë³„"""
    if not hasattr(is_basic_by_external_db, "db"):
        is_basic_by_external_db.db = ExternalVocabDatabase()
    return is_basic_by_external_db.db.is_basic_word(word)


class AdvancedIdiomChecker:
    """ê³ ê¸‰ ìˆ™ì–´ ê²€ì¦ê¸° - ë¶„ë¦¬í˜•ê³¼ ë¬¸ë²• íŒ¨í„´ êµ¬ë¶„"""

    def __init__(self, nlp_model):
        self.nlp = nlp_model

        # ğŸ”¥ ì—°ì†í˜• ê°€ëŠ¥ êµ¬ë™ì‚¬ (ë¶™ì—¬ì„œ ì¨ë„ OK)
        self.optional_separable = {
            "pick up",
            "turn on",
            "turn off",
            "look up",
            "put on",
            "take off",
            "bring up",
            "call up",
            "give up",
            "set up",
            "clean up",
            "fill up",
            "write down",
            "sit down",
            "stand up",
            "wake up",
            "get up",
        }

        # ğŸ”¥ ë°˜ë“œì‹œ ë¶„ë¦¬ë˜ì–´ì•¼ í•˜ëŠ” êµ¬ë™ì‚¬ (ëª©ì ì–´ í•„ìˆ˜)
        self.mandatory_separable = {
            "pick": ["up", "out", "off"],
            "turn": ["down", "up"],
            "put": ["away", "back"],
            "take": ["apart", "down"],
            "figure": ["out"],
            "work": ["out"],
            "point": ["out"],
            "carry": ["out"],
            "bring": ["about"],
        }

        # ğŸ”¥ ë¬¸ë²• íŒ¨í„´ ìˆ™ì–´ë“¤ (íŠ¹ì • í’ˆì‚¬ í•„ìˆ˜)
        self.grammar_patterns = {
            # V-ing íŒ¨í„´
            r"\bspend\s+(?:time|money|hours?|days?|years?)\s+(\w+ing)\b": "spend time V-ing",
            r"\bis\s+worth\s+(\w+ing)\b": "be worth V-ing",
            r"\bkeep\s+(?:on\s+)?(\w+ing)\b": "keep V-ing",
            r"\bavoid\s+(\w+ing)\b": "avoid V-ing",
            r"\benjoy\s+(\w+ing)\b": "enjoy V-ing",
            r"\bfinish\s+(\w+ing)\b": "finish V-ing",
            # N + V-ing íŒ¨í„´
            r"\bprevent\s+(\w+)\s+from\s+(\w+ing)\b": "prevent N from V-ing",
            r"\bstop\s+(\w+)\s+from\s+(\w+ing)\b": "stop N from V-ing",
            # ê¸°íƒ€ íŒ¨í„´
            r"\bit\s+takes\s+(\w+)\s+to\s+(\w+)": "it takes N to V",
            r"\bthere\s+is\s+no\s+point\s+in\s+(\w+ing)\b": "there is no point in V-ing",
        }

        # ğŸ”¥ ì•Œë ¤ì§„ ì¼ë°˜ ìˆ™ì–´ íŒ¨í„´ë“¤
        self.known_phrasal_patterns = {
            r"\bas\s+\w+\s+as\b",
            r"\bin\s+order\s+to\b",
            r"\bas\s+a\s+result\b",
            r"\bon\s+the\s+other\s+hand\b",
            r"\bfor\s+instance\b",
            r"\bin\s+spite\s+of\b",
            r"\bbecause\s+of\b",
            r"\binstead\s+of\b",
            r"\baccording\s+to\b",
        }

    def analyze_phrasal_verb_pattern(self, text, verb_token, particle_token):
        """êµ¬ë™ì‚¬ íŒ¨í„´ ë¶„ì„ - ì—°ì†í˜• vs ë¶„ë¦¬í˜• vs ë¬¸ë²•íŒ¨í„´"""
        verb = verb_token.lemma_.lower()
        particle = particle_token.text.lower()
        base_phrasal = f"{verb} {particle}"

        # ğŸ”¥ 1. ì—°ì†í˜• ê°€ëŠ¥í•œ êµ¬ë™ì‚¬ì¸ì§€ í™•ì¸
        if base_phrasal in self.optional_separable:
            return {
                "pattern_type": "optional_separable",
                "base_form": base_phrasal,
                "display_form": base_phrasal,  # ê·¸ëƒ¥ ì—°ì†í˜•ìœ¼ë¡œ í‘œì‹œ
                "is_separated": False,
            }

        # ğŸ”¥ 2. ë°˜ë“œì‹œ ë¶„ë¦¬ë˜ì–´ì•¼ í•˜ëŠ” êµ¬ë™ì‚¬ì¸ì§€ í™•ì¸
        if (
            verb in self.mandatory_separable
            and particle in self.mandatory_separable[verb]
        ):
            return {
                "pattern_type": "mandatory_separable",
                "base_form": base_phrasal,
                "display_form": f"{verb} ~ {particle}",  # ~ ë¡œ í‘œì‹œ
                "is_separated": True,
            }

        # ğŸ”¥ 3. ì¼ë°˜ êµ¬ë™ì‚¬ (ì‹¤ì œ ë¶„ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸)
        verb_idx = verb_token.i
        particle_idx = particle_token.i

        # ë™ì‚¬ì™€ ì…ì ì‚¬ì´ì— ë‹¤ë¥¸ í† í°ì´ ìˆëŠ”ì§€ í™•ì¸
        if abs(verb_idx - particle_idx) > 1:
            # ì‹¤ì œë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŒ
            return {
                "pattern_type": "actually_separated",
                "base_form": base_phrasal,
                "display_form": f"{verb} ~ {particle}",
                "is_separated": True,
            }
        else:
            # ì—°ì†ìœ¼ë¡œ ë¶™ì–´ìˆìŒ
            return {
                "pattern_type": "continuous",
                "base_form": base_phrasal,
                "display_form": base_phrasal,
                "is_separated": False,
            }

    def analyze_grammar_pattern(self, text):
        """ë¬¸ë²• íŒ¨í„´ ë¶„ì„"""
        results = []

        for pattern_regex, pattern_name in self.grammar_patterns.items():
            matches = re.finditer(pattern_regex, text, re.IGNORECASE)
            for match in matches:
                start, end = match.span()
                original_text = match.group()

                # ë§¤ì¹­ëœ ê·¸ë£¹ë“¤ ë¶„ì„
                groups = match.groups()

                # V-ing íŒ¨í„´ ê²€ì¦
                if "V-ing" in pattern_name and groups:
                    # ë§ˆì§€ë§‰ ê·¸ë£¹ì´ ì‹¤ì œë¡œ ë™ëª…ì‚¬ì¸ì§€ í™•ì¸
                    last_word = groups[-1]
                    if last_word.endswith("ing"):
                        results.append(
                            {
                                "original": original_text,
                                "pattern_type": "grammar_pattern",
                                "display_form": pattern_name,
                                "base_form": pattern_name,
                                "start": start,
                                "end": end,
                                "is_separated": False,
                            }
                        )

                # ê¸°íƒ€ íŒ¨í„´
                else:
                    results.append(
                        {
                            "original": original_text,
                            "pattern_type": "grammar_pattern",
                            "display_form": pattern_name,
                            "base_form": pattern_name,
                            "start": start,
                            "end": end,
                            "is_separated": False,
                        }
                    )

        return results


class VocabularyQualityChecker:
    def __init__(self, vocabulary_file):
        self.vocabulary_file = vocabulary_file
        self.df = (
            pd.read_excel(vocabulary_file)
            if vocabulary_file.endswith(".xlsx")
            else pd.read_csv(vocabulary_file)
        )

    def generate_quality_report(self):
        issues = []
        if "ëœ»(í•œê¸€)" in self.df.columns:
            empty_meanings = self.df["ëœ»(í•œê¸€)"].isna().sum()
            issues.append(f"ì˜ë¯¸ ëˆ„ë½: {empty_meanings}ê°œ")

        if "ë‹¨ì–´" in self.df.columns:
            duplicate_words = self.df["ë‹¨ì–´"].duplicated().sum()
            issues.append(f"ì¤‘ë³µ ë‹¨ì–´: {duplicate_words}ê°œ")

        total_issues = len(issues)
        quality_score = max(0, 100 - total_issues * 10)

        return {
            "quality_score": quality_score,
            "total_issues": total_issues,
            "issues": issues,
            "discovered_patterns": 0,
        }

    def update_vocabulary_with_fixes(self, apply_high_confidence_fixes=True):
        fixed_df = self.df.copy()
        if "ëœ»(í•œê¸€)" in fixed_df.columns:
            fixed_df["ëœ»(í•œê¸€)"] = (
                fixed_df["ëœ»(í•œê¸€)"].astype(str).fillna("ì˜ë¯¸ í™•ì¸ í•„ìš”")
            )

        if "ë‹¨ì–´" in fixed_df.columns:
            fixed_df = fixed_df.drop_duplicates(subset=["ë‹¨ì–´"])

        return fixed_df


class GPTDifficultyFilter:
    """GPT ê¸°ë°˜ ì‚¬ìš©ì DB ìˆ˜ì¤€ ë‚œì´ë„ í•„í„°"""

    def __init__(
        self, client, user_words=None, cache_file="difficulty_filter_cache.json"
    ):
        self.client = client
        self.user_words = user_words or set()
        self.cache_file = cache_file
        self.difficulty_cache = {}
        self.gpt_calls = 0
        self._load_cache()

        # ì‚¬ìš©ì DB ë‹¨ì–´ë“¤ì˜ í‰ê·  ë‚œì´ë„ ë¶„ì„ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        self.user_db_baseline = None
        if self.user_words:
            self._analyze_user_db_baseline()

    def _load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    self.difficulty_cache = json.load(f)
                print(f"âœ… ë‚œì´ë„ í•„í„° ìºì‹œ ë¡œë“œ: {len(self.difficulty_cache)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.difficulty_cache = {}

    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        try:
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self.difficulty_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _analyze_user_db_baseline(self):
        """ì‚¬ìš©ì DB ë‹¨ì–´ë“¤ì˜ í‰ê·  ë‚œì´ë„ ë¶„ì„í•˜ì—¬ ê¸°ì¤€ì  ì„¤ì •"""
        if not self.user_words or len(self.user_words) == 0:
            return

        print("ğŸ” ì‚¬ìš©ì DB ë‹¨ì–´ ë‚œì´ë„ ê¸°ì¤€ì  ë¶„ì„ ì¤‘...")

        # ğŸ”¥ ì‚¬ìš©ì DBì—ì„œ ë‹¨ì¼ ë‹¨ì–´ë§Œ ì„ íƒí•˜ì—¬ ìƒ˜í”Œë§ (ìˆ™ì–´ ì œì™¸)
        single_words = [
            word for word in self.user_words if " " not in word and "-" not in word
        ]
        sample_words = single_words[:15] if len(single_words) > 15 else single_words

        if len(sample_words) == 0:
            self.user_db_baseline = {
                "average_score": 5.0,
                "min_threshold": 3.5,
                "sample_count": 0,
            }
            print("   âš ï¸ ë¶„ì„í•  ë‹¨ì¼ ë‹¨ì–´ê°€ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return

        difficulty_scores = []
        for word in sample_words:
            difficulty = self._gpt_analyze_word_difficulty(word, for_baseline=True)
            if difficulty and "difficulty_score" in difficulty:
                difficulty_scores.append(difficulty["difficulty_score"])

        if difficulty_scores:
            avg_score = sum(difficulty_scores) / len(difficulty_scores)
            # í‰ê· ë³´ë‹¤ 1.5ì  ë‚®ì€ ìˆ˜ì¤€ê¹Œì§€ë§Œ í—ˆìš©, ìµœì†Œ 3.0ì ì€ ë³´ì¥
            min_threshold = max(3.0, avg_score - 1.5)

            self.user_db_baseline = {
                "average_score": avg_score,
                "min_threshold": min_threshold,
                "sample_count": len(difficulty_scores),
            }
            print(
                f"   âœ… ì‚¬ìš©ì DB ê¸°ì¤€ì : í‰ê·  {avg_score:.1f}ì , ìµœì†Œ ì„ê³„ê°’ {min_threshold:.1f}ì "
            )
        else:
            self.user_db_baseline = {
                "average_score": 6.0,
                "min_threshold": 5.5,
                "sample_count": 0,
            }
            print("   âš ï¸ ì‚¬ìš©ì DB ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")

    def is_word_appropriate_for_user_db(self, word, context="", pos=""):
        """ë‹¨ì–´ê°€ ì‚¬ìš©ì DB ìˆ˜ì¤€ì— ì í•©í•œì§€ GPTë¡œ íŒë³„"""

        word_lower = word.lower().strip()

        # ğŸ”¥ ì‚¬ìš©ì DBì— ìˆëŠ” ë‹¨ì–´ëŠ” ë¬´ì¡°ê±´ ì í•© (ìµœìš°ì„ )
        if word_lower in self.user_words:
            return True, "ì‚¬ìš©ìDBí¬í•¨"

        # ìˆ™ì–´ëŠ” ë³„ë„ ì²˜ë¦¬
        if " " in word or "-" in word or "~" in word:
            return True, "ìˆ™ì–´íŒ¨í„´"

        # ìºì‹œ í™•ì¸
        cache_key = f"{word_lower}:{context[:30]}"
        if cache_key in self.difficulty_cache:
            cached_result = self.difficulty_cache[cache_key]
            return cached_result["appropriate"], cached_result["reason"]

        # GPT ë¶„ì„ (ì‚¬ìš©ì DBê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
        difficulty_analysis = self._gpt_analyze_word_difficulty(word, context, pos)

        # ì í•©ì„± íŒë³„
        appropriate, reason = self._determine_appropriateness(word, difficulty_analysis)

        # ìºì‹œ ì €ì¥
        self.difficulty_cache[cache_key] = {
            "appropriate": appropriate,
            "reason": reason,
            "analysis": difficulty_analysis,
        }

        return appropriate, reason

    def _gpt_analyze_word_difficulty(
        self, word, context="", pos="", for_baseline=False
    ):
        """GPTë¡œ ë‹¨ì–´ ë‚œì´ë„ ë¶„ì„"""

        # ì‚¬ìš©ì DB ê¸°ì¤€ì  ì •ë³´
        baseline_info = ""
        if self.user_db_baseline and not for_baseline:
            baseline_info = f"""
ì°¸ê³ : í˜„ì¬ ì‚¬ìš©ì ë‹¨ì–´ DBì˜ í‰ê·  ë‚œì´ë„ëŠ” {self.user_db_baseline['average_score']:.1f}ì ì…ë‹ˆë‹¤.
ì´ ìˆ˜ì¤€ì— ë§ëŠ” ë‹¨ì–´ë“¤ì„ ì„ ë³„í•´ì£¼ì„¸ìš”.
"""

        prompt = f"""

Analyze the difficulty of the English word "{word}" for Korean high school students.

Context: "{context}"
Part of Speech: {pos}

CRITICAL EXCLUSION CRITERIA (Rate as 1-2 points):
- Basic daily verbs: feel, see, hear, go, come, make, take, get, have, etc.
- Basic adjectives: good, bad, big, small, happy, sad, easy, hard, etc.  
- Basic nouns: man, woman, house, school, work, time, etc.
- Words learned in elementary/middle school
- Common conversation vocabulary

IMPORTANT NOTES:
- Educational value is NOT a factor for inclusion
- Emotional expression importance is NOT a factor  
- Focus ONLY on vocabulary difficulty level
- Rate based on the SPECIFIC meaning in the given context
- Korean high school students already know basic English vocabulary

Difficulty Scale (1-10):
1-3: Elementary/Middle school level (EXCLUDE these)
4-5: Basic high school level (borderline - usually EXCLUDE)  
6-7: Intermediate high school level (INCLUDE only if truly challenging)
8-10: Advanced/University prep level (INCLUDE)

For the word "{word}" in context "{context}":
- What is the specific meaning being used?
- Would a Korean high school student struggle with THIS specific usage?
- Is this an advanced/academic usage of the word?

Respond in JSON format:
{{
    "difficulty_score": 1-10,
    "specific_meaning": "meaning in this context",
    "level_category": "elementary|middle_school|basic_high_school|advanced_high_school|university",
    "is_basic_vocabulary": true/false,
    "recommendation": "include|exclude",
    "reasoning": "focus on difficulty of the specific contextual meaning"
}}
"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in English education for Korean students. You understand the appropriate vocabulary levels for different stages of English learning.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=400,
                temperature=0.1,
            )

            self.gpt_calls += 1
            content = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            result = json.loads(content)
            return result

        except Exception as e:
            print(f"âŒ GPT ë‚œì´ë„ ë¶„ì„ ì‹¤íŒ¨ ({word}): {e}")
            return None

    def _determine_appropriateness(self, word, analysis):
        """ìˆ˜ì •ëœ ì í•©ì„± íŒë³„ - ë” ì—„ê²©í•œ ê¸°ì¤€"""

        if not analysis:
            return False, "ë¶„ì„ì‹¤íŒ¨"

        # ğŸ”¥ ê¸°ë³¸ ë‹¨ì–´ ê°•ë ¥ ì°¨ë‹¨
        if analysis.get("is_basic_vocabulary", False):
            return False, "ê¸°ë³¸ì–´íœ˜ì œì™¸"

        difficulty_score = analysis.get("difficulty_score", 5)
        contextual_difficulty = analysis.get("contextual_difficulty", 5)

        # ğŸ”¥ ëŒ€í­ ìƒí–¥ëœ ì„ê³„ê°’ (ê¸°ì¡´ 4.0 â†’ 7.0)
        MIN_DIFFICULTY = 6.0
        MIN_CONTEXTUAL_DIFFICULTY = 6.0

        if difficulty_score < 6.0:
            return False, f"ë‚œì´ë„ë¶€ì¡±({difficulty_score}<{MIN_DIFFICULTY})"

        if contextual_difficulty < MIN_CONTEXTUAL_DIFFICULTY:
            return (
                False,
                f"ë¬¸ë§¥ë‚œì´ë„ë¶€ì¡±({contextual_difficulty}<{MIN_CONTEXTUAL_DIFFICULTY})",
            )

        # ğŸ”¥ êµìœ¡ì  ê°€ì¹˜ ê¸°ì¤€ ì™„ì „ ì œê±°
        recommendation = analysis.get("recommendation", "exclude")
        if recommendation == "include":
            return (
                True,
                f"ê³ ë‚œì´ë„í™•ì¸({difficulty_score}ì ,ë¬¸ë§¥{contextual_difficulty}ì )",
            )
        else:
            return False, f"GPTì œì™¸ì¶”ì²œ({analysis.get('reasoning', 'ì´ìœ ì—†ìŒ')})"

    def batch_filter_words(self, words_with_context, batch_size=10):
        """ì—¬ëŸ¬ ë‹¨ì–´ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""

        results = []
        appropriate_count = 0

        print(f"ğŸ” GPT ê¸°ë°˜ ë‹¨ì–´ ì í•©ì„± ê²€ì‚¬: {len(words_with_context)}ê°œ")

        for i in range(0, len(words_with_context), batch_size):
            batch = words_with_context[i : i + batch_size]

            for word_info in batch:
                word = word_info.get("word", "")
                context = word_info.get("context", "")
                pos = word_info.get("pos", "")

                appropriate, reason = self.is_word_appropriate_for_user_db(
                    word, context, pos
                )

                results.append(
                    {
                        "word": word,
                        "appropriate": appropriate,
                        "reason": reason,
                        "original_info": word_info,
                    }
                )

                if appropriate:
                    appropriate_count += 1

            # ë°°ì¹˜ë§ˆë‹¤ ìºì‹œ ì €ì¥
            if i % (batch_size * 5) == 0:
                self._save_cache()

        # ìµœì¢… ìºì‹œ ì €ì¥
        self._save_cache()

        print(f"âœ… í•„í„°ë§ ì™„ë£Œ: {appropriate_count}/{len(words_with_context)}ê°œ ì„ íƒ")
        print(f"ğŸ¤– GPT í˜¸ì¶œ: {self.gpt_calls}íšŒ")

        return results


class AdvancedVocabExtractor(MissingMethodsMixin):

    def __init__(
        self,
        user_words_file="ë‹¨ì–´DB.csv",
        settings=None,
        csv_file=None,
        verbose=False,
        **kwargs,
    ):
        # ê¸°ë³¸ ì„¤ì •
        self.default_settings = {
            "DIFFICULTY_THRESHOLD": 3.0,
            "GPT_CALL_LIMIT": 1000,
            "USER_PRIORITY": 1,
            "CACHE_FILE": "gpt_cache.json",
            "MIN_WORD_LENGTH": 4,
            "EASY_WORDS_CACHE": "elementary_words.pkl",
            "MAX_TOKENS": 200000,
            "USE_CACHE": True,
            "USE_INTEGRATED_CONTEXTUAL": True,
            "USE_INTEGRATED_DIFFICULTY": True,
            "ENHANCED_MEANING_GENERATION": True,
            "USE_GPT_DIFFICULTY_FILTER": True,
        }

        self.settings = self.default_settings.copy()
        if settings:
            self.settings.update(settings)

        # ê¸°ë³¸ ë³€ìˆ˜ë“¤
        self.gpt_call_count = 0
        self.GPT_CALL_LIMIT = self.settings["GPT_CALL_LIMIT"]
        self.DIFFICULTY_THRESHOLD = self.settings["DIFFICULTY_THRESHOLD"]
        self.MIN_WORD_LENGTH = self.settings["MIN_WORD_LENGTH"]
        self.MAX_TOKENS = self.settings["MAX_TOKENS"]
        self.USE_CACHE = self.settings["USE_CACHE"]
        self.gpt_cache = {}
        self.gpt_token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }
        self.verbose = verbose

        # ğŸ”¥ ì‚¬ìš©ì ë‹¨ì–´ DB ê°œì„ ëœ ë¡œë”©
        self.user_words = set()
        self.user_idioms = set()  # ğŸ”¥ ìˆ™ì–´ë§Œ ë”°ë¡œ ê´€ë¦¬
        self.user_single_words = set()  # ğŸ”¥ ë‹¨ì¼ ë‹¨ì–´ë§Œ ë”°ë¡œ ê´€ë¦¬

        # Phase 1: í†µí•© ëª¨ë“ˆ ì´ˆê¸°í™”
        self.phrase_db = None

        # ìºì‹œ ë¡œë“œ
        self.load_cache_from_file(self.settings["CACHE_FILE"])

        # ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡
        self.easy_words = self._load_easy_words()

        # ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ë“¤
        self.freq_tiers = {}
        if csv_file and os.path.exists(csv_file):
            print(f"ğŸ“Š '{csv_file}'ì—ì„œ ë¹ˆë„ ë°ì´í„° êµ¬ì¶• ì¤‘...")
            self.freq_tiers = self._build_frequency_from_csv(csv_file)

        # ğŸ”¥ client ì¶”ê°€
        self.client = client

        # ğŸ”¥ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
        try:
            # 1ìˆœìœ„: ë¬¸ë§¥ ê¸°ë°˜ ì¶”ì¶œê¸° ì‹œë„ (contextual_synonym_refiner.py)
            try:
                from contextual_synonym_refiner import ImprovedSynonymRefiner

                self.synonym_extractor = ImprovedSynonymRefiner(
                    client=client,
                    cache_file="contextual_synonym_cache.json",
                    verbose=self.verbose,
                )
                print("âœ… ë¬¸ë§¥ ê¸°ë°˜ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError:
                print("âš ï¸ contextual_synonym_refiner.py ì—†ìŒ, ê¸°ì¡´ ëª¨ë“ˆ ì‚¬ìš©")
                # 2ìˆœìœ„: ê¸°ì¡´ ëª¨ë“ˆ ì‚¬ìš© (synonym_antonym_module.py)
                from synonym_antonym_module import SynonymAntonymExtractor

                self.synonym_extractor = SynonymAntonymExtractor(
                    client=client,
                    cache_file="synonym_antonym_cache.json",
                    verbose=self.verbose,
                )
                print("âœ… ê¸°ì¡´ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸°ë¡œ ì´ˆê¸°í™”")
        except Exception as e:
            print(f"âŒ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.synonym_extractor = None
        # ğŸ”¥ ê³ ê¸‰ ê²€ì¦ê¸°ë¡œ ì´ˆê¸°í™”
        self.idiom_checker = AdvancedIdiomChecker(nlp)

        # ğŸ”¥ ì™¸ë¶€ DB ì´ˆê¸°í™” ì¶”ê°€
        self.external_vocab_db = ExternalVocabDatabase()

        # ğŸ”¥ ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ ë¡œë”© (ê°œì„ ëœ ë²„ì „)
        if user_words_file and os.path.exists(user_words_file):
            print(f"ğŸ“– ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ ë¡œë”©: {user_words_file}")
            self._load_user_words_with_idiom_detection(user_words_file)
        else:
            print(f"ğŸ” ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_words_file}")

        # ğŸ”¥ GPT ê¸°ë°˜ ë‚œì´ë„ í•„í„° ì´ˆê¸°í™”
        self.initialize_gpt_difficulty_filter()
        print(f"âœ… ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(
            f"   â€¢ í†µí•© ì»¨í…ìŠ¤íŠ¸ ì˜ë¯¸ ìƒì„±: {'âœ…' if self.settings['USE_INTEGRATED_CONTEXTUAL'] else 'âŒ'}"
        )
        print(
            f"   â€¢ í†µí•© ë‚œì´ë„ ë¶„ì„: {'âœ…' if self.settings['USE_INTEGRATED_DIFFICULTY'] else 'âŒ'}"
        )
        print(f"   â€¢ ê³ ê¸‰ êµ¬ë™ì‚¬ ë¶„ì„: âœ…")
        print(f"   â€¢ ë¬¸ë²• íŒ¨í„´ ë¶„ì„: âœ…")

        # ğŸ”¥ ë°ì´í„° ì†ŒìŠ¤ ë¡œë”© ìƒí™©
        print(f"\nğŸ“Š ë°ì´í„° ì†ŒìŠ¤ ë¡œë”© ìƒí™©:")
        print(f"=" * 60)

        # ğŸ”¥ ì‰¬ìš´ ë‹¨ì–´ ë¡œë”© ìƒíƒœ
        print(f"\nğŸ“š ì‰¬ìš´ ë‹¨ì–´ ë¡œë”© ìƒí™©:")
        if os.path.exists(self.settings["EASY_WORDS_CACHE"]):
            print(f"   âœ… ìºì‹œì—ì„œ ê¸°ë³¸ ì‰¬ìš´ ë‹¨ì–´ ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"   âš ï¸ ìºì‹œ ì—†ìŒ, stopwords + ê¸°ë³¸ ë‹¨ì–´ë“¤ ì‚¬ìš© ì¤‘")
        print(f"   ğŸ“Š ì´ ì‰¬ìš´ ë‹¨ì–´: {len(self.easy_words)}ê°œ")

        # ğŸ”¥ data ë””ë ‰í† ë¦¬ ìˆ™ì–´ ë¡œë”©
        data_dir = self.settings.get("data_dir", "data")
        self.reference_idioms = load_custom_idioms_from_data_directory(data_dir)

        # ğŸ”¥ ìµœì¢… ë°ì´í„° ìš”ì•½
        print(f"\nğŸ“ˆ ìµœì¢… ë°ì´í„° ìš”ì•½:")
        print(f"   ğŸ‘¤ ì‚¬ìš©ì ì „ì²´ ë‹¨ì–´: {len(self.user_words)}ê°œ")
        print(f"   ğŸ“ ì‚¬ìš©ì ìˆ™ì–´: {len(self.user_idioms)}ê°œ")
        print(f"   ğŸ”¤ ì‚¬ìš©ì ë‹¨ì¼ ë‹¨ì–´: {len(self.user_single_words)}ê°œ")
        print(f"   ğŸ›ï¸ ì°¸ì¡° ìˆ™ì–´ DB: {len(self.reference_idioms)}ê°œ")
        print(f"   ğŸ“š ì‰¬ìš´ ë‹¨ì–´: {len(self.easy_words)}ê°œ")
        print(f"   ğŸ“Š ë¹ˆë„ ë°ì´í„°: {len(self.freq_tiers)}ê°œ")
        print(f"=" * 60)

    def initialize_gpt_difficulty_filter(self):
        """GPT ê¸°ë°˜ ë‚œì´ë„ í•„í„° ì´ˆê¸°í™”"""
        if not self.settings.get("USE_GPT_DIFFICULTY_FILTER", True):
            print("âš ï¸ GPT ë‚œì´ë„ í•„í„°ê°€ ì„¤ì •ì—ì„œ ë¹„í™œì„±í™”ë¨")
            return

        try:
            print("ğŸ¤– GPT ê¸°ë°˜ ë‚œì´ë„ í•„í„° ì´ˆê¸°í™” ì¤‘...")
            print(f"   ğŸ“Š ì‚¬ìš©ì ë‹¨ì¼ ë‹¨ì–´: {len(self.user_single_words)}ê°œ")

            self.gpt_filter = GPTDifficultyFilter(
                client=client,
                user_words=self.user_single_words,  # ì‚¬ìš©ì ë‹¨ì¼ ë‹¨ì–´ë§Œ ì‚¬ìš©
                cache_file="gpt_difficulty_filter_cache.json",
            )

            print("âœ… GPT ë‚œì´ë„ í•„í„° ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ GPT ë‚œì´ë„ í•„í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            self.gpt_filter = None

    # AdvancedVocabExtractor í´ë˜ìŠ¤ì— ì´ ë©”ì„œë“œ ì¶”ê°€
    def debug_gpt_filter_status(self):
        """GPT í•„í„° ìƒíƒœ ë””ë²„ê¹…"""
        print("\nğŸ” GPT í•„í„° ìƒíƒœ í™•ì¸:")
        print(f"   â€¢ hasattr(self, 'gpt_filter'): {hasattr(self, 'gpt_filter')}")
        if hasattr(self, "gpt_filter"):
            print(f"   â€¢ gpt_filter is not None: {self.gpt_filter is not None}")
            if self.gpt_filter:
                print(f"   â€¢ user_words ìˆ˜: {len(self.gpt_filter.user_words)}")
                print(f"   â€¢ baseline ì„¤ì •: {self.gpt_filter.user_db_baseline}")
                print(f"   â€¢ GPT í˜¸ì¶œ ìˆ˜: {self.gpt_filter.gpt_calls}")
        print()

    def test_gpt_filter_integration(self):
        """GPT í•„í„° í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª GPT í•„í„° í†µí•© í…ŒìŠ¤íŠ¸:")
        print(
            f"   â€¢ gpt_filter ì¡´ì¬: {hasattr(self, 'gpt_filter') and self.gpt_filter is not None}"
        )
        if hasattr(self, "gpt_filter") and self.gpt_filter:
            print(f"   â€¢ ì‚¬ìš©ì ë‹¨ì–´ ìˆ˜: {len(self.gpt_filter.user_words)}ê°œ")
            print(f"   â€¢ ê¸°ì¤€ì  ì„¤ì •: {self.gpt_filter.user_db_baseline is not None}")
            if self.gpt_filter.user_db_baseline:
                baseline = self.gpt_filter.user_db_baseline
                print(f"   â€¢ í‰ê·  ì ìˆ˜: {baseline['average_score']:.1f}")
                print(f"   â€¢ ìµœì†Œ ì„ê³„ê°’: {baseline['min_threshold']:.1f}")
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")

    # 3. extract_difficult_words ë©”ì„œë“œì—ì„œ GPT í•„í„° ì‚¬ìš© í™•ì¸
    def extract_difficult_words(self, text, easy_words, child_vocab, freq_tiers):
        """ì‚¬ìš©ì DB ë§¤ì¹­ ìš°ì„  + GPT í•„í„°ë§ ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ"""
        text_str = self._force_extract_text(text)
        word_candidates = []

        # ğŸ”¥ ì›í˜• ê¸°ì¤€ ì¤‘ë³µ ì¶”ì 
        seen_lemmas = set()  # ì´ë¯¸ ì²˜ë¦¬ëœ ì›í˜•ë“¤
        lemma_to_info = {}  # ì›í˜• â†’ ë‹¨ì–´ ì •ë³´ ë§¤í•‘

        try:
            doc = nlp(text_str)
            for token in doc:
                word = token.text.lower()
                lemma = token.lemma_.lower()
                original_word = token.text

                # ê¸°ë³¸ í•„í„°ë§
                if (
                    len(word) < 3
                    or not word.isalpha()
                    or token.is_stop
                    or token.pos_ in ["PUNCT", "SPACE", "SYM"]
                ):
                    continue

                # ğŸ”¥ ì›í˜• ê¸°ì¤€ ì¤‘ë³µ ì²´í¬
                if lemma in seen_lemmas:
                    continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ì›í˜•ì€ ê±´ë„ˆë›°ê¸°

                seen_lemmas.add(lemma)

                # 1ì°¨ ë¹ ë¥¸ í•„í„°ë§
                if word in easy_words or (child_vocab and word in child_vocab):
                    continue

                # ë¬¸ë§¥ ì¶”ì¶œ
                context = self._get_sentence_context(
                    text_str, token.idx, token.idx + len(token.text)
                )

                # í›„ë³´ ë‹¨ì–´ ìˆ˜ì§‘
                word_candidates.append(
                    {
                        "word": original_word,
                        "lemma": lemma,
                        "context": context,
                        "pos": self._get_simple_pos(token.pos_),
                        "token_info": {
                            "start": token.idx,
                            "end": token.idx + len(token.text),
                            "original": original_word,
                        },
                    }
                )

        except Exception as e:
            print(f"âŒ í† í° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

        # ğŸ”¥ GPT í•„í„°ë§ ì ìš© ì—¬ë¶€ ê²°ì • (ìˆ˜ì •ëœ ì¡°ê±´)
        print(
            f"   ğŸ” GPT í•„í„° ì‚¬ìš© ê°€ëŠ¥: {hasattr(self, 'gpt_filter') and self.gpt_filter is not None}"
        )

        if hasattr(self, "gpt_filter") and self.gpt_filter and word_candidates:
            print(f"   ğŸ¤– GPT í•„í„°ë§ ëª¨ë“œ ì‚¬ìš©")
            return self._process_words_with_gpt_filter(word_candidates)
        else:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ (GPT í•„í„° ì—†ìŒ)
            print(f"   âš™ï¸ ê¸°ì¡´ ë°©ì‹ ëª¨ë“œ ì‚¬ìš©")
            return self._process_words_without_gpt_filter(word_candidates, freq_tiers)

    def enhanced_extract_user_db_idioms_with_separable(self, text):
        """ë¶„ë¦¬í˜• ê°ì§€ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ì‚¬ìš©ì DB ìˆ™ì–´ ì¶”ì¶œ"""

        results = []
        text_str = self._force_extract_text(text)
        found_positions = set()

        print(
            f"   ğŸ” ì‚¬ìš©ì DB ìˆ™ì–´ ë§¤ì¹­ ê²€ì‚¬ (ë¶„ë¦¬í˜• í¬í•¨): {len(self.user_idioms)}ê°œ"
        )

        # 1. ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ (ìµœìš°ì„ )
        if hasattr(self, "separable_detector"):
            separable_results = self.separable_detector.detect_separable_idioms_in_text(
                text_str
            )

            for sep_result in separable_results:
                context = self._get_sentence_context(
                    text_str, sep_result["start"], sep_result["end"]
                )
                meaning = self.enhanced_korean_definition(
                    sep_result["display_form"], context, is_phrase=True
                )

                results.append(
                    {
                        "original": sep_result["original"],
                        "base_form": sep_result["display_form"],  # ğŸ”¥ ë¶„ë¦¬í˜• í‘œì‹œ í¬í•¨
                        "meaning": meaning,
                        "context": context,
                        "type": "user_db_separable_idiom",
                        "is_separated": sep_result["is_separated"],
                        "confidence": sep_result["confidence"],
                        "user_db_match": True,
                        "match_type": f"ì‚¬ìš©ìDBë¶„ë¦¬í˜•_{sep_result['description']}",
                        "separable_info": sep_result["separable_info"],
                    }
                )

                found_positions.add((sep_result["start"], sep_result["end"]))

        # 2. ì¼ë°˜ ìˆ™ì–´ ì¶”ì¶œ (ë¶„ë¦¬í˜•ì´ ì•„ë‹Œ ê²ƒë“¤)
        non_separable_idioms = self.user_idioms
        if hasattr(self, "user_separable_idioms"):
            non_separable_idioms = self.user_idioms - set(
                self.user_separable_idioms.keys()
            )

        # ê¸¸ì´ìˆœ ì •ë ¬ (ê¸´ ìˆ™ì–´ë¶€í„° ë§¤ì¹­í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
        sorted_regular_idioms = sorted(non_separable_idioms, key=len, reverse=True)

        for idiom in sorted_regular_idioms:
            # ì •í™•í•œ ë§¤ì¹­ (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤)
            pattern = r"\b" + re.escape(idiom) + r"\b"
            matches = re.finditer(pattern, text_str, re.IGNORECASE)

            for match in matches:
                start, end = match.span()

                # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                if any(abs(start - pos[0]) < 5 for pos in found_positions):
                    continue

                context = self._get_sentence_context(text_str, start, end)
                original_text = text_str[start:end]

                meaning = self.enhanced_korean_definition(
                    idiom, context, is_phrase=True
                )

                results.append(
                    {
                        "original": original_text,
                        "base_form": idiom,
                        "meaning": meaning,
                        "context": context,
                        "type": "user_db_idiom",
                        "is_separated": False,
                        "confidence": 0.95,
                        "user_db_match": True,
                        "match_type": "ì‚¬ìš©ìDBì¼ë°˜ìˆ™ì–´",
                    }
                )
                found_positions.add((start, end))

        # ê²°ê³¼ í†µê³„
        separable_count = len(
            [r for r in results if r.get("type") == "user_db_separable_idiom"]
        )
        regular_count = len([r for r in results if r.get("type") == "user_db_idiom"])

        print(
            f"   ğŸ“Š ì‚¬ìš©ì DB ë§¤ì¹­ ê²°ê³¼: ë¶„ë¦¬í˜• {separable_count}ê°œ, ì¼ë°˜ {regular_count}ê°œ"
        )

        return results

    def initialize_separable_detection(self, user_words_file=None):
        """ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""

        print(f"ğŸ”§ ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")

        # SeparableIdiomDetector ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.separable_detector = SeparableIdiomDetector(client, verbose=self.verbose)

        # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì‹œë„
        cache_file = "separable_analysis.json"
        if not self.separable_detector.load_separable_analysis(cache_file):
            # ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ë¶„ì„
            if hasattr(self, "user_idioms") and self.user_idioms:
                print(f"ğŸ¤– ì‚¬ìš©ì ìˆ™ì–´ ë¶„ë¦¬í˜• ë¶„ì„ ì‹œì‘...")
                separable_analysis = (
                    self.separable_detector.analyze_user_idioms_with_gpt(
                        self.user_idioms
                    )
                )
                self.separable_detector.build_separable_patterns(separable_analysis)

                # ë¶„ì„ ê²°ê³¼ ì €ì¥
                self.separable_detector.save_separable_analysis(cache_file)
            else:
                print("âš ï¸ ì‚¬ìš©ì ìˆ™ì–´ê°€ ì—†ì–´ ë¶„ë¦¬í˜• ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        # extractorì™€ ì—°ë™
        self.user_separable_idioms = self.separable_detector.user_separable_idioms

        print(
            f"âœ… ë¶„ë¦¬í˜• ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.user_separable_idioms)}ê°œ ë¶„ë¦¬í˜• ìˆ™ì–´"
        )

    # ğŸ”¥ ìƒˆë¡œìš´ ì‚¬ìš©ì ë‹¨ì–´ ë¡œë”© í•¨ìˆ˜ (ìˆ™ì–´ ê°ì§€ í¬í•¨)
    def _load_user_words_with_idiom_detection(self, user_words_file):
        """ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ì—ì„œ ìˆ™ì–´ì™€ ë‹¨ì¼ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ì—¬ ë¡œë”©"""
        try:
            if user_words_file.endswith(".csv"):
                # ğŸ”¥ ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
                encodings = ["utf-8", "cp949", "euc-kr", "latin1", "utf-8-sig"]
                user_df = None

                for encoding in encodings:
                    try:
                        user_df = pd.read_csv(user_words_file, encoding=encoding)
                        print(f"   âœ… ì¸ì½”ë”© ì„±ê³µ: {encoding}")
                        break
                    except UnicodeDecodeError:
                        continue

                if user_df is None:
                    print(f"   âŒ ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨")
                    return

            elif user_words_file.endswith((".xlsx", ".xls")):
                user_df = pd.read_excel(user_words_file)
            else:
                print(f"   âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
                return

            if not user_df.empty and len(user_df.columns) > 0:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì˜ ë‹¨ì–´ë“¤ ì¶”ì¶œ
                user_words = (
                    user_df.iloc[:, 0].dropna().astype(str).str.strip().tolist()
                )

                # ğŸ”¥ ìˆ™ì–´ì™€ ë‹¨ì¼ ë‹¨ì–´ ë¶„ë¦¬
                idiom_count = 0
                single_word_count = 0

                for word in user_words:
                    word_clean = word.lower().strip()
                    self.user_words.add(word_clean)

                    # ë„ì–´ì“°ê¸°ê°€ ìˆìœ¼ë©´ ìˆ™ì–´ë¡œ ë¶„ë¥˜
                    if " " in word_clean and len(word_clean.split()) >= 2:
                        self.user_idioms.add(word_clean)
                        idiom_count += 1
                    else:
                        self.user_single_words.add(word_clean)
                        single_word_count += 1

                print(f"   âœ… ì´ {len(user_words)}ê°œ ë‹¨ì–´ ë¡œë“œ ì™„ë£Œ")
                print(f"   ğŸ“‹ ìˆ™ì–´: {idiom_count}ê°œ")
                print(f"   ğŸ“‹ ë‹¨ì¼ ë‹¨ì–´: {single_word_count}ê°œ")

                # ğŸ”¥ ì‚¬ìš©ì ìˆ™ì–´ ìƒ˜í”Œ ì¶œë ¥
                if self.user_idioms:
                    sample_idioms = list(self.user_idioms)[:5]
                    print(f"   ğŸ“ ì‚¬ìš©ì ìˆ™ì–´ ì˜ˆì‹œ: {sample_idioms}")

            else:
                print(f"   âš ï¸ íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŒ")

            # ì‚¬ìš©ì ë‹¨ì–´ ë¡œë“œ ì™„ë£Œ í›„ GPT í•„í„° ì¬ì´ˆê¸°í™”
            if hasattr(self, "gpt_filter") and self.gpt_filter:
                self.gpt_filter.user_words = self.user_single_words
                print(
                    f"   ğŸ”„ GPT í•„í„° ì‚¬ìš©ì ë‹¨ì–´ ì—…ë°ì´íŠ¸: {len(self.user_single_words)}ê°œ"
                )

        except Exception as e:
            print(f"   âŒ ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _load_easy_words(self):
        """ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡ ë¡œë“œ (Excel íŒŒì¼ ì§€ì› ì¶”ê°€)"""
        try:
            # 1. ë¨¼ì € pickle ìºì‹œ í™•ì¸
            easy_words_cache = self.settings[
                "EASY_WORDS_CACHE"
            ]  # "elementary_words.pkl"
            if os.path.exists(easy_words_cache):
                with open(easy_words_cache, "rb") as f:
                    easy_words = pickle.load(f)
                print(f"âœ… ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡ {len(easy_words)}ê°œ ìºì‹œì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return easy_words

            # 2. Excel íŒŒì¼ í™•ì¸
            excel_files = [
                "easy_words.xlsx",
                "elementary_words.xlsx",
                "basic_words.xlsx",
            ]
            for excel_file in excel_files:
                if os.path.exists(excel_file):
                    print(f"ğŸ“Š Excel íŒŒì¼ì—ì„œ ì‰¬ìš´ ë‹¨ì–´ ë¡œë”©: {excel_file}")
                    try:
                        df = pd.read_excel(excel_file)
                        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì˜ ë‹¨ì–´ë“¤ ì¶”ì¶œ
                        words_column = df.columns[0]
                        easy_words = set(
                            df[words_column]
                            .dropna()
                            .astype(str)
                            .str.strip()
                            .str.lower()
                        )

                        # pickle ìºì‹œë¡œ ì €ì¥ (ë‹¤ìŒë²ˆì— ë¹ ë¥´ê²Œ ë¡œë“œí•˜ê¸° ìœ„í•´)
                        try:
                            with open(easy_words_cache, "wb") as f:
                                pickle.dump(easy_words, f)
                            print(f"âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ: {easy_words_cache}")
                        except Exception as e:
                            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

                        print(f"âœ… Excelì—ì„œ ì‰¬ìš´ ë‹¨ì–´ {len(easy_words)}ê°œ ë¡œë“œ ì™„ë£Œ")
                        return easy_words

                    except Exception as e:
                        print(f"âš ï¸ {excel_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue

            # 3. CSV íŒŒì¼ë„ í™•ì¸
            csv_files = ["easy_words.csv", "elementary_words.csv", "basic_words.csv"]
            for csv_file in csv_files:
                if os.path.exists(csv_file):
                    print(f"ğŸ“Š CSV íŒŒì¼ì—ì„œ ì‰¬ìš´ ë‹¨ì–´ ë¡œë”©: {csv_file}")
                    try:
                        df = pd.read_csv(csv_file, encoding="utf-8")
                        words_column = df.columns[0]
                        easy_words = set(
                            df[words_column]
                            .dropna()
                            .astype(str)
                            .str.strip()
                            .str.lower()
                        )

                        print(f"âœ… CSVì—ì„œ ì‰¬ìš´ ë‹¨ì–´ {len(easy_words)}ê°œ ë¡œë“œ ì™„ë£Œ")
                        return easy_words

                    except Exception as e:
                        print(f"âš ï¸ {csv_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue

        except Exception as e:
            print(f"âš ï¸ ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 4. ëª¨ë“  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹¨ì–´ë“¤ ì‚¬ìš©
        print("ğŸ“š ê¸°ë³¸ ë‹¨ì–´ ì„¸íŠ¸ ì‚¬ìš©")
        return set(stopwords.words("english")).union(
            BASIC_VERBS,
            BASIC_ADJECTIVES,
            BASIC_NOUNS,
            {
                "the",
                "a",
                "an",
                "this",
                "that",
                "these",
                "those",
                "good",
                "bad",
                "big",
                "small",
                "new",
                "old",
                "young",
                "make",
                "take",
                "get",
                "go",
                "come",
                "see",
                "know",
            },
        )

    def _build_frequency_from_csv(self, csv_file):
        """CSV íŒŒì¼ì—ì„œ ë¹ˆë„ ë°ì´í„° êµ¬ì¶• (ê°œì„ ëœ ë²„ì „)"""
        try:
            print(f"ğŸ“Š ë¹ˆë„ ë¶„ì„ ì‹œì‘: {csv_file}")

            # ğŸ”¥ ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
            encodings = ["utf-8", "cp949", "euc-kr", "latin1", "utf-8-sig"]
            df = None

            for encoding in encodings:
                try:
                    df = pd.read_csv(csv_file, encoding=encoding)
                    print(f"   âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ: {encoding}")
                    break
                except UnicodeDecodeError:
                    continue

            if df is None:
                print(f"   âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨")
                return {}

            print(f"   ğŸ“‹ ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {len(df)} rows, {len(df.columns)} columns")
            print(f"   ğŸ“‹ ì»¬ëŸ¼ëª…: {list(df.columns)}")

            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
            text_columns = [
                "content",
                "ì§€ë¬¸",
                "text",
                "í…ìŠ¤íŠ¸",
                "ë³¸ë¬¸",
                "Content",
                "TEXT",
            ]
            found_column = None

            for col in text_columns:
                if col in df.columns:
                    found_column = col
                    break

            if not found_column:
                print(
                    f"   âš ï¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}"
                )
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ìœ¼ë¡œ ì‚¬ìš©
                if len(df.columns) > 0:
                    found_column = df.columns[0]
                    print(f"   ğŸ“ ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©: {found_column}")
                else:
                    return {}

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = df[found_column].dropna().astype(str).tolist()
            print(f"   ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")

            if not texts:
                print(f"   âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                return {}

            # ë¹ˆë„ ë¶„ì„
            freq_result = self._calculate_word_frequencies(texts)
            print(f"   âœ… ë¹ˆë„ ë¶„ì„ ì™„ë£Œ: {len(freq_result)}ê°œ ë‹¨ì–´")

            return freq_result

        except Exception as e:
            print(f"âŒ ë¹ˆë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
        return {}

    def _calculate_word_frequencies(self, texts):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°"""
        word_counts = Counter()
        for text in texts:
            doc = nlp(text)
            for token in doc:
                if (
                    len(token.text) >= 3
                    and token.is_alpha
                    and not token.is_stop
                    and token.pos_ not in ["PUNCT", "SPACE", "SYM"]
                ):
                    lemma = token.lemma_.lower()
                    if lemma != "-PRON-":
                        word_counts[lemma] += 1

        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë“±ê¸‰ ë¶€ì—¬
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        freq_tiers = {}
        total_words = len(sorted_words)

        for rank, (word, count) in enumerate(sorted_words, 1):
            if rank <= total_words * 0.05:
                tier = 1.0
            elif rank <= total_words * 0.2:
                tier = 2.0
            elif rank <= total_words * 0.5:
                tier = 3.0
            else:
                tier = 4.0 + (rank - total_words * 0.5) / (total_words * 0.5)
            freq_tiers[word] = tier

        print(f"âœ… ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì™„ë£Œ: {len(freq_tiers)}ê°œ ë‹¨ì–´")
        return freq_tiers

    def save_cache_to_file(self, cache_file=None):
        """GPT ìºì‹œ ì €ì¥"""
        if not cache_file:
            cache_file = self.settings["CACHE_FILE"]

        serializable_cache = {str(k): v for k, v in self.gpt_cache.items()}
        cache_data = {
            "cache": serializable_cache,
            "token_usage": self.gpt_token_usage,
            "call_count": self.gpt_call_count,
        }

        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… GPT ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.gpt_cache)}ê°œ í•­ëª©")
        except Exception as e:
            print(f"âš ï¸ GPT ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_cache_from_file(self, cache_file=None):
        """GPT ìºì‹œ ë¡œë“œ"""
        if not cache_file:
            cache_file = self.settings["CACHE_FILE"]

        if os.path.exists(cache_file):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)

                if isinstance(cache_data, dict) and "cache" in cache_data:
                    serialized_cache = cache_data["cache"]
                    if "token_usage" in cache_data:
                        self.gpt_token_usage = cache_data["token_usage"]
                    if "call_count" in cache_data:
                        self.gpt_call_count = cache_data["call_count"]
                else:
                    serialized_cache = cache_data

                for k, v in serialized_cache.items():
                    try:
                        self.gpt_cache[k] = v
                    except Exception as e:
                        if self.verbose:
                            print(f"âš ï¸ ìºì‹œ í•­ëª© ë³€í™˜ ì‹¤íŒ¨: {k} - {e}")

                print(f"âœ… GPT ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.gpt_cache)}ê°œ í•­ëª©")
            except Exception as e:
                print(f"âš ï¸ GPT ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.gpt_cache = {}

    # ğŸ”¥ í†µí•©ëœ ì˜ë¯¸ ìƒì„± í•¨ìˆ˜
    def enhanced_korean_definition(
        self, word, sentence, is_phrase=False, pos_hint=None
    ):
        """ë¬¸ë§¥ì„ ê³ ë ¤í•œ í–¥ìƒëœ ì˜ë¯¸ ìƒì„±"""
        if not self.settings["USE_INTEGRATED_CONTEXTUAL"]:
            return self._legacy_korean_definition(word, sentence, is_phrase, pos_hint)

        try:
            result, updated_cache, updated_call_count, updated_token_usage = (
                integrated_get_best_korean_definition(
                    word=word,
                    phrase_db=self.phrase_db,
                    is_phrase=is_phrase,
                    max_tokens=self.MAX_TOKENS,
                    client=client,
                    gpt_cache=self.gpt_cache,
                    gpt_call_count=self.gpt_call_count,
                    GPT_CALL_LIMIT=self.GPT_CALL_LIMIT,
                    token_usage=self.gpt_token_usage,
                    custom_prompt=None,
                    sentence=sentence,  # ğŸ”¥
                )
            )

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.gpt_cache = updated_cache
            self.gpt_call_count = updated_call_count
            self.gpt_token_usage = updated_token_usage

            return self._clean_korean_definition(word, result)

        except Exception as e:
            print(f"   âŒ integrated_get_best_korean_definition ì‹¤íŒ¨: {e}")
            return word

    def integrated_comprehensive_analysis(self, word, context="", pos=""):
        """Single GPT call for complete word analysis"""

        cache_key = f"comprehensive:{word.lower()}:{context[:50]}"
        if cache_key in self.gpt_cache:
            return self.gpt_cache[cache_key]

        # Check if it's an idiom/phrase first
        is_idiom = " " in word or "-" in word or "~" in word

        comprehensive_prompt = f"""
Analyze the English word/phrase: "{word}"
Context: "{context}"
Part of Speech: {pos}

Provide comprehensive analysis in JSON format:

1. Basic Information:
   - Korean meaning (concise, 2-5 words)
   - Confirmed part of speech
   - Is this the base/root form?
   - 2-3 synonyms
   - 1-2 antonyms

2. Difficulty Analysis:
   - Difficulty score (0.5-10 scale)
     * 0.5-2: Basic elementary words (a, the, cat, red)
     * 3-4: Elementary level (big, good, come, go)
     * 5-6: Intermediate level (difficult, because, important)
     * 7-8: Advanced level (significant, comprehensive)
     * 9-10: Expert level (sophisticated, paradigm)
   - Level category (elementary/advanced)
   - Recommendation (keep/remove)
   - Detailed reasoning

3. Quality Assessment:
   - Is this a proper noun? (person/place/brand names)
   - Is this a separable phrasal verb? (needs ~ notation)
   - Tilde consistency check
   - Quality score (0-100)
   - Identified issues

4. Confidence Evaluation:
   - Analysis certainty (1-10)
   - Needs secondary verification?
   - Risk factors for re-evaluation

Special considerations:
- Korean high school student perspective
- Educational curriculum context
- Separable phrasal verb patterns (pick up â†’ pick ~ up)
- Collocation importance (make effort, take care)

Response format:
{{
    "basic_info": {{
        "korean_meaning": "Korean translation",
        "pos": "confirmed part of speech",
        "is_base_form": true/false,
        "suggested_base": "base form if not base",
        "synonyms": ["synonym1", "synonym2"],
        "antonyms": ["antonym1"]
    }},
    "difficulty": {{
        "score": 7.5,
        "level": "advanced",
        "recommendation": "keep",
        "reasoning": "detailed explanation",
        "educational_value": "high/medium/low",
        "is_collocation_part": true/false
    }},
    "quality": {{
        "is_proper_noun": false,
        "proper_noun_type": "none/person/place/brand",
        "is_separable": false,
        "needs_tilde": false,
        "separable_pattern": "none/pick ~ up format",
        "quality_score": 85,
        "issues": ["list of quality issues"]
    }},
    "confidence": {{
        "certainty": 8,
        "needs_verification": false,
        "risk_factors": ["factors requiring re-evaluation"],
        "verification_priority": "none/low/medium/high"
    }}
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",  # Cost optimization
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert English educator specializing in Korean curriculum. Provide accurate, comprehensive analysis for vocabulary assessment.",
                    },
                    {"role": "user", "content": comprehensive_prompt},
                ],
                max_tokens=800,  # Increased for comprehensive analysis
                temperature=0.1,
            )

            # Parse and cache result
            content = response.choices[0].message.content.strip()
            result = self._parse_comprehensive_result(content, word, is_idiom)

            # Update tracking
            self.gpt_cache[cache_key] = result
            self.gpt_call_count += 1

            # Update token usage
            if hasattr(response, "usage"):
                usage = response.usage
                self.gpt_token_usage["prompt_tokens"] += usage.prompt_tokens
                self.gpt_token_usage["completion_tokens"] += usage.completion_tokens
                self.gpt_token_usage["total_tokens"] += usage.total_tokens

            if self.verbose:
                score = result.get("difficulty", {}).get("score", "unknown")
                level = result.get("difficulty", {}).get("level", "unknown")
                print(f"      ğŸ” Comprehensive: {word} â†’ {level} ({score}pts)")

            return result

        except Exception as e:
            if self.verbose:
                print(f"   âŒ Comprehensive analysis failed ({word}): {e}")
            return self._get_fallback_comprehensive_result(word, is_idiom)

    def _parse_comprehensive_result(self, content, word, is_idiom):
        """Parse comprehensive analysis JSON result"""
        try:
            # Extract JSON from response
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            result = json.loads(content)

            # Validate and enhance result
            result = self._validate_comprehensive_result(result, word, is_idiom)

            return result

        except Exception as e:
            if self.verbose:
                print(f"   âš ï¸ JSON parsing failed: {e}")
            return self._get_fallback_comprehensive_result(word, is_idiom)

    def _validate_comprehensive_result(self, result, word, is_idiom):
        """Validate and enhance comprehensive analysis result"""

        # Ensure all required sections exist
        if "basic_info" not in result:
            result["basic_info"] = {}
        if "difficulty" not in result:
            result["difficulty"] = {}
        if "quality" not in result:
            result["quality"] = {}
        if "confidence" not in result:
            result["confidence"] = {}

        # Validate basic_info
        basic = result["basic_info"]
        if "korean_meaning" not in basic or not basic["korean_meaning"]:
            basic["korean_meaning"] = word  # Fallback
        if "is_base_form" not in basic:
            basic["is_base_form"] = True
        if "synonyms" not in basic:
            basic["synonyms"] = []
        if "antonyms" not in basic:
            basic["antonyms"] = []

        # Validate difficulty
        difficulty = result["difficulty"]
        try:
            score = float(difficulty.get("score", 5.0))
            difficulty["score"] = max(0.5, min(10.0, score))  # Ensure 0.5-10 range
        except (ValueError, TypeError):
            difficulty["score"] = 5.0

        if difficulty.get("level") not in ["elementary", "advanced"]:
            difficulty["level"] = (
                "advanced" if difficulty["score"] >= 6 else "elementary"
            )

        if difficulty.get("recommendation") not in ["keep", "remove"]:
            difficulty["recommendation"] = (
                "keep" if difficulty["score"] >= 5 else "remove"
            )

        # Special handling for idioms
        if is_idiom:
            difficulty["score"] = max(
                7.0, difficulty["score"]
            )  # Idioms are at least 7.0
            difficulty["level"] = "advanced"
            difficulty["recommendation"] = "keep"
            difficulty["educational_value"] = "high"

        # Validate quality
        quality = result["quality"]
        if "quality_score" not in quality:
            quality["quality_score"] = 80  # Default good quality
        if "is_proper_noun" not in quality:
            quality["is_proper_noun"] = False
        if "is_separable" not in quality:
            quality["is_separable"] = False
        if "issues" not in quality:
            quality["issues"] = []

        # Validate confidence
        confidence = result["confidence"]
        try:
            certainty = int(confidence.get("certainty", 8))
            confidence["certainty"] = max(1, min(10, certainty))
        except (ValueError, TypeError):
            confidence["certainty"] = 8

        if "needs_verification" not in confidence:
            confidence["needs_verification"] = confidence["certainty"] < 7
        if "risk_factors" not in confidence:
            confidence["risk_factors"] = []
        if "verification_priority" not in confidence:
            confidence["verification_priority"] = "none"

        # Add user DB information
        word_lower = word.lower()
        result["user_db_info"] = {
            "in_user_words": word_lower in self.user_single_words,
            "in_user_idioms": word_lower in self.user_idioms,
            "user_priority": word_lower in self.user_words,
        }

        return result

    def _get_fallback_comprehensive_result(self, word, is_idiom):
        """Fallback result when GPT analysis fails"""

        word_lower = word.lower()

        # Basic heuristics
        if is_idiom:
            score, level, recommendation = 8.0, "advanced", "keep"
        elif len(word) <= 4:
            score, level, recommendation = 3.0, "elementary", "remove"
        elif len(word) <= 6:
            score, level, recommendation = 5.0, "elementary", "remove"
        else:
            score, level, recommendation = 7.0, "advanced", "keep"

        return {
            "basic_info": {
                "korean_meaning": word,
                "pos": "unknown",
                "is_base_form": True,
                "synonyms": [],
                "antonyms": [],
            },
            "difficulty": {
                "score": score,
                "level": level,
                "recommendation": recommendation,
                "reasoning": "Fallback analysis due to GPT failure",
                "educational_value": "high" if is_idiom else "medium",
            },
            "quality": {
                "is_proper_noun": False,
                "is_separable": False,
                "needs_tilde": False,
                "quality_score": 70,
                "issues": ["GPT analysis failed"],
            },
            "confidence": {
                "certainty": 5,
                "needs_verification": True,
                "risk_factors": ["fallback_analysis"],
                "verification_priority": "high",
            },
            "user_db_info": {
                "in_user_words": word_lower in self.user_single_words,
                "in_user_idioms": word_lower in self.user_idioms,
                "user_priority": word_lower in self.user_words,
            },
            "fallback": True,
        }

    def _should_include_word_comprehensive(self, word, comprehensive_result):
        """Determine if word should be included based on comprehensive analysis"""

        word_lower = word.lower()

        # 1. User DB priority
        if comprehensive_result["user_db_info"]["user_priority"]:
            return True, "user_db_priority"

        # 2. Idioms/phrases always keep
        if " " in word or "-" in word or "~" in word:
            return True, "idiom_pattern"

        # 3. GPT recommendation
        gpt_rec = comprehensive_result["difficulty"]["recommendation"]
        if gpt_rec == "keep":
            return (
                True,
                f"gpt_recommended_{comprehensive_result['difficulty']['score']}pts",
            )

        # 4. Quality issues
        quality_score = comprehensive_result["quality"]["quality_score"]
        if quality_score < 50:
            return False, f"poor_quality_{quality_score}pts"

        # 5. Proper noun check
        if comprehensive_result["quality"]["is_proper_noun"]:
            return False, "proper_noun_excluded"

        # 6. Final decision based on difficulty
        difficulty_score = comprehensive_result["difficulty"]["score"]
        if difficulty_score >= 5.5:
            return True, f"sufficient_difficulty_{difficulty_score}pts"

        return False, f"insufficient_difficulty_{difficulty_score}pts"

    def _parse_comprehensive_result(self, content, word, is_idiom):
        """Parse comprehensive analysis JSON result"""
        try:
            # Extract JSON from response
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            result = json.loads(content)

            # Validate and enhance result
            result = self._validate_comprehensive_result(result, word, is_idiom)

            return result

        except Exception as e:
            if self.verbose:
                print(f"   âš ï¸ JSON parsing failed: {e}")
            return self._get_fallback_comprehensive_result(word, is_idiom)

    def _validate_comprehensive_result(self, result, word, is_idiom):
        """Validate and enhance comprehensive analysis result"""

        # Ensure all required sections exist
        if "basic_info" not in result:
            result["basic_info"] = {}
        if "difficulty" not in result:
            result["difficulty"] = {}
        if "quality" not in result:
            result["quality"] = {}
        if "confidence" not in result:
            result["confidence"] = {}

        # Validate basic_info
        basic = result["basic_info"]
        if "korean_meaning" not in basic or not basic["korean_meaning"]:
            basic["korean_meaning"] = word  # Fallback
        if "is_base_form" not in basic:
            basic["is_base_form"] = True
        if "synonyms" not in basic:
            basic["synonyms"] = []
        if "antonyms" not in basic:
            basic["antonyms"] = []

        # Validate difficulty
        difficulty = result["difficulty"]
        try:
            score = float(difficulty.get("score", 5.0))
            difficulty["score"] = max(0.5, min(10.0, score))  # Ensure 0.5-10 range
        except (ValueError, TypeError):
            difficulty["score"] = 5.0

        if difficulty.get("level") not in ["elementary", "advanced"]:
            difficulty["level"] = (
                "advanced" if difficulty["score"] >= 6 else "elementary"
            )

        if difficulty.get("recommendation") not in ["keep", "remove"]:
            difficulty["recommendation"] = (
                "keep" if difficulty["score"] >= 5 else "remove"
            )

        # Special handling for idioms
        if is_idiom:
            difficulty["score"] = max(
                7.0, difficulty["score"]
            )  # Idioms are at least 7.0
            difficulty["level"] = "advanced"
            difficulty["recommendation"] = "keep"
            difficulty["educational_value"] = "high"

        # Validate quality
        quality = result["quality"]
        if "quality_score" not in quality:
            quality["quality_score"] = 80  # Default good quality
        if "is_proper_noun" not in quality:
            quality["is_proper_noun"] = False
        if "is_separable" not in quality:
            quality["is_separable"] = False
        if "issues" not in quality:
            quality["issues"] = []

        # Validate confidence
        confidence = result["confidence"]
        try:
            certainty = int(confidence.get("certainty", 8))
            confidence["certainty"] = max(1, min(10, certainty))
        except (ValueError, TypeError):
            confidence["certainty"] = 8

        if "needs_verification" not in confidence:
            confidence["needs_verification"] = confidence["certainty"] < 7
        if "risk_factors" not in confidence:
            confidence["risk_factors"] = []
        if "verification_priority" not in confidence:
            confidence["verification_priority"] = "none"

        # Add user DB information
        word_lower = word.lower()
        result["user_db_info"] = {
            "in_user_words": word_lower in self.user_single_words,
            "in_user_idioms": word_lower in self.user_idioms,
            "user_priority": word_lower in self.user_words,
        }

        return result

    def _get_fallback_comprehensive_result(self, word, is_idiom):
        """Fallback result when GPT analysis fails"""

        word_lower = word.lower()

        # Basic heuristics
        if is_idiom:
            score, level, recommendation = 8.0, "advanced", "keep"
        elif len(word) <= 4:
            score, level, recommendation = 3.0, "elementary", "remove"
        elif len(word) <= 6:
            score, level, recommendation = 5.0, "elementary", "remove"
        else:
            score, level, recommendation = 7.0, "advanced", "keep"

        return {
            "basic_info": {
                "korean_meaning": word,
                "pos": "unknown",
                "is_base_form": True,
                "synonyms": [],
                "antonyms": [],
            },
            "difficulty": {
                "score": score,
                "level": level,
                "recommendation": recommendation,
                "reasoning": "Fallback analysis due to GPT failure",
                "educational_value": "high" if is_idiom else "medium",
            },
            "quality": {
                "is_proper_noun": False,
                "is_separable": False,
                "needs_tilde": False,
                "quality_score": 70,
                "issues": ["GPT analysis failed"],
            },
            "confidence": {
                "certainty": 5,
                "needs_verification": True,
                "risk_factors": ["fallback_analysis"],
                "verification_priority": "high",
            },
            "user_db_info": {
                "in_user_words": word_lower in self.user_single_words,
                "in_user_idioms": word_lower in self.user_idioms,
                "user_priority": word_lower in self.user_words,
            },
            "fallback": True,
        }

    def _clean_korean_definition(self, word, text):
        """í•œê¸€ ëœ» ì •ë¦¬ í•¨ìˆ˜"""
        if not isinstance(text, str):
            return ""

        text = text.strip("\"'").strip()
        text = (
            text.replace('"', "")
            .replace("'", "")
            .replace(
                """, "")
            .replace(""",
                "",
            )
            .replace("'", "")
            .replace("'", "")
        )

        # âœ… ìˆ™ì–´/íŒ¨í„´ì¸ì§€ í™•ì¸
        if " A " in word or " B " in word or " " in word.strip():
            return text  # ì˜ì–´ í‘œí˜„ ìœ ì§€

        # âœ… ì¼ë°˜ ë‹¨ì–´ì¸ ê²½ìš° ì˜ì–´ ì œê±°
        text = re.sub(r"[a-zA-Z]{3,}", "", text)  # 3ê¸€ì ì´ìƒ ì˜ì–´ ë‹¨ì–´ ì œê±°
        return text.strip()

    def _legacy_korean_definition(self, word, sentence, is_phrase, pos_hint):
        """ê¸°ì¡´ ë°©ì‹ì˜ ì˜ë¯¸ ìƒì„± (í˜¸í™˜ì„±ìš©)"""
        basic_definitions = {
            "the": "ê·¸",
            "a": "í•˜ë‚˜ì˜",
            "an": "í•˜ë‚˜ì˜",
            "and": "ê·¸ë¦¬ê³ ",
            "or": "ë˜ëŠ”",
            "but": "í•˜ì§€ë§Œ",
            "in": "~ì•ˆì—",
            "on": "~ìœ„ì—",
        }

        if word.lower() in basic_definitions:
            return basic_definitions[word.lower()]

        if is_phrase:
            return f"{word}ì˜ ìˆ™ì–´ì  ì˜ë¯¸"
        else:
            return f"{word}ì˜ ì˜ë¯¸"

    # ğŸ”¥ í†µí•©ëœ ë‚œì´ë„ ë¶„ì„
    def enhanced_difficulty_analysis(self, word, context=""):
        """í†µí•©ëœ ë‚œì´ë„ ë¶„ì„"""
        if not self.settings["USE_INTEGRATED_DIFFICULTY"]:
            return self._legacy_difficulty_analysis(word)

        try:
            # ğŸ”¥ í†µí•©ëœ vocab_difficulty í•¨ìˆ˜ë“¤ ì‚¬ìš©
            definition, synonyms, antonyms = integrated_extract_info(word)
            difficulty_score = integrated_get_word_difficulty_score(word, nlp)

            # ì¶”ê°€ ë¶„ì„
            factors = []

            # ê¸¸ì´ ê¸°ë°˜ ë‚œì´ë„
            length_score = min(len(word) / 10, 1.0)
            factors.append(f"ê¸¸ì´: {length_score:.2f}")

            # WordNet ë³µì¡ë„
            synsets = wordnet.synsets(word)
            synset_score = min(len(synsets) / 5, 1.0)
            factors.append(f"ì˜ë¯¸ìˆ˜: {synset_score:.2f}")

            # ìŒì„±ì  ë³µì¡ë„
            phonetic_score = integrated_calculate_phonetic_complexity(word)
            factors.append(f"ìŒì„±: {phonetic_score:.2f}")

            # ìµœì¢… ë‚œì´ë„ ë ˆë²¨ ê²°ì •
            if difficulty_score < 0.3:
                level = "easy"
            elif difficulty_score < 0.6:
                level = "medium"
            elif difficulty_score < 0.8:
                level = "hard"
            else:
                level = "very_hard"

            result = {
                "word": word,
                "difficulty_score": difficulty_score,
                "level": level,
                "factors": factors,
                "definition": definition,
                "synonyms": synonyms,
                "antonyms": antonyms,
                "synset_count": len(synsets),
                "phonetic_complexity": phonetic_score,
            }

            return result

        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ í†µí•© ë‚œì´ë„ ë¶„ì„ ì‹¤íŒ¨ ({word}): {e}")
            return self._legacy_difficulty_analysis(word)

    def _should_include_word_comprehensive(self, word, comprehensive_result):
        """Determine if word should be included based on comprehensive analysis"""

        word_lower = word.lower()

        # 1. User DB priority
        if comprehensive_result["user_db_info"]["user_priority"]:
            return True, "user_db_priority"

        # 2. Idioms/phrases always keep
        if " " in word or "-" in word or "~" in word:
            return True, "idiom_pattern"

        # 3. GPT recommendation
        gpt_rec = comprehensive_result["difficulty"]["recommendation"]
        if gpt_rec == "keep":
            return (
                True,
                f"gpt_recommended_{comprehensive_result['difficulty']['score']}pts",
            )

        # 4. Quality issues
        quality_score = comprehensive_result["quality"]["quality_score"]
        if quality_score < 50:
            return False, f"poor_quality_{quality_score}pts"

        # 5. Proper noun check
        if comprehensive_result["quality"]["is_proper_noun"]:
            return False, "proper_noun_excluded"

        # 6. Final decision based on difficulty
        difficulty_score = comprehensive_result["difficulty"]["score"]
        if difficulty_score >= 6.0:
            return True, f"sufficient_difficulty_{difficulty_score}pts"

        return False, f"insufficient_difficulty_{difficulty_score}pts"

    def _legacy_difficulty_analysis(self, word):
        """ê¸°ì¡´ ë°©ì‹ì˜ ë‚œì´ë„ ë¶„ì„ (í˜¸í™˜ì„±ìš©)"""
        return {
            "word": word,
            "difficulty_score": 0.5,
            "level": "medium",
            "factors": ["ê¸°ë³¸ë¶„ì„"],
            "definition": "",
            "synonyms": [],
            "antonyms": [],
        }

    # ğŸ”¥ ìƒˆë¡œìš´ ì‚¬ìš©ì DB ìˆ™ì–´ ìš°ì„  ì¶”ì¶œ
    def extract_user_db_idioms(self, text):
        """ì‚¬ìš©ì DBì—ì„œ ìˆ™ì–´ ìš°ì„  ì¶”ì¶œ"""
        results = []
        text_str = self._force_extract_text(text)
        text_lower = text_str.lower()
        found_positions = set()

        print(f"   ğŸ” ì‚¬ìš©ì DB ìˆ™ì–´ ë§¤ì¹­ ê²€ì‚¬: {len(self.user_idioms)}ê°œ")

        if not self.user_idioms:
            print(f"   âš ï¸ ì‚¬ìš©ì DBì— ìˆ™ì–´ ì—†ìŒ")
            return results

        # ğŸ”¥ ê¸¸ì´ìˆœ ì •ë ¬ (ê¸´ ìˆ™ì–´ë¶€í„° ë§¤ì¹­í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
        sorted_user_idioms = sorted(self.user_idioms, key=len, reverse=True)

        for idiom in sorted_user_idioms:
            # ì •í™•í•œ ë§¤ì¹­ (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤)
            pattern = r"\b" + re.escape(idiom) + r"\b"
            matches = re.finditer(pattern, text_lower, re.IGNORECASE)

            for match in matches:
                start, end = match.span()

                # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                if any(abs(start - pos[0]) < 5 for pos in found_positions):
                    continue

                context = self._get_sentence_context(text_str, start, end)

                # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ í‘œí˜„ ì¶”ì¶œ
                original_text = text_str[start:end]

                # í•œê¸€ ì˜ë¯¸ ìƒì„±
                meaning = self.enhanced_korean_definition(
                    idiom, context, is_phrase=True
                )

                results.append(
                    {
                        "original": original_text,
                        "base_form": idiom,
                        "meaning": meaning,
                        "context": context,
                        "type": "user_db_idiom",  # ğŸ”¥ ì‚¬ìš©ì DB ìˆ™ì–´ í‘œì‹œ
                        "is_separated": False,
                        "confidence": 0.95,  # ì‚¬ìš©ì DBëŠ” ë†’ì€ ì‹ ë¢°ë„
                        "user_db_match": True,
                        "match_type": "ì‚¬ìš©ìDBìˆ™ì–´",
                    }
                )
                found_positions.add((start, end))

        return results

    # ğŸ”¥ ê°œì„ ëœ ìˆ™ì–´ ì¶”ì¶œ (ì‚¬ìš©ì DB ìš°ì„  + ê³ ê¸‰ íŒ¨í„´ ë¶„ì„)
    def extract_advanced_idioms(self, text):
        """ê°œì„ ëœ ìˆ™ì–´ ì¶”ì¶œ - ì‚¬ìš©ì DB ìš°ì„  + ë¶„ë¦¬í˜•/ë¬¸ë²•íŒ¨í„´ ë¶„ì„"""
        results = []
        text_str = self._force_extract_text(text)
        found_positions = set()

        try:
            # ğŸ”¥ 1. ì‚¬ìš©ì DB ìˆ™ì–´ ìš°ì„  ê²€ì‚¬ (ìµœìš°ì„ )
            user_idioms = self.extract_user_db_idioms(text)
            results.extend(user_idioms)

            # ì‚¬ìš©ì DB ìˆ™ì–´ ìœ„ì¹˜ ê¸°ë¡
            for idiom in user_idioms:
                start = text_str.lower().find(idiom["base_form"].lower())
                if start != -1:
                    end = start + len(idiom["base_form"])
                    found_positions.add((start, end))

            # ğŸ”¥ 2. ë¬¸ë²• íŒ¨í„´ ìˆ™ì–´ ê²€ì‚¬ (V-ing, N V ë“±)
            print(f"   ğŸ” ë¬¸ë²• íŒ¨í„´ ë¶„ì„ ì¤‘...")
            grammar_patterns = self.idiom_checker.analyze_grammar_pattern(text_str)

            for pattern in grammar_patterns:
                # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                if any(abs(pattern["start"] - pos[0]) < 10 for pos in found_positions):
                    continue

                context = self._get_sentence_context(
                    text_str, pattern["start"], pattern["end"]
                )
                meaning = self.enhanced_korean_definition(
                    pattern["display_form"], context, is_phrase=True
                )

                results.append(
                    {
                        "original": pattern["original"],
                        "base_form": pattern[
                            "display_form"
                        ],  # ğŸ”¥ íŒ¨í„´ í‘œì‹œ (spend time V-ing)
                        "meaning": meaning,
                        "context": context,
                        "type": "grammar_pattern",
                        "is_separated": pattern["is_separated"],
                        "confidence": 0.9,
                        "user_db_match": False,
                        "match_type": "ë¬¸ë²•íŒ¨í„´",
                    }
                )
                found_positions.add((pattern["start"], pattern["end"]))
                print(
                    f"      âœ… ë¬¸ë²• íŒ¨í„´: '{pattern['original']}' â†’ {pattern['display_form']}"
                )

            # ğŸ”¥ 3. ì°¸ì¡° DB ìˆ™ì–´ (ì‚¬ìš©ì DB ë‹¤ìŒ ìš°ì„ ìˆœìœ„)
            print(f"   ğŸ” ì°¸ì¡° ìˆ™ì–´ DBì—ì„œ ë§¤ì¹­ ê²€ì‚¬: {len(self.reference_idioms)}ê°œ")
            for idiom in self.reference_idioms:
                if idiom.lower() in text_str.lower():
                    start = text_str.lower().find(idiom.lower())
                    end = start + len(idiom)

                    # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                    if any(abs(start - pos[0]) < 10 for pos in found_positions):
                        continue

                    context = self._get_sentence_context(text_str, start, end)
                    meaning = self.enhanced_korean_definition(
                        idiom, context, is_phrase=True
                    )

                    results.append(
                        {
                            "original": idiom,
                            "base_form": idiom,
                            "meaning": meaning,
                            "context": context,
                            "type": "reference_idiom_db",
                            "is_separated": False,
                            "confidence": 0.85,
                            "user_db_match": False,
                            "match_type": "ì°¸ì¡°DB",
                        }
                    )
                    found_positions.add((start, end))

            # ğŸ”¥ 4. SpaCy ê¸°ë°˜ ê³ ê¸‰ êµ¬ë™ì‚¬ ì¶”ì¶œ
            spacy_results = self._extract_advanced_phrasal_verbs(
                text_str, found_positions
            )
            results.extend(spacy_results)

            # ğŸ”¥ 5. ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
            pattern_results = self._extract_simple_patterns(text_str, found_positions)
            results.extend(pattern_results)

            # ì‹ ë¢°ë„ ë° ì‚¬ìš©ì DB ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
            results.sort(
                key=lambda x: (-x.get("user_db_match", False), -x.get("confidence", 0))
            )

        except Exception as e:
            if self.verbose:
                print(f"ìˆ™ì–´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return results

    def _extract_advanced_phrasal_verbs(self, text, found_positions):
        """ê³ ê¸‰ êµ¬ë™ì‚¬ ì¶”ì¶œ - ì—°ì†í˜•/ë¶„ë¦¬í˜•/íŒ¨í„´ êµ¬ë¶„"""
        results = []
        try:
            doc = nlp(text)

            # ì˜ì¡´ì„± íŒŒì‹±ì„ ì´ìš©í•œ êµ¬ë™ì‚¬ íƒì§€
            for token in doc:
                if token.pos_ == "VERB":
                    # ë™ì‚¬ì˜ ëª¨ë“  ìì‹ í† í°ë“¤ í™•ì¸
                    particles = []

                    for child in token.children:
                        # particle (prt) ì˜ì¡´ì„±ì„ ê°€ì§„ í† í°ë“¤ ì°¾ê¸°
                        if child.dep_ == "prt":
                            particles.append(child)

                    # êµ¬ë™ì‚¬ê°€ ë°œê²¬ë˜ë©´
                    if particles:
                        for particle in particles:
                            # ğŸ”¥ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„
                            pattern_analysis = (
                                self.idiom_checker.analyze_phrasal_verb_pattern(
                                    text, token, particle
                                )
                            )

                            # ìœ„ì¹˜ ê³„ì‚°
                            if pattern_analysis["is_separated"]:
                                # ë¶„ë¦¬í˜•ì¸ ê²½ìš° ì „ì²´ ë²”ìœ„ ê³„ì‚°
                                start = min(token.idx, particle.idx)
                                end = max(
                                    token.idx + len(token.text),
                                    particle.idx + len(particle.text),
                                )
                                # ì¤‘ê°„ì— ìˆëŠ” ë‹¨ì–´ë“¤ë„ í¬í•¨
                                for i in range(
                                    min(token.i, particle.i) + 1,
                                    max(token.i, particle.i),
                                ):
                                    if i < len(doc):
                                        end = max(end, doc[i].idx + len(doc[i].text))
                                original_text = text[start:end].strip()
                            else:
                                # ì—°ì†í˜•ì¸ ê²½ìš°
                                start = min(token.idx, particle.idx)
                                end = max(
                                    token.idx + len(token.text),
                                    particle.idx + len(particle.text),
                                )
                                original_text = text[start:end].strip()

                            # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                            if any(abs(start - pos[0]) < 5 for pos in found_positions):
                                continue

                            context = self._get_sentence_context(text, start, end)

                            # ğŸ”¥ í‘œì‹œ í˜•íƒœ ê²°ì •
                            display_form = pattern_analysis["display_form"]
                            base_form = pattern_analysis["base_form"]

                            meaning = self.enhanced_korean_definition(
                                base_form, context, is_phrase=True
                            )

                            results.append(
                                {
                                    "original": original_text,
                                    "base_form": display_form,  # ğŸ”¥ íŒ¨í„´ì— ë”°ë¥¸ í‘œì‹œ (pick up vs pick ~ up)
                                    "meaning": meaning,
                                    "context": context,
                                    "type": f"advanced_phrasal_{pattern_analysis['pattern_type']}",
                                    "is_separated": pattern_analysis["is_separated"],
                                    "confidence": (
                                        0.9 if pattern_analysis["is_separated"] else 0.8
                                    ),
                                    "user_db_match": False,
                                    "match_type": f"ê³ ê¸‰êµ¬ë™ì‚¬({pattern_analysis['pattern_type']})",
                                    "pattern_info": pattern_analysis,  # ğŸ”¥ íŒ¨í„´ ìƒì„¸ ì •ë³´
                                }
                            )
                            found_positions.add((start, end))

                            pattern_desc = {
                                "optional_separable": "ì—°ì†í˜•ê°€ëŠ¥",
                                "mandatory_separable": "ë¶„ë¦¬í•„ìˆ˜",
                                "actually_separated": "ì‹¤ì œë¶„ë¦¬",
                                "continuous": "ì—°ì†í˜•",
                            }.get(
                                pattern_analysis["pattern_type"],
                                pattern_analysis["pattern_type"],
                            )

                            print(
                                f"      âœ… ê³ ê¸‰ êµ¬ë™ì‚¬ ({pattern_desc}): '{original_text}' â†’ {display_form}"
                            )

        except Exception as e:
            if self.verbose:
                print(f"ê³ ê¸‰ êµ¬ë™ì‚¬ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return results

    def _extract_simple_patterns(self, text, found_positions):
        """ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­"""
        results = []

        # ì•Œë ¤ì§„ ìˆ™ì–´ íŒ¨í„´ë“¤
        simple_patterns = [
            r"\bas\s+\w+\s+as\b",  # as ... as
            r"\bin\s+order\s+to\b",  # in order to
            r"\bas\s+a\s+result\b",  # as a result
            r"\bon\s+the\s+other\s+hand\b",  # on the other hand
            r"\bfor\s+instance\b",  # for instance
            r"\bin\s+spite\s+of\b",  # in spite of
            r"\bbecause\s+of\b",  # because of
            r"\binstead\s+of\b",  # instead of
            r"\baccording\s+to\b",  # according to
            r"\bas\s+well\s+as\b",  # as well as
        ]

        for pattern in simple_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                start, end = match.span()

                # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                if any(abs(start - pos[0]) < 5 for pos in found_positions):
                    continue

                phrase = match.group()
                context = self._get_sentence_context(text, start, end)
                meaning = self.enhanced_korean_definition(
                    phrase, context, is_phrase=True
                )

                results.append(
                    {
                        "original": phrase,
                        "base_form": phrase.lower(),
                        "meaning": meaning,
                        "context": context,
                        "type": "simple_pattern",
                        "is_separated": False,
                        "confidence": 0.7,
                        "user_db_match": False,
                        "match_type": "íŒ¨í„´ë§¤ì¹­",
                    }
                )
                found_positions.add((start, end))
                print(f"      âœ… íŒ¨í„´ ë§¤ì¹­: {phrase}")

        return results

    # ğŸ”¥ ê°œì„ ëœ ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ (ì‚¬ìš©ì DB ìš°ì„ )
    def extract_difficult_words(self, text, easy_words, child_vocab, freq_tiers):
        """ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  ì¶”ì¶œ + GPT í•„í„°ë§"""
        text_str = self._force_extract_text(text)
        word_candidates = []
        user_db_candidates = []  # ğŸ”¥ ì‚¬ìš©ì DB ë‹¨ì–´ ë³„ë„ ê´€ë¦¬

        # ğŸ”¥ ì›í˜• ê¸°ì¤€ ì¤‘ë³µ ì¶”ì 
        seen_lemmas = set()  # ì´ë¯¸ ì²˜ë¦¬ëœ ì›í˜•ë“¤
        lemma_to_info = {}  # ì›í˜• â†’ ë‹¨ì–´ ì •ë³´ ë§¤í•‘

        try:
            doc = nlp(text_str)
            for token in doc:
                word = token.text.lower()
                lemma = token.lemma_.lower()
                original_word = token.text

                # ê¸°ë³¸ í•„í„°ë§
                if (
                    len(word) < 3
                    or not word.isalpha()
                    or token.is_stop
                    or token.pos_ in ["PUNCT", "SPACE", "SYM"]
                ):
                    continue

                # ğŸ”¥ ì›í˜• ê¸°ì¤€ ì¤‘ë³µ ì²´í¬
                if lemma in seen_lemmas:
                    continue  # ì´ë¯¸ ì²˜ë¦¬ëœ ì›í˜•ì€ ê±´ë„ˆë›°ê¸°

                seen_lemmas.add(lemma)

                # ë¬¸ë§¥ ì¶”ì¶œ
                context = self._get_sentence_context(
                    text_str, token.idx, token.idx + len(token.text)
                )

                word_info = {
                    "word": original_word,  # ì›ë³¸ í˜•íƒœ (decided)
                    "lemma": lemma,  # ì›í˜• (decide)
                    "context": context,
                    "pos": self._get_simple_pos(token.pos_),
                    "token_info": {
                        "start": token.idx,
                        "end": token.idx + len(token.text),
                        "original": original_word,
                    },
                }

                # ğŸ”¥ ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  ë¶„ë¦¬ (ì›í˜• ê¸°ì¤€)
                if lemma in self.user_single_words or word in self.user_single_words:
                    user_db_candidates.append(word_info)
                    print(
                        f"      âœ… ì‚¬ìš©ì DB ë‹¨ì–´ ë°œê²¬: '{original_word}' (ì›í˜•: {lemma})"
                    )
                else:
                    # 1ì°¨ ë¹ ë¥¸ í•„í„°ë§ (ì›í˜• ê¸°ì¤€)
                    if lemma in easy_words or (child_vocab and lemma in child_vocab):
                        continue
                    word_candidates.append(word_info)

        except Exception as e:
            print(f"âŒ í† í° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

        final_words = []

        # ğŸ”¥ ì‚¬ìš©ì DB ë‹¨ì–´ë“¤ ì²˜ë¦¬ (ì›í˜•ìœ¼ë¡œ ì˜ë¯¸ ìƒì„±)
        print(f"   ğŸ‘¤ ì‚¬ìš©ì DB ë‹¨ì–´ ì²˜ë¦¬: {len(user_db_candidates)}ê°œ")
        for word_info in user_db_candidates:
            korean_meaning = self.enhanced_korean_definition(
                word_info["lemma"],  # ğŸ”¥ ì›í˜•ìœ¼ë¡œ ì˜ë¯¸ ìƒì„±
                word_info["context"],
                is_phrase=False,
            )

            word_result = {
                "original": word_info["token_info"]["original"],  # ì›ë³¸ í˜•íƒœ í‘œì‹œ
                "lemma": word_info["lemma"],  # ì›í˜•
                "pos": word_info["pos"],
                "korean_meaning": korean_meaning,
                "context": word_info["context"],
                "difficulty_score": 8.0,
                "difficulty_level": "user_priority",
                "confidence": 1.0,
                "inclusion_reason": "ì‚¬ìš©ìDBìš°ì„ í¬í•¨",
                "user_db_match": True,
                "is_separated": False,
                "match_type": "ì‚¬ìš©ìDBë‹¨ì–´",
                "gpt_filtered": False,
                "english_definition": "",
                "synonyms": "",
                "antonyms": "",
            }
            final_words.append(word_result)

        # ğŸ”¥ ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ë„ ì›í˜• ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
        if word_candidates and hasattr(self, "gpt_filter") and self.gpt_filter:
            print(f"   ğŸ¤– GPT í•„í„°ë§: {len(word_candidates)}ê°œ í›„ë³´")
            gpt_results = self._process_words_with_gpt_filter(word_candidates)
            final_words.extend(gpt_results)
        else:
            print(f"   âš™ï¸ ê¸°ì¡´ ë°©ì‹: {len(word_candidates)}ê°œ í›„ë³´")
            traditional_results = self._process_words_without_gpt_filter(
                word_candidates, freq_tiers
            )
            final_words.extend(traditional_results)

        user_db_count = len([w for w in final_words if w.get("user_db_match", False)])
        gpt_count = len([w for w in final_words if not w.get("user_db_match", False)])

        print(
            f"   ğŸ“Š ìµœì¢… ê²°ê³¼: ì‚¬ìš©ìDB {user_db_count}ê°œ + ê¸°íƒ€ {gpt_count}ê°œ = ì´ {len(final_words)}ê°œ"
        )

        return final_words

    def is_word_appropriate_for_extraction(self, word, context="", pos=""):
        """í†µí•©ëœ ë‹¨ì–´ ì í•©ì„± íŒë³„"""

        word_lower = word.lower()

        # ğŸ”¥ ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€
        print(f"   ğŸ” ë””ë²„ê·¸: '{word}' â†’ '{word_lower}'")
        print(f"   ğŸ” ì‚¬ìš©ìDBí¬ê¸°: {len(self.user_words)}")
        print(f"   ğŸ” ì‚¬ìš©ìDBí¬í•¨ì—¬ë¶€: {word_lower in self.user_words}")
        if len(self.user_words) > 0:
            print(f"   ğŸ” ì‚¬ìš©ìDBìƒ˜í”Œ: {list(self.user_words)[:3]}")

        # ì‚¬ìš©ì DBì— ì´ë¯¸ ìˆëŠ” ë‹¨ì–´ëŠ” ë¬´ì¡°ê±´ ì í•©
        if word_lower in self.user_words:
            return True, "ì‚¬ìš©ìDBí¬í•¨"

        # ğŸ”¥ 1ë‹¨ê³„: ê¸°ë³¸ ë‹¨ì–´ ê°•ë ¥ ì°¨ë‹¨
        if (
            word_lower in BASIC_VERBS
            or word_lower in BASIC_ADJECTIVES
            or word_lower in BASIC_NOUNS
        ):
            return False, "ê¸°ë³¸ë‹¨ì–´ì œì™¸"

        # ğŸ”¥ 2ë‹¨ê³„: ì™¸ë¶€ DB ê¸°ë³¸ ì–´íœ˜ ì°¨ë‹¨
        if is_basic_by_external_db(word_lower):
            return False, "ì™¸ë¶€DBê¸°ë³¸ì–´íœ˜"

        # ğŸ”¥ 3ë‹¨ê³„: ì‚¬ìš©ì DB ìš°ì„  í¬í•¨
        if word_lower in self.user_single_words:
            return True, "ì‚¬ìš©ìDBìš°ì„ í¬í•¨"

        # ğŸ”¥ 4ë‹¨ê³„: GPT ë¬¸ë§¥ë³„ ë‚œì´ë„ ë¶„ì„
        contextual_analysis = self._gpt_analyze_contextual_difficulty(
            word, context, pos
        )
        general_analysis = self._gpt_analyze_word_difficulty(word, context, pos)

        # ë‘ ë¶„ì„ ê²°ê³¼ ëª¨ë‘ ê³ ë ¤
        return self._determine_final_appropriateness(
            word, contextual_analysis, general_analysis
        )

    def _process_words_with_gpt_filter(self, word_candidates):
        """GPT í•„í„°ë¥¼ ì‚¬ìš©í•œ ë‹¨ì–´ ì²˜ë¦¬ (ì‚¬ìš©ì DB ìš°ì„  ë³´ì¥)"""
        if not hasattr(self, "gpt_filter") or not self.gpt_filter:
            print("   âŒ GPT í•„í„°ê°€ ì—†ì–´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
            return self._process_words_without_gpt_filter(word_candidates, {})

        print(f"   ğŸ¤– GPT í•„í„°ë§ ëª¨ë“œ: {len(word_candidates)}ê°œ í›„ë³´")

        # ğŸ”¥ ì‚¬ìš©ì DB ë‹¨ì–´ ì‚¬ì „ ë¶„ë¦¬
        user_db_words = []
        regular_words = []

        for word_info in word_candidates:
            word_lower = word_info["word"].lower().strip()
            lemma_lower = word_info["lemma"].lower().strip()

            if (
                word_lower in self.gpt_filter.user_words
                or lemma_lower in self.gpt_filter.user_words
            ):
                user_db_words.append(word_info)
                print(f"      ğŸ‘¤ ì‚¬ìš©ì DB í™•ì¸: '{word_info['word']}'")
            else:
                regular_words.append(word_info)

        final_words = []

        # ì‚¬ìš©ì DB ë‹¨ì–´ë“¤ ë¬´ì¡°ê±´ í¬í•¨
        for word_info in user_db_words:
            korean_meaning = self.enhanced_korean_definition(
                word_info["lemma"], word_info["context"], is_phrase=False
            )

            word_result = {
                "original": word_info["token_info"]["original"],
                "lemma": word_info["lemma"],
                "pos": word_info["pos"],
                "korean_meaning": korean_meaning,
                "context": word_info["context"],
                "difficulty_score": 8.0,
                "difficulty_level": "user_priority",
                "confidence": 1.0,
                "inclusion_reason": "ì‚¬ìš©ìDBìš°ì„ í¬í•¨",
                "user_db_match": True,
                "is_separated": False,
                "match_type": "ì‚¬ìš©ìDBìš°ì„ ",
                "gpt_filtered": False,
                "english_definition": "",
                "synonyms": "",
                "antonyms": "",
            }
            final_words.append(word_result)

        # ğŸ”¥ ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ë§Œ GPT í•„í„°ë§
        if regular_words:
            words_for_filtering = []
            for word_info in regular_words:
                words_for_filtering.append(
                    {
                        "word": word_info["word"],
                        "context": word_info["context"],
                        "pos": word_info["pos"],
                    }
                )

            try:
                print(f"   ğŸ“ GPT ë°°ì¹˜ í•„í„°ë§ ì‹œì‘: {len(regular_words)}ê°œ...")
                filter_results = self.gpt_filter.batch_filter_words(words_for_filtering)

                gpt_selected_count = 0
                for i, filter_result in enumerate(filter_results):
                    if filter_result["appropriate"]:
                        word_info = regular_words[i]
                        korean_meaning = self.enhanced_korean_definition(
                            word_info["lemma"],  # ğŸ”¥ ì›í˜•ìœ¼ë¡œ ì˜ë¯¸ ìƒì„±
                            word_info["context"],
                            is_phrase=False,
                        )

                        word_result = {
                            "original": word_info["token_info"]["original"],
                            "lemma": word_info["lemma"],
                            "pos": word_info["pos"],
                            "korean_meaning": korean_meaning,
                            "context": word_info["context"],
                            "difficulty_score": 0.0,
                            "difficulty_level": "gpt_selected",
                            "confidence": 0.8,
                            "inclusion_reason": filter_result["reason"],
                            "user_db_match": False,
                            "is_separated": False,
                            "match_type": "GPTí•„í„°ìŠ¹ì¸",
                            "gpt_filtered": True,
                            "english_definition": "",
                            "synonyms": "",
                            "antonyms": "",
                        }
                        final_words.append(word_result)
                        gpt_selected_count += 1

                print(
                    f"   âœ… GPT í•„í„°ë§ ì™„ë£Œ: {gpt_selected_count}/{len(regular_words)}ê°œ ì„ íƒ"
                )

            except Exception as e:
                print(f"   âŒ GPT í•„í„°ë§ ì‹¤íŒ¨: {e}")

        user_db_count = len(user_db_words)
        gpt_count = len([w for w in final_words if not w.get("user_db_match", False)])

        print(
            f"   ğŸ“Š í•„í„°ë§ ê²°ê³¼: ì‚¬ìš©ìDB {user_db_count}ê°œ + GPT {gpt_count}ê°œ = ì´ {len(final_words)}ê°œ"
        )

        return final_words

    def _process_words_without_gpt_filter(self, word_candidates, freq_tiers):
        """GPT í•„í„° ì—†ì´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë‹¨ì–´ ì²˜ë¦¬"""

        print(f"   âš™ï¸ ê¸°ì¡´ ë°©ì‹ í•„í„°ë§: {len(word_candidates)}ê°œ í›„ë³´")

        final_words = []

        for word_info in word_candidates:
            word = word_info["word"]
            lemma = word_info["lemma"]  # ğŸ”¥ ì›í˜• ì‚¬ìš©
            context = word_info["context"]
            pos = word_info["pos"]

            # í†µí•© ë¶„ì„ ì‚¬ìš© (ì›í˜•ìœ¼ë¡œ)
            comprehensive_result = self.integrated_comprehensive_analysis(
                lemma, context, pos  # ğŸ”¥ ì›í˜•ìœ¼ë¡œ ë¶„ì„
            )

            # í¬í•¨ ì—¬ë¶€ ê²°ì •
            should_include, reason = self._should_include_word_comprehensive(
                lemma, comprehensive_result  # ğŸ”¥ ì›í˜• ê¸°ì¤€ íŒë‹¨
            )

            if should_include:
                word_result = {
                    "original": word_info["token_info"]["original"],  # ì›ë³¸ í˜•íƒœ
                    "lemma": lemma,  # ì›í˜•
                    "pos": pos,
                    "korean_meaning": comprehensive_result["basic_info"][
                        "korean_meaning"
                    ],
                    "context": context,
                    "difficulty_score": comprehensive_result["difficulty"]["score"],
                    "difficulty_level": comprehensive_result["difficulty"]["level"],
                    "recommendation": comprehensive_result["difficulty"][
                        "recommendation"
                    ],
                    "quality_score": comprehensive_result["quality"]["quality_score"],
                    "confidence": comprehensive_result["confidence"]["certainty"],
                    "needs_verification": comprehensive_result["confidence"][
                        "needs_verification"
                    ],
                    "inclusion_reason": reason,
                    "user_db_match": comprehensive_result["user_db_info"][
                        "user_priority"
                    ],
                    "is_separated": comprehensive_result["quality"]["is_separable"],
                    "comprehensive_analysis": True,  # í‘œì‹œìš©
                    # ê¸°ì¡´ í•„ë“œë“¤ ì¶”ê°€
                    "match_type": "í†µí•©ë¶„ì„",
                    "gpt_filtered": False,
                    "difficulty_factors": [],
                    "english_definition": "",
                    "synonyms": ", ".join(
                        comprehensive_result["basic_info"]["synonyms"]
                    ),
                    "antonyms": ", ".join(
                        comprehensive_result["basic_info"]["antonyms"]
                    ),
                }
                final_words.append(word_result)

        user_matches = len([w for w in final_words if w["user_db_match"]])
        print(f"   ğŸ“Š ê¸°ì¡´ ë°©ì‹ ê²°ê³¼: {len(final_words)}ê°œ ì„ íƒ")
        print(f"   ğŸ‘¤ ì‚¬ìš©ì DB ë§¤ì¹­: {user_matches}ê°œ")

        return final_words

    def _legacy_is_difficult(self, word, freq_tiers):
        """ê¸°ì¡´ ë°©ì‹ì˜ ë‚œì´ë„ íŒë³„"""
        if len(word) < self.MIN_WORD_LENGTH:
            return False
        if word in freq_tiers:
            return freq_tiers[word] >= self.DIFFICULTY_THRESHOLD
        synsets = wordnet.synsets(word)
        return len(synsets) > 0

    def _force_extract_text(self, text, max_depth=5):
        """í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        depth = 0
        current = text
        while isinstance(current, dict) and depth < max_depth:
            if "ì§€ë¬¸" in current:
                current = current["ì§€ë¬¸"]
            else:
                current = list(current.values())[0] if current else ""
            depth += 1
        return str(current)

    def _get_sentence_context(self, text, start, end):
        """ë¬¸ì¥ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_str = self._force_extract_text(text) if not isinstance(text, str) else text
        left = text_str.rfind(".", 0, start)
        right = text_str.find(".", end)

        if left != -1 and right != -1:
            return text_str[left + 1 : right].strip()
        elif left != -1:
            return text_str[left + 1 :].strip()
        elif right != -1:
            return text_str[:right].strip()
        else:
            return text_str.strip()

    def _get_simple_pos(self, spacy_pos):
        """SpaCy í’ˆì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ë³€í™˜"""
        return {"NOUN": "ëª…ì‚¬", "VERB": "ë™ì‚¬", "ADJ": "í˜•ìš©ì‚¬", "ADV": "ë¶€ì‚¬"}.get(
            spacy_pos, "ê¸°íƒ€"
        )

    # ğŸ”¥ 2. add_synonyms_antonyms_to_results ë©”ì„œë“œ ì™„ì „ êµì²´
    def add_synonyms_antonyms_to_results(self, results):
        """ë¬¸ë§¥ ê¸°ë°˜ ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ë°€ ì¶”ì¶œ (ìµœëŒ€ 3ê°œ ì œí•œ)"""

        if not hasattr(self, "synonym_extractor") or not self.synonym_extractor:
            print("âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return self._add_empty_synonym_columns(results)

        print("ğŸ” ë¬¸ë§¥ ê¸°ë°˜ ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ë°€ ì¶”ì¶œ ì¤‘... (ìµœëŒ€ 3ê°œ ì œí•œ)")

        # ë¬¸ë§¥ ê¸°ë°˜ ì¶”ì¶œê¸°ì¸ì§€ í™•ì¸
        if hasattr(self.synonym_extractor, "enhanced_process_vocabulary"):
            return self._extract_with_contextual_precision(results)
        else:
            return self._extract_with_enhanced_filtering(results)

    def _extract_with_gpt_module(self, results):
        """GPT ëª¨ë“ˆì„ ì‚¬ìš©í•œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            word_list = []
            for result in results:
                word_info = {
                    "word": result.get("ë‹¨ì–´", ""),
                    "context": result.get("ë¬¸ë§¥", ""),
                    "pos": result.get("í’ˆì‚¬", ""),
                    "meaning": result.get("ëœ»(í•œê¸€)", ""),
                }
                word_list.append(word_info)

            synonym_results = self.synonym_extractor.batch_extract(word_list)

            for result in results:
                word = result.get("ë‹¨ì–´", "")
                if word in synonym_results:
                    syn_data = synonym_results[word]

                    # ğŸ”¥ ì¶”ê°€ í•„í„°ë§ ì ìš©
                    raw_synonyms = syn_data.get("synonyms", [])
                    raw_antonyms = syn_data.get("antonyms", [])

                    # ì—„ê²©í•œ í•„í„°ë§ ì ìš©
                    filtered_synonyms = enhanced_filter_synonyms_antonyms(
                        raw_synonyms, word, max_count=3
                    )
                    filtered_antonyms = enhanced_filter_synonyms_antonyms(
                        raw_antonyms, word, max_count=2
                    )

                    result["ë™ì˜ì–´"] = ", ".join(filtered_synonyms)
                    result["ë°˜ì˜ì–´"] = ", ".join(filtered_antonyms)
                else:
                    result["ë™ì˜ì–´"] = ""
                    result["ë°˜ì˜ì–´"] = ""

            print("âœ… GPT ê¸°ë°˜ ì¶”ì¶œ ì™„ë£Œ (í•„í„°ë§ ì ìš©)")
            return results

        except Exception as e:
            print(f"âŒ GPT ì¶”ì¶œ ì‹¤íŒ¨: {e}, WordNetìœ¼ë¡œ ì „í™˜")
            return self._extract_with_wordnet_enhanced(results)

    def _extract_with_contextual_precision(self, results):
        """ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ì¶”ì¶œ"""
        try:
            print("   ğŸ¯ ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ëª¨ë“œ ì‚¬ìš©")

            # vocabulary í˜•íƒœë¡œ ë³€í™˜
            vocabulary = []
            for result in results:
                vocab_item = {
                    "word": result.get("ë‹¨ì–´", result.get("original", "")),
                    "context": result.get("ë¬¸ë§¥", result.get("context", "")),
                    "pos": result.get("í’ˆì‚¬", result.get("pos", "")),
                    "meaning": result.get("ëœ»(í•œê¸€)", result.get("korean_meaning", "")),
                }
                vocabulary.append(vocab_item)

            # ë¬¸ë§¥ ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬
            enhanced_vocab = self.synonym_extractor.enhanced_process_vocabulary(
                vocabulary
            )

            # ê²°ê³¼ì— ë°˜ì˜
            for i, result in enumerate(results):
                if i < len(enhanced_vocab):
                    enhanced_item = enhanced_vocab[i]

                    # ğŸ”¥ ìµœëŒ€ 3ê°œ ì œí•œ ì ìš©
                    synonyms = enhanced_item.get("contextual_synonyms", [])[:3]
                    antonyms = enhanced_item.get("contextual_antonyms", [])[:2]

                    result["ë™ì˜ì–´"] = ", ".join(synonyms) if synonyms else ""
                    result["ë°˜ì˜ì–´"] = ", ".join(antonyms) if antonyms else ""

                    # ì¶”ê°€ ì •ë³´
                    result["ì˜ë¯¸ì •í™•ë„"] = enhanced_item.get("meaning_accuracy", 0)
                    result["ë¬¸ë§¥ì í•©ë„"] = enhanced_item.get("contextual_fitness", 0)

            contextual_count = len(
                [r for r in results if r.get("ë™ì˜ì–´") or r.get("ë°˜ì˜ì–´")]
            )
            print(f"   âœ… ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ì¶”ì¶œ ì™„ë£Œ: {contextual_count}ê°œ í•­ëª©")

            return results

        except Exception as e:
            print(f"   âŒ ë¬¸ë§¥ ê¸°ë°˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì „í™˜")
            return self._extract_with_enhanced_filtering(results)

    def _extract_with_enhanced_filtering(self, results):
        """ê¸°ì¡´ ë°©ì‹ + í–¥ìƒëœ í•„í„°ë§"""
        try:
            print("   âš™ï¸ ê¸°ì¡´ ë°©ì‹ + í–¥ìƒëœ í•„í„°ë§ ëª¨ë“œ")

            word_list = []
            for result in results:
                word_info = {
                    "word": result.get("ë‹¨ì–´", result.get("original", "")),
                    "context": result.get("ë¬¸ë§¥", result.get("context", "")),
                    "pos": result.get("í’ˆì‚¬", result.get("pos", "")),
                    "meaning": result.get("ëœ»(í•œê¸€)", result.get("korean_meaning", "")),
                }
                word_list.append(word_info)

            # ê¸°ì¡´ ë°°ì¹˜ ì¶”ì¶œ
            synonym_results = self.synonym_extractor.batch_extract(word_list)

            for result in results:
                word = result.get("ë‹¨ì–´", result.get("original", ""))
                if word in synonym_results:
                    syn_data = synonym_results[word]

                    # ğŸ”¥ í–¥ìƒëœ í•„í„°ë§ ì ìš©
                    raw_synonyms = syn_data.get("synonyms", [])
                    raw_antonyms = syn_data.get("antonyms", [])

                    # ë¬¸ë§¥ ì í•©ì„± ê²€ì‚¬ (ê°„ë‹¨ ë²„ì „)
                    context = result.get("ë¬¸ë§¥", result.get("context", ""))
                    filtered_synonyms = self._simple_contextual_filter(
                        raw_synonyms, word, context, max_count=3
                    )
                    filtered_antonyms = self._simple_contextual_filter(
                        raw_antonyms, word, context, max_count=2
                    )

                    result["ë™ì˜ì–´"] = ", ".join(filtered_synonyms)
                    result["ë°˜ì˜ì–´"] = ", ".join(filtered_antonyms)
                else:
                    result["ë™ì˜ì–´"] = ""
                    result["ë°˜ì˜ì–´"] = ""

            print("   âœ… í–¥ìƒëœ í•„í„°ë§ ì™„ë£Œ")
            return results

        except Exception as e:
            print(f"   âŒ í–¥ìƒëœ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return self._add_empty_synonym_columns(results)

    def _simple_contextual_filter(
        self, candidates, original_word, context, max_count=3
    ):
        """ê°„ë‹¨í•œ ë¬¸ë§¥ ê¸°ë°˜ í•„í„°ë§"""
        if not candidates:
            return []

        # ğŸ”¥ ê¸°ë³¸ í•„í„°ë§ (ë‘ ê°œ ì´ìƒ ë‹¨ì–´, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        filtered = []
        for candidate in candidates:
            candidate = candidate.strip()

            # ë‘ ê°œ ì´ìƒ ë‹¨ì–´ ì œê±°
            if " " in candidate or "-" in candidate or "_" in candidate:
                continue

            # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ì œê±°
            if not candidate.replace("'", "").isalpha():
                continue

            # ê¸¸ì´ ì œí•œ
            if len(candidate) < 3 or len(candidate) > 12:
                continue

            # ì›ë³¸ê³¼ ë„ˆë¬´ ìœ ì‚¬í•œ ê²ƒ ì œê±°
            if candidate.lower() == original_word.lower():
                continue

            filtered.append(candidate)

        # ğŸ”¥ ì–´ê·¼ ì¤‘ë³µ ì œê±°
        unique_filtered = self._remove_root_duplicates(filtered, original_word)

        # ğŸ”¥ ê°œìˆ˜ ì œí•œ
        return unique_filtered[:max_count]

    def _remove_root_duplicates(self, words, original_word):
        """ì–´ê·¼ ì¤‘ë³µ ì œê±°"""
        try:
            from nltk.stem import PorterStemmer

            ps = PorterStemmer()

            seen_stems = set()
            unique_words = []

            # ì›ë³¸ ë‹¨ì–´ì˜ ì–´ê·¼ë„ ì¶”ê°€
            original_stem = ps.stem(original_word.lower())
            seen_stems.add(original_stem)

            for word in words:
                stem = ps.stem(word.lower())
                if stem not in seen_stems:
                    unique_words.append(word)
                    seen_stems.add(stem)

            return unique_words
        except:
            # NLTK ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì¤‘ë³µ ì œê±°ë§Œ
            return list(dict.fromkeys(words))

    def _add_empty_synonym_columns(self, results):
        """ë™ì˜ì–´/ë°˜ì˜ì–´ ë¹ˆ ì»¬ëŸ¼ ì¶”ê°€"""
        for result in results:
            result["ë™ì˜ì–´"] = ""
            result["ë°˜ì˜ì–´"] = ""
        return results

    def _extract_with_wordnet_enhanced(self, results):
        """WordNetì„ ì‚¬ìš©í•œ fallback ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        print("ğŸ” ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì¤‘... (í•„í„°ë§ ì ìš©)")

        try:
            from nltk.corpus import wordnet

            for result in results:
                word = result.get("ë‹¨ì–´", "").lower()

                # ê¸°ë³¸ê°’ ì„¤ì •
                result["ë™ì˜ì–´"] = ""
                result["ë°˜ì˜ì–´"] = ""

                if word and len(word) > 2:
                    try:
                        synonyms = set()
                        antonyms = set()

                        # WordNetì—ì„œ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ
                        for syn in wordnet.synsets(word):
                            for lemma in syn.lemmas():
                                # ë™ì˜ì–´
                                if lemma.name().lower() != word:
                                    synonyms.add(lemma.name().replace("_", " "))

                                # ë°˜ì˜ì–´
                                for antonym in lemma.antonyms():
                                    antonyms.add(antonym.name().replace("_", " "))

                        # ğŸ”¥ ì—„ê²©í•œ í•„í„°ë§ ì ìš©
                        filtered_synonyms = enhanced_filter_synonyms_antonyms(
                            list(synonyms), word, max_count=3
                        )
                        filtered_antonyms = enhanced_filter_synonyms_antonyms(
                            list(antonyms), word, max_count=2
                        )

                        # ê²°ê³¼ì— ì¶”ê°€
                        if filtered_synonyms:
                            result["ë™ì˜ì–´"] = ", ".join(filtered_synonyms)
                        if filtered_antonyms:
                            result["ë°˜ì˜ì–´"] = ", ".join(filtered_antonyms)

                    except Exception as e:
                        if self.verbose:
                            print(f"   âš ï¸ {word} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue

        except Exception as e:
            print(f"âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")  # ì‹¤íŒ¨í•˜ë©´ ëª¨ë“  ê²°ê³¼ì— ë¹ˆ ê°’ ì„¤ì •
            for result in results:
                result["ë™ì˜ì–´"] = ""
                result["ë°˜ì˜ì–´"] = ""

        return results

    # ğŸ”¥ ê°œì„ ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ê³ ê¸‰ íŒ¨í„´ ë¶„ì„)
    def process_text(self, text, text_id, easy_words, child_vocab, freq_tiers):
        """ê°œì„ ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ - ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  ë³´ì¥"""
        text_str = self._force_extract_text(text)
        rows = []

        if self.gpt_token_usage["total_tokens"] >= self.MAX_TOKENS:
            print(f"âš ï¸ í† í° ì‚¬ìš©ëŸ‰ í•œë„ ë„ë‹¬. ì§€ë¬¸ {text_id} ì²˜ë¦¬ ì œí•œë¨.")
            return []

        print(f"ğŸ“ ì§€ë¬¸ {text_id} ì²˜ë¦¬ ì‹œì‘ (ì‚¬ìš©ì DB ìš°ì„ ) - ê¸¸ì´: {len(text_str)}ì")

        excluded_words = set()

        # í†µê³„ ì¹´ìš´í„°
        stats = {
            "user_db_idioms": 0,
            "grammar_patterns": 0,
            "reference_idioms": 0,
            "user_db_words": 0,  # ğŸ”¥ ì‚¬ìš©ì DB ë‹¨ì–´
            "gpt_words": 0,  # ğŸ”¥ GPT ì„ íƒ ë‹¨ì–´
            "advanced_phrasal_verbs": 0,
            "simple_patterns": 0,
        }

        # 1. ìˆ™ì–´ ì¶”ì¶œ
        print(f"ğŸ” ì§€ë¬¸ {text_id}ì—ì„œ ìˆ™ì–´ ì¶”ì¶œ ì¤‘...")
        idioms = self.extract_advanced_idioms(text)

        for idiom in idioms:
            for word in idiom["base_form"].lower().split():
                if word not in ["~", "n", "v", "v-ing"]:
                    excluded_words.add(word)

            # í†µê³„ ì—…ë°ì´íŠ¸
            if idiom["type"] == "user_db_idiom":
                stats["user_db_idioms"] += 1
            elif idiom["type"] == "grammar_pattern":
                stats["grammar_patterns"] += 1
            elif idiom["type"] == "reference_idiom_db":
                stats["reference_idioms"] += 1
            elif "advanced_phrasal" in idiom["type"]:
                stats["advanced_phrasal_verbs"] += 1
            elif idiom["type"] == "simple_pattern":
                stats["simple_patterns"] += 1

            rows.append(
                {
                    "ì§€ë¬¸ID": text_id,
                    "ì§€ë¬¸": text_str,
                    "ë‹¨ì–´": idiom["original"],
                    "ì›í˜•": idiom["base_form"],
                    "í’ˆì‚¬": idiom["type"],
                    "ëœ»(í•œê¸€)": idiom["meaning"],
                    "ëœ»(ì˜ì–´)": "",
                    "ë™ì˜ì–´": "",
                    "ë°˜ì˜ì–´": "",
                    "ë¬¸ë§¥": idiom["context"],
                    "ë¶„ë¦¬í˜•ì—¬ë¶€": idiom["is_separated"],
                    "ì‹ ë¢°ë„": f"{idiom['confidence']:.2f}",
                    "ì‚¬ìš©ìDBë§¤ì¹­": idiom.get("user_db_match", False),
                    "ë§¤ì¹­ë°©ì‹": idiom.get("match_type", ""),
                    "íŒ¨í„´ì •ë³´": idiom.get("pattern_info", {}),
                }
            )

        # 2. ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ (ì‚¬ìš©ì DB ìš°ì„ )
        if self.gpt_token_usage["total_tokens"] < self.MAX_TOKENS:
            print(f"ğŸ” ì§€ë¬¸ {text_id}ì—ì„œ ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ ì¤‘... (ì‚¬ìš©ì DB ìµœìš°ì„ )")
            try:
                difficult_words = self.extract_difficult_words(
                    text, easy_words, child_vocab, freq_tiers
                )

                for word in difficult_words:
                    if word.get("lemma", "").lower() in excluded_words:
                        continue

                    # ğŸ”¥ í†µê³„ ë¶„ë¥˜ ê°œì„ 
                    if word.get("user_db_match", False):
                        stats["user_db_words"] += 1
                    else:
                        stats["gpt_words"] += 1

                    rows.append(
                        {
                            "ì§€ë¬¸ID": text_id,
                            "ì§€ë¬¸": text_str,
                            "ë‹¨ì–´": word.get("original", ""),
                            "ì›í˜•": word.get("lemma", ""),
                            "í’ˆì‚¬": word.get("pos", ""),
                            "ëœ»(í•œê¸€)": word.get("korean_meaning", ""),
                            "ëœ»(ì˜ì–´)": word.get("english_definition", ""),
                            "ë™ì˜ì–´": word.get("synonyms", ""),
                            "ë°˜ì˜ì–´": word.get("antonyms", ""),
                            "ë¬¸ë§¥": word.get("context", ""),
                            "ë¶„ë¦¬í˜•ì—¬ë¶€": word.get("is_separated", False),
                            "ì‹ ë¢°ë„": f"{word.get('confidence', 0):.2f}",
                            "ì‚¬ìš©ìDBë§¤ì¹­": word.get("user_db_match", False),
                            "ë§¤ì¹­ë°©ì‹": word.get("match_type", ""),
                            "í¬í•¨ì´ìœ ": word.get("inclusion_reason", ""),
                            "ë‚œì´ë„ì ìˆ˜": f"{word.get('difficulty_score', 0):.2f}",
                            "ë‚œì´ë„ë ˆë²¨": word.get("difficulty_level", "medium"),
                        }
                    )
            except Exception as e:
                if self.verbose:
                    print(f"âŒ ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ğŸ”¥ ê°œì„ ëœ í†µê³„ ì¶œë ¥
        print(f"âœ… ì§€ë¬¸ {text_id} ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   ğŸ“Š ì‚¬ìš©ì DB ìˆ™ì–´: {stats['user_db_idioms']}ê°œ")
        print(f"   ğŸ“Š ë¬¸ë²• íŒ¨í„´: {stats['grammar_patterns']}ê°œ")
        print(f"   ğŸ“Š ì°¸ì¡° DB ìˆ™ì–´: {stats['reference_idioms']}ê°œ")
        print(f"   ğŸ“Š ì‚¬ìš©ì DB ë‹¨ì–´: {stats['user_db_words']}ê°œ âœ…")  # ğŸ”¥ ê°•ì¡°
        print(f"   ğŸ“Š GPT ì„ íƒ ë‹¨ì–´: {stats['gpt_words']}ê°œ")
        print(f"   ğŸ“Š ê³ ê¸‰ êµ¬ë™ì‚¬: {stats['advanced_phrasal_verbs']}ê°œ")

        # ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€
        if rows:
            try:
                rows = self.add_synonyms_antonyms_to_results(rows)
                print(f"âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")

        return rows

    # ğŸ”¥ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±
    def generate_vocabulary_workbook(
        self, texts, output_file="vocabulary_advanced.xlsx", **kwargs
    ):
        """ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±"""
        start_time = time.time()

        print(f"ğŸš€ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ì‹œì‘")
        print(f"   â€¢ ì‚¬ìš©ì DB ìˆ™ì–´: âœ… {len(self.user_idioms)}ê°œ í™œìš©")
        print(f"   â€¢ ì‚¬ìš©ì DB ë‹¨ì–´: âœ… {len(self.user_single_words)}ê°œ í™œìš©")
        print(f"   â€¢ ë¬¸ë²• íŒ¨í„´ ë¶„ì„: âœ… V-ing, N V-ing ë“±")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
        print(f"   â€¢ ê³ ê¸‰ êµ¬ë™ì‚¬ ë¶„ì„: âœ… ì—°ì†í˜•/ë¶„ë¦¬í˜• ìë™ êµ¬ë¶„")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
        print(f"   â€¢ ì°¸ì¡° ìˆ™ì–´ DB: âœ… í™œìš©")

        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        results = []
        for idx, text in enumerate(
            tqdm(texts, desc="ğŸ“ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ (ê³ ê¸‰ íŒ¨í„´ ë¶„ì„)", unit="ì§€ë¬¸")
        ):
            try:
                result = self.process_text(
                    text, idx + 1, self.easy_words, set(), self.freq_tiers
                )
                results.extend(result)

                if (idx + 1) % 10 == 0:
                    tqdm.write(
                        f"âœ… {idx + 1}/{len(texts)} ì™„ë£Œ ({len(results)}ê°œ í•­ëª© ì¶”ì¶œ)"
                    )
                    tqdm.write(
                        f"   ğŸ“Š GPT í˜¸ì¶œ: {self.gpt_call_count}/{self.GPT_CALL_LIMIT}"
                    )
                    tqdm.write(
                        f"   ğŸ“Š í† í° ì‚¬ìš©: {self.gpt_token_usage['total_tokens']}/{self.MAX_TOKENS}"
                    )

            except Exception as e:
                tqdm.write(f"âŒ í…ìŠ¤íŠ¸ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        if not results:
            print("âš ï¸ ì¶”ì¶œëœ ë‹¨ì–´/ìˆ™ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        df = pd.DataFrame(results)

        # í’ˆì§ˆ ê²€ì‚¬
        quality_results = None
        if kwargs.get("enable_quality_check", True):
            df, quality_results = self.run_quality_check_and_fix(df, output_file)

        # ì‚¬ìš©ì DB ë§¤ì¹­ ë¶„ì„
        if len(df) > 0:
            self._analyze_user_db_matching(df)

        # Excel ì €ì¥
        try:
            df.to_excel(output_file, index=False)
            print(f"âœ… ë‹¨ì–´ì¥ ì €ì¥ ì™„ë£Œ: {output_file}")
        except Exception as e:
            print(f"âŒ Excel ì €ì¥ ì‹¤íŒ¨: {e}")

        # ìºì‹œ ì €ì¥
        self.save_cache_to_file()

        # ê²°ê³¼ ìš”ì•½
        processing_time = time.time() - start_time
        print(f"\nğŸ¯ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ê²°ê³¼:")
        print(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        print(f"   ğŸ“Š ì´ í•­ëª© ìˆ˜: {len(df)}ê°œ")
        print(f"   ğŸ“Š GPT í˜¸ì¶œ íšŸìˆ˜: {self.gpt_call_count}íšŒ")
        print(f"   ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: {self.gpt_token_usage['total_tokens']}ê°œ")

        if self.settings["USE_INTEGRATED_CONTEXTUAL"]:
            print(f"   âœ¨ í†µí•© ì»¨í…ìŠ¤íŠ¸ ì˜ë¯¸ ìƒì„±ìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ")
        if self.settings["USE_INTEGRATED_DIFFICULTY"]:
            print(f"   âœ¨ í†µí•© ë‚œì´ë„ ë¶„ì„ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ")

        print(f"   ğŸ”¥ ì‚¬ìš©ì DB ìš°ì„  + ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ì •ë°€í•œ ë‹¨ì–´ì¥ ìƒì„±")

        # ğŸ”¥ íŒ¨í„´ë³„ í†µê³„ ì¶œë ¥
        if "ë§¤ì¹­ë°©ì‹" in df.columns:
            pattern_stats = df["ë§¤ì¹­ë°©ì‹"].value_counts()
            print(f"\nğŸ“Š íŒ¨í„´ë³„ ì¶”ì¶œ í†µê³„:")
            for pattern, count in pattern_stats.items():
                if pattern:
                    print(f"   â€¢ {pattern}: {count}ê°œ")

        return df

    def _analyze_user_db_matching(self, df):
        """ì‚¬ìš©ì DB ë§¤ì¹­ ë¶„ì„"""

        try:
            if "ì‚¬ìš©ìDBë§¤ì¹­" in df.columns:
                user_matched = df["ì‚¬ìš©ìDBë§¤ì¹­"].sum()
                total_items = len(df)
                match_ratio = (
                    (user_matched / total_items * 100) if total_items > 0 else 0
                )

                print(f"   ğŸ‘¤ ì‚¬ìš©ì DB ë§¤ì¹­ í•­ëª©: {user_matched}ê°œ")
                print(f"   ğŸ“Š ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨: {match_ratio:.1f}%")

                # ë§¤ì¹­ ë°©ì‹ë³„ ë¶„í¬
                if "ë§¤ì¹­ë°©ì‹" in df.columns and user_matched > 0:
                    user_df = df[df["ì‚¬ìš©ìDBë§¤ì¹­"] == True]
                    match_types = user_df["ë§¤ì¹­ë°©ì‹"].value_counts()
                    print(f"   ğŸ” ë§¤ì¹­ ë°©ì‹ë³„ ë¶„í¬:")
                    for match_type, count in match_types.items():
                        if match_type:
                            print(f"      â€¢ {match_type}: {count}ê°œ")

                # í¬í•¨ ì´ìœ ë³„ ë¶„í¬
                if "í¬í•¨ì´ìœ " in df.columns:
                    inclusion_reasons = df["í¬í•¨ì´ìœ "].value_counts()
                    print(f"   ğŸ“‹ í¬í•¨ ì´ìœ ë³„ ë¶„í¬:")
                    for reason, count in inclusion_reasons.items():
                        if reason:
                            print(f"      â€¢ {reason}: {count}ê°œ")

        except Exception as e:
            print(f"   âŒ ì‚¬ìš©ì DB ë§¤ì¹­ ë¶„ì„ ì‹¤íŒ¨: {e}")

    def run_quality_check_and_fix(self, df, output_file):
        """í’ˆì§ˆ ê²€ì‚¬ ë° ìë™ ìˆ˜ì •"""
        print("\nğŸ” í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘...")

        try:
            temp_file = output_file.replace(".xlsx", "_temp.xlsx")
            df.to_excel(temp_file, index=False)

            checker = VocabularyQualityChecker(temp_file)
            results = checker.generate_quality_report()

            print("ğŸ”§ í’ˆì§ˆ ë¬¸ì œ ìë™ ìˆ˜ì • ì¤‘...")
            fixed_df = checker.update_vocabulary_with_fixes()

            if os.path.exists(temp_file):
                os.remove(temp_file)

            print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {results['quality_score']:.1f}/100")
            print(f"ğŸ“Š ë°œê²¬ëœ ë¬¸ì œ: {results['total_issues']}ê°œ")

            return fixed_df, results

        except Exception as e:
            print(f"âŒ í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return df, None

    def _gpt_analyze_contextual_difficulty(self, word, context, pos=""):
        """ì§€ë¬¸ ë¬¸ë§¥ì—ì„œì˜ íŠ¹ì • ì˜ë¯¸ ë‚œì´ë„ ë¶„ì„"""

        prompt = f"""
Analyze the difficulty of the word "{word}" specifically as used in this context for Korean high school students.

Context: "{context}"

Focus on:
1. What does "{word}" mean in THIS specific context?
2. Is THIS particular meaning/usage advanced for high school students?
3. Would students struggle with THIS specific usage (not the word in general)?

Examples of contextual analysis:
- "feel tired" â†’ basic usage (exclude)
- "feel the economic impact" â†’ more advanced usage (consider including)
- "work hard" â†’ basic usage (exclude)  
- "the work explores themes" â†’ academic usage (include)

Rate the CONTEXTUAL difficulty (1-10):
1-4: Basic usage that high school students know
5-6: Intermediate usage (borderline)
7-10: Advanced/academic usage that students might not know

JSON response:
{{
    "contextual_meaning": "specific meaning in this context",
    "contextual_difficulty": 1-10,
    "usage_type": "basic|intermediate|advanced|academic",
    "recommendation": "include|exclude",
    "reasoning": "why this specific usage is/isn't challenging"
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in contextual English analysis for Korean students.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=1000,
                temperature=0.1,
            )

            content = response.choices[0].message.content.strip()
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            return json.loads(content)
        except Exception as e:
            return {
                "contextual_difficulty": 5,
                "recommendation": "exclude",
                "reasoning": f"ë¶„ì„ ì‹¤íŒ¨: {e}",
            }

    def _determine_final_appropriateness(
        self, word, contextual_analysis, general_analysis
    ):
        """ìµœì¢… ì í•©ì„± íŒë³„"""

        if not contextual_analysis or not general_analysis:
            return False, "ë¶„ì„ì‹¤íŒ¨"

        # ë¬¸ë§¥ ë¶„ì„ ìš°ì„ 
        contextual_difficulty = contextual_analysis.get("contextual_difficulty", 5)
        general_difficulty = general_analysis.get("difficulty_score", 5)

        # ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
        if contextual_difficulty >= 6 and general_difficulty >= 7:
            return (
                True,
                f"ê³ ë‚œì´ë„í™•ì¸(ë¬¸ë§¥{contextual_difficulty},ì¼ë°˜{general_difficulty})",
            )
        elif contextual_difficulty >= 6 and general_difficulty >= 8:
            return (
                True,
                f"ì¼ë°˜ê³ ë‚œì´ë„(ë¬¸ë§¥{contextual_difficulty},ì¼ë°˜{general_difficulty})",
            )
        else:
            return (
                False,
                f"ë‚œì´ë„ë¶€ì¡±(ë¬¸ë§¥{contextual_difficulty},ì¼ë°˜{general_difficulty})",
            )


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° v4.0")
    parser.add_argument("--input", "-i", default="ì§€ë¬¸DB.csv", help="ì…ë ¥ CSV íŒŒì¼")
    parser.add_argument(
        "--output", "-o", default="vocabulary_advanced.xlsx", help="ì¶œë ¥ ì—‘ì…€ íŒŒì¼"
    )
    parser.add_argument(
        "--threshold", "-t", type=float, default=2.5, help="ë‚œì´ë„ ì„ê³„ê°’"
    )
    parser.add_argument("--cache", "-c", default="gpt_cache.json", help="GPT ìºì‹œ íŒŒì¼")
    parser.add_argument(
        "--max-tokens", "-mt", type=int, default=200000, help="ìµœëŒ€ í† í° ì‚¬ìš©ëŸ‰"
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥")
    parser.add_argument("--user-words", help="ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ (CSV/XLSX)")
    parser.add_argument("--data-dir", default="data", help="ìˆ™ì–´ ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument(
        "--no-quality-check", action="store_true", help="í’ˆì§ˆ ê²€ì‚¬ ê±´ë„ˆë›°ê¸°"
    )

    args = parser.parse_args()

    settings = {
        "DIFFICULTY_THRESHOLD": args.threshold,
        "CACHE_FILE": args.cache,
        "MAX_TOKENS": args.max_tokens,
        "USE_CACHE": True,
        "USE_INTEGRATED_CONTEXTUAL": True,
        "USE_INTEGRATED_DIFFICULTY": True,
        "data_dir": args.data_dir,
    }

    print(f"ğŸš€ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° v4.0")
    print(f"   â€¢ ì‚¬ìš©ì DB ìˆ™ì–´ ìš°ì„  ì¸ì‹: âœ…")
    print(f"   â€¢ ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  í¬í•¨: âœ…")
    print(f"   â€¢ ë¬¸ë²• íŒ¨í„´ ë¶„ì„: âœ… (V-ing)")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
    print(f"   â€¢ ê³ ê¸‰ êµ¬ë™ì‚¬ ë¶„ì„: âœ… (ì—°ì†í˜•/ë¶„ë¦¬í˜• ìë™ êµ¬ë¶„)")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
    print(f"   â€¢ ë¶„ë¦¬í˜• í‘œì‹œ ê°œì„ : âœ… (pick ~ up, spend time V-ing)")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
    print(f"   â€¢ í†µí•© ì»¨í…ìŠ¤íŠ¸ ì˜ë¯¸: âœ…")
    print(f"   â€¢ í†µí•© ë‚œì´ë„ ë¶„ì„: âœ…")

    try:
        extractor = AdvancedVocabExtractor(
            user_words_file=args.user_words if args.user_words else "ë‹¨ì–´DB.csv",
            settings=settings,
            csv_file=args.input,
            verbose=args.verbose,
        )

        # ì…ë ¥ íŒŒì¼ ë¡œë“œ
        if args.input.endswith(".xlsx"):
            text_df = pd.read_excel(args.input)
        else:
            try:
                text_df = pd.read_csv(args.input, encoding="utf-8")
            except UnicodeDecodeError:
                text_df = pd.read_csv(args.input, encoding="cp949")

        text_column = "content" if "content" in text_df.columns else text_df.columns[0]
        print(f"âœ… '{args.input}' íŒŒì¼ì—ì„œ '{text_column}' ì—´ì„ ì‚¬ìš©")

        texts = text_df[text_column].dropna().astype(str).tolist()

        print(f"ğŸ“š ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸ì—ì„œ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ì‹œì‘")

        df = extractor.generate_vocabulary_workbook(
            texts,
            output_file=args.output,
            enable_quality_check=not args.no_quality_check,
        )

        # ê²°ê³¼ ì¶œë ¥
        if df is not None and len(df) > 0:
            print(f"\nğŸ‰ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ì™„ë£Œ!")
            print(f"   ğŸ“ íŒŒì¼: {args.output}")
            print(f"   ğŸ“Š ì´ í•­ëª©: {len(df)}ê°œ")
            print(
                f"   âœ¨ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ì ìš© (ë¶„ë¦¬í˜•: pick ~ up, ë¬¸ë²•íŒ¨í„´: spend time V-ing)"
            )

            # ì‚¬ìš©ì DB ë§¤ì¹­ í†µê³„ ì¶œë ¥
            if "ì‚¬ìš©ìDBë§¤ì¹­" in df.columns:
                user_matched = df["ì‚¬ìš©ìDBë§¤ì¹­"].sum()
                total_items = len(df)
                match_ratio = (
                    (user_matched / total_items * 100) if total_items > 0 else 0
                )
                print(f"\nğŸ‘¤ ì‚¬ìš©ì DB ë§¤ì¹­ ê²°ê³¼:")
                print(f"   â€¢ ë§¤ì¹­ëœ í•­ëª©: {user_matched}ê°œ")
                print(f"   â€¢ ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨: {match_ratio:.1f}%")

            # ğŸ”¥ íŒ¨í„´ë³„ í†µê³„ ì¶œë ¥
            if "ë§¤ì¹­ë°©ì‹" in df.columns:
                pattern_stats = df["ë§¤ì¹­ë°©ì‹"].value_counts()
                print(f"\nğŸ“Š íŒ¨í„´ë³„ ì¶”ì¶œ í†µê³„:")
                for pattern, count in pattern_stats.items():
                    if pattern:
                        print(f"   â€¢ {pattern}: {count}ê°œ")

        else:
            print("âš ï¸ ë‹¨ì–´ì¥ ìƒì„±ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ì¶”ì¶œëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    except FileNotFoundError:
        print(f"âŒ '{args.input}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback

            traceback.print_exc()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "help":
        print(
            """
    ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° v4.0 ì‚¬ìš©ë²•:

    ê¸°ë³¸ ì‚¬ìš©ë²•:
    python improved_vocab_extractor.py --input ì§€ë¬¸DB.csv --output vocabulary.xlsx

    ğŸ”¥ ì£¼ìš” ê°œì„ ì‚¬í•­ (v4.0):
    âœ… ê³ ê¸‰ êµ¬ë™ì‚¬ íŒ¨í„´ ë¶„ì„
    â€¢ ì—°ì†í˜• ê°€ëŠ¥: pick up (ê·¸ëŒ€ë¡œ í‘œì‹œ)
    â€¢ ë¶„ë¦¬ í•„ìˆ˜: pick ~ up (~ í‘œì‹œ)
    â€¢ ì‹¤ì œ ë¶„ë¦¬: pick something up â†’ pick ~ up

    âœ… ë¬¸ë²• íŒ¨í„´ ìë™ ì¸ì‹
    â€¢ V-ing íŒ¨í„´: spend time reading â†’ spend time V-ing
    â€¢ N V-ing íŒ¨í„´: prevent him from going â†’ prevent N from V-ing

    âœ… ì‚¬ìš©ì DB ìš°ì„  ì²˜ë¦¬
    â€¢ ì‚¬ìš©ì DB ìˆ™ì–´ ìµœìš°ì„  ì¸ì‹
    â€¢ ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  í¬í•¨
    â€¢ ë§¤ì¹­ ë°©ì‹ë³„ ìƒì„¸ í†µê³„

    âœ… íŒ¨í„´ë³„ ì •ë°€ ë¶„ì„
    â€¢ ìœ„ì¹˜ ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€
    â€¢ ë¬¸ë²•ì  ê²€ì¦ ê°•í™”
    â€¢ ì‹ ë¢°ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„

    íŒ¨í„´ í‘œì‹œ ì˜ˆì‹œ:
    - pick up â†’ pick up (ì—°ì†í˜• ê°€ëŠ¥)
    - pick something up â†’ pick ~ up (ë¶„ë¦¬ í•„ìˆ˜)
    - spend time reading â†’ spend time V-ing (ë¬¸ë²• íŒ¨í„´)

    ì£¼ìš” ì˜µì…˜:
    --input: ì…ë ¥ CSV íŒŒì¼ (ê¸°ë³¸: ì§€ë¬¸DB.csv)
    --output: ì¶œë ¥ Excel íŒŒì¼ (ê¸°ë³¸: vocabulary_advanced.xlsx)
    --user-words: ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ (ê¸°ë³¸: ë‹¨ì–´DB.csv)
    --data-dir: ì°¸ì¡° ìˆ™ì–´ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data)
    --verbose: ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
    --no-quality-check: í’ˆì§ˆ ê²€ì‚¬ ê±´ë„ˆë›°ê¸°

    ì‚¬ìš© ì˜ˆì‹œ:
    python improved_vocab_extractor.py --input ì§€ë¬¸DB.csv --output my_vocab.xlsx --verbose
        """
        )
        sys.exit(0)

    try:
        print("ğŸš€ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„±ê¸° ì‹œì‘...")
        print("ğŸ”¥ v4.0 - ë¶„ë¦¬í˜•/ë¬¸ë²•íŒ¨í„´ ê³ ê¸‰ ë¶„ì„ ë²„ì „")
        main()
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
