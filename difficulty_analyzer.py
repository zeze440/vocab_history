# difficulty_analyzer.py - ë‚œì´ë„ ë¶„ì„ ëª¨ë“ˆ

import json
from nltk.corpus import wordnet
from package_manager import get_nlp_model
from config import BASIC_VERBS, BASIC_ADJECTIVES, BASIC_NOUNS
from external_vocab_db import is_basic_by_external_db


def enhanced_is_too_easy_for_highschool(word, pos, easy_words, child_vocab, freq_tiers):
    """ê³ ë“±í•™ìƒ ìˆ˜ì¤€ì—ì„œ ë„ˆë¬´ ì‰¬ìš´ ë‹¨ì–´ì¸ì§€ íŒë³„ - GPT í•„í„°ì™€ í•¨ê»˜ ì‚¬ìš©"""
    word_lower = word.lower()

    # 1. ê¸°ë³¸ ë‹¨ì–´ ê°•ë ¥ í•„í„°ë§ ì¶”ê°€ (ë§¨ ì•ì—)
    if (
        word_lower in BASIC_VERBS
        or word_lower in BASIC_ADJECTIVES
        or word_lower in BASIC_NOUNS
    ):
        return True

    # 2. ì™¸ë¶€ DB ì²´í¬ ì¶”ê°€
    if is_basic_by_external_db(word_lower):
        return True

    # 3. ì•„ë™ ì–´íœ˜ ì²´í¬
    if child_vocab and word_lower in child_vocab:
        return True

    # 4. ë§¤ìš° ì§§ì€ ë‹¨ì–´
    if len(word_lower) <= 2:
        return True

    # 5. ê¸°ë³¸ ë¬¸ë²• ìš”ì†Œ
    if pos in {"DET", "ADP", "CONJ", "PRON", "AUX", "INTJ"}:
        return True

    return False


def integrated_extract_info(word, pos=None):
    """vocab_difficulty.pyì˜ extract_info í†µí•© ë²„ì „"""
    definition = ""
    synonyms = []
    antonyms = []

    # í’ˆì‚¬ íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì¶”ë¡ 
    if not pos:
        try:
            from nltk import pos_tag

            tagged = pos_tag([word])
            pos = integrated_get_wordnet_pos(tagged[0][1]) if tagged else None
        except:
            pos = None

    # WordNetì—ì„œ ì •ë³´ ì¶”ì¶œ
    synsets = wordnet.synsets(word, pos=pos) if pos else wordnet.synsets(word)

    if synsets:
        # ì²« ë²ˆì§¸ ë™ì˜ì–´ ì§‘í•©ì˜ ì •ì˜ ì‚¬ìš©
        definition = synsets[0].definition()

        # ë™ì˜ì–´ ìˆ˜ì§‘
        for synset in synsets:
            for lemma in synset.lemmas():
                if lemma.name().lower() != word.lower():
                    synonyms.append(lemma.name().replace("_", " "))

                # ë°˜ì˜ì–´ ìˆ˜ì§‘
                for antonym in lemma.antonyms():
                    antonyms.append(antonym.name().replace("_", " "))

    # ì¤‘ë³µ ì œê±°
    synonyms = list(set(synonyms))[:5]
    antonyms = list(set(antonyms))[:5]

    return definition, synonyms, antonyms


def integrated_get_wordnet_pos(tag):
    """vocab_difficulty.pyì˜ get_wordnet_pos í†µí•© ë²„ì „"""
    if tag.startswith("J"):
        return wordnet.ADJ
    elif tag.startswith("V"):
        return wordnet.VERB
    elif tag.startswith("N"):
        return wordnet.NOUN
    elif tag.startswith("R"):
        return wordnet.ADV
    else:
        return None


def integrated_calculate_phonetic_complexity(word):
    """vocab_difficulty.pyì˜ calculate_phonetic_complexity í†µí•© ë²„ì „"""
    vowels = set("aeiou")
    consonants = set("bcdfghjklmnpqrstvwxyz")
    complexity = 0
    consonant_clusters = 0
    prev_consonant = False

    for char in word.lower():
        if char in consonants:
            if prev_consonant:
                consonant_clusters += 1
            prev_consonant = True
        else:
            prev_consonant = False

    complexity += consonant_clusters * 0.5

    rare_combinations = ["ph", "th", "ch", "sh", "gh", "ck", "ng", "qu"]
    for combo in rare_combinations:
        if combo in word.lower():
            complexity += 0.3

    return complexity


def integrated_get_word_difficulty_score(word, nlp_model=None):
    """vocab_difficulty.pyì˜ get_word_difficulty_score í†µí•© ë²„ì „"""
    try:
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        morphological_complexity = 0

        if nlp_model:
            doc = nlp_model(word)
            for token in doc:
                if token.lemma_ != token.text:
                    morphological_complexity += 1
                if token.pos_ in ["NOUN", "VERB"]:
                    morphological_complexity += 0.5
                elif token.pos_ in ["ADJ", "ADV"]:
                    morphological_complexity += 1

        # WordNet ê¸°ë°˜ ì˜ë¯¸ ë³µì¡ë„
        synsets = wordnet.synsets(word)
        semantic_complexity = len(synsets) * 0.1

        # ìŒì„±ì  ë³µì¡ë„
        phonetic_complexity = integrated_calculate_phonetic_complexity(word)

        # ì´ ì ìˆ˜ ê³„ì‚°
        total_score = (
            morphological_complexity * 0.4
            + semantic_complexity * 0.3
            + phonetic_complexity * 0.3
        )

        return total_score
    except:
        return 0.5  # ê¸°ë³¸ê°’


def integrated_is_difficult_word(
    word,
    easy_words,
    children_vocab,
    frequency_tiers=None,
    nlp_model=None,
    threshold=2.8,
):
    """vocab_difficulty.pyì˜ is_difficult_word í†µí•© ë²„ì „"""
    min_length = 4

    if not word or len(word) < min_length:
        return False

    if not word.isalpha():
        return False

    # ì‰¬ìš´ ë‹¨ì–´ ì²´í¬
    if word.lower() in easy_words:
        return False

    if children_vocab and word.lower() in children_vocab:
        return False

    # WordNet ê¸°ë°˜ ê¸°ë³¸ íŒë³„
    synsets = wordnet.synsets(word)
    if not synsets:
        return True  # ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ëŠ” ì–´ë ¤ìš´ ê²ƒìœ¼ë¡œ ê°„ì£¼

    # ë‚œì´ë„ ì ìˆ˜ ê¸°ë°˜ íŒë³„
    difficulty_score = integrated_get_word_difficulty_score(word, nlp_model)
    return difficulty_score > 0.6


class GPTDifficultyFilter:
    """GPT ê¸°ë°˜ ì‚¬ìš©ì DB ìˆ˜ì¤€ ë‚œì´ë„ í•„í„°"""

    def __init__(
        self, client, user_words=None, cache_file="difficulty_filter_cache.json"
    ):
        self.client = client
        self.user_words = user_words or set()
        self.cache_file = cache_file
        self.difficulty_cache = {}
        self.gpt_calls = 0
        self._load_cache()

        # ì‚¬ìš©ì DB ë‹¨ì–´ë“¤ì˜ í‰ê·  ë‚œì´ë„ ë¶„ì„ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        self.user_db_baseline = None
        if self.user_words:
            self._analyze_user_db_baseline()

    def _load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        from utils import load_json_safe

        self.difficulty_cache = load_json_safe(self.cache_file, {})
        if self.difficulty_cache:
            print(f"âœ… ë‚œì´ë„ í•„í„° ìºì‹œ ë¡œë“œ: {len(self.difficulty_cache)}ê°œ")

    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        from utils import save_json_safe

        save_json_safe(self.difficulty_cache, self.cache_file)

    def _analyze_user_db_baseline(self):
        """ì‚¬ìš©ì DB ë‹¨ì–´ë“¤ì˜ í‰ê·  ë‚œì´ë„ ë¶„ì„í•˜ì—¬ ê¸°ì¤€ì  ì„¤ì •"""
        if not self.user_words or len(self.user_words) == 0:
            return

        print("ğŸ” ì‚¬ìš©ì DB ë‹¨ì–´ ë‚œì´ë„ ê¸°ì¤€ì  ë¶„ì„ ì¤‘...")

        # ì‚¬ìš©ì DBì—ì„œ ë‹¨ì¼ ë‹¨ì–´ë§Œ ì„ íƒí•˜ì—¬ ìƒ˜í”Œë§ (ìˆ™ì–´ ì œì™¸)
        single_words = [
            word for word in self.user_words if " " not in word and "-" not in word
        ]
        sample_words = single_words[:15] if len(single_words) > 15 else single_words

        if len(sample_words) == 0:
            self.user_db_baseline = {
                "average_score": 5.0,
                "min_threshold": 3.5,
                "sample_count": 0,
            }
            print("   âš ï¸ ë¶„ì„í•  ë‹¨ì¼ ë‹¨ì–´ê°€ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return

        difficulty_scores = []
        for word in sample_words:
            difficulty = self._gpt_analyze_word_difficulty(word, for_baseline=True)
            if difficulty and "difficulty_score" in difficulty:
                difficulty_scores.append(difficulty["difficulty_score"])

        if difficulty_scores:
            avg_score = sum(difficulty_scores) / len(difficulty_scores)
            # í‰ê· ë³´ë‹¤ 1.5ì  ë‚®ì€ ìˆ˜ì¤€ê¹Œì§€ë§Œ í—ˆìš©, ìµœì†Œ 3.0ì ì€ ë³´ì¥
            min_threshold = max(3.0, avg_score - 1.5)

            self.user_db_baseline = {
                "average_score": avg_score,
                "min_threshold": min_threshold,
                "sample_count": len(difficulty_scores),
            }
            print(
                f"   âœ… ì‚¬ìš©ì DB ê¸°ì¤€ì : í‰ê·  {avg_score:.1f}ì , ìµœì†Œ ì„ê³„ê°’ {min_threshold:.1f}ì "
            )
        else:
            self.user_db_baseline = {
                "average_score": 6.0,
                "min_threshold": 5.5,
                "sample_count": 0,
            }
            print("   âš ï¸ ì‚¬ìš©ì DB ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")

    def is_word_appropriate_for_user_db(self, word, context="", pos=""):
        """ë‹¨ì–´ê°€ ì‚¬ìš©ì DB ìˆ˜ì¤€ì— ì í•©í•œì§€ GPTë¡œ íŒë³„"""
        word_lower = word.lower().strip()

        # ì‚¬ìš©ì DBì— ìˆëŠ” ë‹¨ì–´ëŠ” ë¬´ì¡°ê±´ ì í•© (ìµœìš°ì„ )
        if word_lower in self.user_words:
            return True, "ì‚¬ìš©ìDBí¬í•¨"

        # ìˆ™ì–´ëŠ” ë³„ë„ ì²˜ë¦¬
        if " " in word or "-" in word or "~" in word:
            return True, "ìˆ™ì–´íŒ¨í„´"

        # ìºì‹œ í™•ì¸
        cache_key = f"{word_lower}:{context[:30]}"
        if cache_key in self.difficulty_cache:
            cached_result = self.difficulty_cache[cache_key]
            return cached_result["appropriate"], cached_result["reason"]

        # GPT ë¶„ì„ (ì‚¬ìš©ì DBê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
        difficulty_analysis = self._gpt_analyze_word_difficulty(word, context, pos)

        # ì í•©ì„± íŒë³„
        appropriate, reason = self._determine_appropriateness(word, difficulty_analysis)

        # ìºì‹œ ì €ì¥
        self.difficulty_cache[cache_key] = {
            "appropriate": appropriate,
            "reason": reason,
            "analysis": difficulty_analysis,
        }

        return appropriate, reason

    def _gpt_analyze_word_difficulty(
        self, word, context="", pos="", for_baseline=False
    ):
        """GPTë¡œ ë‹¨ì–´ ë‚œì´ë„ ë¶„ì„"""
        # ì‚¬ìš©ì DB ê¸°ì¤€ì  ì •ë³´
        baseline_info = ""
        if self.user_db_baseline and not for_baseline:
            baseline_info = f"""
ì°¸ê³ : í˜„ì¬ ì‚¬ìš©ì ë‹¨ì–´ DBì˜ í‰ê·  ë‚œì´ë„ëŠ” {self.user_db_baseline['average_score']:.1f}ì ì…ë‹ˆë‹¤.
ì´ ìˆ˜ì¤€ì— ë§ëŠ” ë‹¨ì–´ë“¤ì„ ì„ ë³„í•´ì£¼ì„¸ìš”.
"""

        prompt = f"""
Analyze the difficulty of the English word "{word}" for Korean high school students.

Context: "{context}"
Part of Speech: {pos}

CRITICAL EXCLUSION CRITERIA (Rate as 1-2 points):
- Basic daily verbs: feel, see, hear, go, come, make, take, get, have, etc.
- Basic adjectives: good, bad, big, small, happy, sad, easy, hard, etc.  
- Basic nouns: man, woman, house, school, work, time, etc.
- Words learned in elementary/middle school
- Common conversation vocabulary
- Proper nouns: names of people, places, countries, organizations, brands
- Capitalized words that are names or locations
- Any word that is primarily a proper noun (even if used as common noun)

IMPORTANT NOTES:
- Educational value is NOT a factor for inclusion
- Emotional expression importance is NOT a factor  
- Focus ONLY on vocabulary difficulty level
- Rate based on the SPECIFIC meaning in the given context
- Korean high school students already know basic English vocabulary

Difficulty Scale (1-10):
1-3: Elementary/Middle school level (EXCLUDE these)
4-5: Basic high school level (borderline - usually EXCLUDE)  
6-7: Intermediate high school level (INCLUDE only if truly challenging)
8-10: Advanced/University prep level (INCLUDE)

For the word "{word}" in context "{context}":
- What is the specific meaning being used?
- Would a Korean high school student struggle with THIS specific usage?
- Is this an advanced/academic usage of the word?

{baseline_info}

Respond in JSON format:
{{
    "difficulty_score": 1-10,
    "specific_meaning": "meaning in this context",
    "level_category": "elementary|middle_school|basic_high_school|advanced_high_school|university",
    "is_basic_vocabulary": true/false,
    "recommendation": "include|exclude",
    "reasoning": "focus on difficulty of the specific contextual meaning"
}}
"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in English education for Korean students. You understand the appropriate vocabulary levels for different stages of English learning.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=400,
                temperature=0.1,
            )

            self.gpt_calls += 1
            content = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            result = json.loads(content)
            return result

        except Exception as e:
            print(f"âŒ GPT ë‚œì´ë„ ë¶„ì„ ì‹¤íŒ¨ ({word}): {e}")
            return None

    def _determine_appropriateness(self, word, analysis):
        """ìˆ˜ì •ëœ ì í•©ì„± íŒë³„ - ë” ì—„ê²©í•œ ê¸°ì¤€"""
        if not analysis:
            return False, "ë¶„ì„ì‹¤íŒ¨"

        # ê¸°ë³¸ ë‹¨ì–´ ê°•ë ¥ ì°¨ë‹¨
        if analysis.get("is_basic_vocabulary", False):
            return False, "ê¸°ë³¸ì–´íœ˜ì œì™¸"

        difficulty_score = analysis.get("difficulty_score", 5)

        # ëŒ€í­ ìƒí–¥ëœ ì„ê³„ê°’ (ê¸°ì¡´ 4.0 â†’ 6.0)
        MIN_DIFFICULTY = 7.0

        if difficulty_score < MIN_DIFFICULTY:
            return False, f"ë‚œì´ë„ë¶€ì¡±({difficulty_score}<{MIN_DIFFICULTY})"

        # GPT ì¶”ì²œ í™•ì¸
        recommendation = analysis.get("recommendation", "exclude")
        if recommendation == "include":
            return True, f"ê³ ë‚œì´ë„í™•ì¸({difficulty_score}ì )"
        else:
            return False, f"GPTì œì™¸ì¶”ì²œ({analysis.get('reasoning', 'ì´ìœ ì—†ìŒ')})"

    def batch_filter_words(self, words_with_context, batch_size=10):
        """ì—¬ëŸ¬ ë‹¨ì–´ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        results = []
        appropriate_count = 0

        print(f"ğŸ” GPT ê¸°ë°˜ ë‹¨ì–´ ì í•©ì„± ê²€ì‚¬: {len(words_with_context)}ê°œ")

        for i in range(0, len(words_with_context), batch_size):
            batch = words_with_context[i : i + batch_size]

            for word_info in batch:
                word = word_info.get("word", "")
                context = word_info.get("context", "")
                pos = word_info.get("pos", "")

                appropriate, reason = self.is_word_appropriate_for_user_db(
                    word, context, pos
                )

                results.append(
                    {
                        "word": word,
                        "appropriate": appropriate,
                        "reason": reason,
                        "original_info": word_info,
                    }
                )

                if appropriate:
                    appropriate_count += 1

            # ë°°ì¹˜ë§ˆë‹¤ ìºì‹œ ì €ì¥
            if i % (batch_size * 5) == 0:
                self._save_cache()

        # ìµœì¢… ìºì‹œ ì €ì¥
        self._save_cache()

        print(f"âœ… í•„í„°ë§ ì™„ë£Œ: {appropriate_count}/{len(words_with_context)}ê°œ ì„ íƒ")
        print(f"ğŸ¤– GPT í˜¸ì¶œ: {self.gpt_calls}íšŒ")

        return results
