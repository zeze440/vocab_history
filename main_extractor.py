# main_extractor.py - ë©”ì¸ ì–´íœ˜ ì¶”ì¶œê¸°

import os
import time
import pandas as pd
import quality_checker
from tqdm import tqdm
from config import DEFAULT_SETTINGS, FilePaths
from utils import (
    force_extract_text,
    get_sentence_context,
    get_simple_pos,
    clean_korean_definition,
    safe_read_csv,
    find_text_column,
)
from package_manager import get_nlp_model, get_openai_client, initialize_packages
from cache_manager import get_cache_manager
from difficulty_analyzer import GPTDifficultyFilter
from extractor_methods import ExtractorMethods
from separable_idiom_detector import SeparableIdiomDetector, AdvancedIdiomChecker
from external_vocab_db import ExternalVocabDatabase

# í•„ìˆ˜ importë“¤
try:
    from safe_data_utils import (
        safe_get_column_value,
        safe_string_operation,
        safe_numeric_operation,
    )
except ImportError:
    print("âš ï¸ safe_data_utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    def safe_get_column_value(df, col, default=""):
        return df.get(col, default)

    def safe_string_operation(text, operation="strip"):
        return str(text).strip() if text else ""

    def safe_numeric_operation(value, operation="float"):
        try:
            return float(value) if value else 0.0
        except:
            return 0.0


try:
    from missing_methods import MissingMethodsMixin
except ImportError:
    print("âš ï¸ missing_methodsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    class MissingMethodsMixin:
        pass


class AdvancedVocabExtractor(ExtractorMethods, MissingMethodsMixin):
    """ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ì–´íœ˜ ì¶”ì¶œê¸°"""

    def __init__(
        self,
        user_words_file=None,
        settings=None,
        csv_file=None,
        verbose=False,
        **kwargs,
    ):
        # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
        initialize_packages()

        # ê¸°ë³¸ ì„¤ì •
        self.settings = DEFAULT_SETTINGS.copy()
        if settings:
            self.settings.update(settings)

        # ê¸°ë³¸ ë³€ìˆ˜ë“¤
        self.gpt_call_count = 0
        self.GPT_CALL_LIMIT = self.settings["GPT_CALL_LIMIT"]
        self.DIFFICULTY_THRESHOLD = self.settings["DIFFICULTY_THRESHOLD"]
        self.MIN_WORD_LENGTH = self.settings["MIN_WORD_LENGTH"]
        self.MAX_TOKENS = self.settings["MAX_TOKENS"]
        self.USE_CACHE = self.settings["USE_CACHE"]
        self.gpt_cache = {}
        self.gpt_token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }
        self.verbose = verbose

        # ë””ë ‰í† ë¦¬ ìƒì„±
        FilePaths.ensure_directories()

        # ì‚¬ìš©ì ë‹¨ì–´ DB ì´ˆê¸°í™”
        self.user_words = set()
        self.user_idioms = set()  # ìˆ™ì–´ë§Œ ë”°ë¡œ ê´€ë¦¬
        self.user_single_words = set()  # ë‹¨ì¼ ë‹¨ì–´ë§Œ ë”°ë¡œ ê´€ë¦¬

        # ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ë“¤
        self.nlp = get_nlp_model()
        self.client = get_openai_client()
        self.phrase_db = None

        # ìºì‹œ ê´€ë¦¬ì
        self.cache_manager = get_cache_manager()
        self.load_cache_from_file(self.settings["CACHE_FILE"])

        # ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡
        self.easy_words = self._load_easy_words()

        # ë¹ˆë„ ë°ì´í„°
        self.freq_tiers = {}
        if csv_file and os.path.exists(csv_file):
            print(f"ğŸ“Š '{csv_file}'ì—ì„œ ë¹ˆë„ ë°ì´í„° êµ¬ì¶• ì¤‘...")
            self.freq_tiers = self._build_frequency_from_csv(csv_file)

        # ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        self._initialize_synonym_extractor()

        # ğŸ”¥ ì§€ë¬¸ DB ë¡œë”© ì¶”ê°€ (data_loader í™œìš©)
        self.passage_db = None
        self.load_texts_data = None

        # ğŸ” ì´ˆê¸°í™” ê²°ê³¼ í™•ì¸ ì½”ë“œ ì¶”ê°€
        print(
            f"ğŸ” ìµœì¢… synonym_extractor ìƒíƒœ: {getattr(self, 'synonym_extractor', 'NOT_SET')}"
        )
        if hasattr(self, "synonym_extractor") and self.synonym_extractor:
            print("âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ê¸°ëŠ¥ í™œì„±í™”ë¨")
        else:
            print("âŒ ë™ì˜ì–´/ë°˜ì˜ì–´ ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨")

        # ê³ ê¸‰ ê²€ì¦ê¸° ì´ˆê¸°í™”
        self.idiom_checker = AdvancedIdiomChecker(self.nlp)

        # ì™¸ë¶€ DB ì´ˆê¸°í™”
        self.external_vocab_db = ExternalVocabDatabase()

        # ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ ë¡œë”©
        user_words_file = user_words_file or FilePaths.USER_WORDS_FILE
        if user_words_file and os.path.exists(user_words_file):
            print(f"ğŸ“– ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ ë¡œë”©: {user_words_file}")
            self._load_user_words_with_idiom_detection(user_words_file)
        else:
            print(f"ğŸ” ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_words_file}")

        if csv_file and os.path.exists(csv_file):
            self.passage_db = data_loader.load_texts_data(csv_file)
        # GPT ê¸°ë°˜ ë‚œì´ë„ í•„í„° ì´ˆê¸°í™”
        self._initialize_gpt_difficulty_filter()
        # ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ê¸° ì´ˆê¸°í™”
        self._initialize_separable_detection()

        # ì°¸ì¡° ìˆ™ì–´ DB ë¡œë”©
        self._load_reference_idioms()

        self._print_initialization_summary()

    def _initialize_synonym_extractor(self):
        try:
            from synonym_antonym_module import SynonymAntonymExtractor

            self.synonym_extractor = SynonymAntonymExtractor(
                client=self.client,
                cache_file="synonym_antonym_cache.json",
                verbose=self.verbose,
            )
            print("âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError as e:
            print(f"âš ï¸ synonym_antonym_module.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            self.synonym_extractor = None

        except ImportError as e:
            print(f"âš ï¸ synonym_antonym_module.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("   ë™ì˜ì–´/ë°˜ì˜ì–´ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
            self.synonym_extractor = None

 
        # ì´ˆê¸°í™” ê²°ê³¼ í™•ì¸
        if self.synonym_extractor:
            print("ğŸ“š ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ê¸°ëŠ¥ í™œì„±í™”ë¨")
        else:
            print("âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨ (ë‹¤ë¥¸ ê¸°ëŠ¥ì€ ì •ìƒ ì‘ë™)")

    def _initialize_gpt_difficulty_filter(self):
        """GPT ê¸°ë°˜ ë‚œì´ë„ í•„í„° ì´ˆê¸°í™”"""
        if not self.settings.get("USE_GPT_DIFFICULTY_FILTER", True):
            print("âš ï¸ GPT ë‚œì´ë„ í•„í„°ê°€ ì„¤ì •ì—ì„œ ë¹„í™œì„±í™”ë¨")
            return

        try:
            print("ğŸ¤– GPT ê¸°ë°˜ ë‚œì´ë„ í•„í„° ì´ˆê¸°í™” ì¤‘...")
            print(f"   ğŸ“Š ì‚¬ìš©ì ë‹¨ì¼ ë‹¨ì–´: {len(self.user_single_words)}ê°œ")

            self.gpt_filter = GPTDifficultyFilter(
                client=self.client,
                user_words=self.user_single_words,
                cache_file="gpt_difficulty_filter_cache.json",
            )

            print("âœ… GPT ë‚œì´ë„ í•„í„° ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ GPT ë‚œì´ë„ í•„í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            self.gpt_filter = None

    def _initialize_separable_detection(self):
        """ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print(f"ğŸ”§ ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")

        # SeparableIdiomDetector ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.separable_detector = SeparableIdiomDetector(
            self.client, verbose=self.verbose
        )

        # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì‹œë„
        cache_file = "separable_analysis.json"
        if not self.separable_detector.load_separable_analysis(cache_file):
            # ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ë¶„ì„
            if hasattr(self, "user_idioms") and self.user_idioms:
                print(f"ğŸ¤– ì‚¬ìš©ì ìˆ™ì–´ ë¶„ë¦¬í˜• ë¶„ì„ ì‹œì‘...")
                separable_analysis = (
                    self.separable_detector.analyze_user_idioms_with_gpt(
                        self.user_idioms
                    )
                )
                self.separable_detector.build_separable_patterns(separable_analysis)

                # ë¶„ì„ ê²°ê³¼ ì €ì¥
                self.separable_detector.save_separable_analysis(cache_file)
            else:
                print("âš ï¸ ì‚¬ìš©ì ìˆ™ì–´ê°€ ì—†ì–´ ë¶„ë¦¬í˜• ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        # extractorì™€ ì—°ë™
        self.user_separable_idioms = self.separable_detector.user_separable_idioms

        print(
            f"âœ… ë¶„ë¦¬í˜• ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.user_separable_idioms)}ê°œ ë¶„ë¦¬í˜• ìˆ™ì–´"
        )

    def _load_user_words_with_idiom_detection(self, user_words_file):
        """ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ì—ì„œ ìˆ™ì–´ì™€ ë‹¨ì¼ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ì—¬ ë¡œë”©"""
        try:
            if user_words_file.endswith(".csv"):
                user_df = safe_read_csv(user_words_file)
                if user_df is None:
                    return
            elif user_words_file.endswith((".xlsx", ".xls")):
                user_df = pd.read_excel(user_words_file)
            else:
                print(f"   âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
                return

            if not user_df.empty and len(user_df.columns) > 0:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì˜ ë‹¨ì–´ë“¤ ì¶”ì¶œ
                user_words = (
                    user_df.iloc[:, 0].dropna().astype(str).str.strip().tolist()
                )

                # ìˆ™ì–´ì™€ ë‹¨ì¼ ë‹¨ì–´ ë¶„ë¦¬
                idiom_count = 0
                single_word_count = 0

                for word in user_words:
                    word_clean = word.lower().strip()
                    self.user_words.add(word_clean)

                    # ë„ì–´ì“°ê¸°ê°€ ìˆìœ¼ë©´ ìˆ™ì–´ë¡œ ë¶„ë¥˜
                    if " " in word_clean and len(word_clean.split()) >= 2:
                        self.user_idioms.add(word_clean)
                        idiom_count += 1
                    else:
                        self.user_single_words.add(word_clean)
                        single_word_count += 1

                print(f"   âœ… ì´ {len(user_words)}ê°œ ë‹¨ì–´ ë¡œë“œ ì™„ë£Œ")
                print(f"   ğŸ“‹ ìˆ™ì–´: {idiom_count}ê°œ")
                print(f"   ğŸ“‹ ë‹¨ì¼ ë‹¨ì–´: {single_word_count}ê°œ")

                # ì‚¬ìš©ì ìˆ™ì–´ ìƒ˜í”Œ ì¶œë ¥
                if self.user_idioms:
                    sample_idioms = list(self.user_idioms)[:5]
                    print(f"   ğŸ“ ì‚¬ìš©ì ìˆ™ì–´ ì˜ˆì‹œ: {sample_idioms}")
            else:
                print(f"   âš ï¸ íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŒ")

        except Exception as e:
            print(f"   âŒ ì‚¬ìš©ì ë‹¨ì–´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _load_reference_idioms(self):
        """ì°¸ì¡° ìˆ™ì–´ DB ë¡œë”©"""
        from data_loader import load_custom_idioms_from_data_directory

        data_dir = self.settings.get("data_dir", FilePaths.DATA_DIR)
        self.reference_idioms = load_custom_idioms_from_data_directory(data_dir)

    def _load_easy_words(self):
        """ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡ ë¡œë“œ"""
        import pickle
        from nltk.corpus import stopwords
        from config import BASIC_VERBS, BASIC_ADJECTIVES, BASIC_NOUNS, EASY_WORDS_FILES

        try:
            # 1. ë¨¼ì € pickle ìºì‹œ í™•ì¸
            easy_words_cache = self.settings["EASY_WORDS_CACHE"]
            if os.path.exists(easy_words_cache):
                with open(easy_words_cache, "rb") as f:
                    easy_words = pickle.load(f)
                print(f"âœ… ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡ {len(easy_words)}ê°œ ìºì‹œì—ì„œ ë¡œë“œ ì™„ë£Œ")
                return easy_words

            # 2. Excel íŒŒì¼ í™•ì¸
            for excel_file in EASY_WORDS_FILES["excel"]:
                if os.path.exists(excel_file):
                    print(f"ğŸ“Š Excel íŒŒì¼ì—ì„œ ì‰¬ìš´ ë‹¨ì–´ ë¡œë”©: {excel_file}")
                    try:
                        df = pd.read_excel(excel_file)
                        words_column = df.columns[0]
                        easy_words = set(
                            df[words_column]
                            .dropna()
                            .astype(str)
                            .str.strip()
                            .str.lower()
                        )

                        # pickle ìºì‹œë¡œ ì €ì¥
                        try:
                            with open(easy_words_cache, "wb") as f:
                                pickle.dump(easy_words, f)
                            print(f"âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ: {easy_words_cache}")
                        except Exception as e:
                            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

                        print(f"âœ… Excelì—ì„œ ì‰¬ìš´ ë‹¨ì–´ {len(easy_words)}ê°œ ë¡œë“œ ì™„ë£Œ")
                        return easy_words

                    except Exception as e:
                        print(f"âš ï¸ {excel_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue

            # 3. CSV íŒŒì¼ë„ í™•ì¸
            for csv_file in EASY_WORDS_FILES["csv"]:
                if os.path.exists(csv_file):
                    print(f"ğŸ“Š CSV íŒŒì¼ì—ì„œ ì‰¬ìš´ ë‹¨ì–´ ë¡œë”©: {csv_file}")
                    try:
                        df = pd.read_csv(csv_file, encoding="utf-8")
                        words_column = df.columns[0]
                        easy_words = set(
                            df[words_column]
                            .dropna()
                            .astype(str)
                            .str.strip()
                            .str.lower()
                        )

                        print(f"âœ… CSVì—ì„œ ì‰¬ìš´ ë‹¨ì–´ {len(easy_words)}ê°œ ë¡œë“œ ì™„ë£Œ")
                        return easy_words

                    except Exception as e:
                        print(f"âš ï¸ {csv_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue

        except Exception as e:
            print(f"âš ï¸ ì‰¬ìš´ ë‹¨ì–´ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 4. ëª¨ë“  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹¨ì–´ë“¤ ì‚¬ìš©
        print("ğŸ“š ê¸°ë³¸ ë‹¨ì–´ ì„¸íŠ¸ ì‚¬ìš©")
        return set(stopwords.words("english")).union(
            BASIC_VERBS,
            BASIC_ADJECTIVES,
            BASIC_NOUNS,
            {
                "the",
                "a",
                "an",
                "this",
                "that",
                "these",
                "those",
                "good",
                "bad",
                "big",
                "small",
                "new",
                "old",
                "young",
                "make",
                "take",
                "get",
                "go",
                "come",
                "see",
                "know",
            },
        )

    def _build_frequency_from_csv(self, csv_file):
        """CSV íŒŒì¼ì—ì„œ ë¹ˆë„ ë°ì´í„° êµ¬ì¶•"""
        from collections import Counter
        import os

        try:
            print(f"ğŸ“Š ë¹ˆë„ ë¶„ì„ ì‹œì‘: {csv_file}")

            df = safe_read_csv(csv_file)
            if df is None:
                return {}

            print(f"   ğŸ“‹ ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {len(df)} rows, {len(df.columns)} columns")
            print(f"   ğŸ“‹ ì»¬ëŸ¼ëª…: {list(df.columns)}")

            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
            found_column = find_text_column(df)
            if not found_column:
                print(f"   âš ï¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {}

            print(f"   ğŸ“ ì‚¬ìš©í•  ì»¬ëŸ¼: {found_column}")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = df[found_column].dropna().astype(str).tolist()
            print(f"   ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")

            if not texts:
                print(f"   âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                return {}

            # ë¹ˆë„ ë¶„ì„
            freq_result = self._calculate_word_frequencies(texts)
            print(f"   âœ… ë¹ˆë„ ë¶„ì„ ì™„ë£Œ: {len(freq_result)}ê°œ ë‹¨ì–´")

            return freq_result

        except Exception as e:
            print(f"âŒ ë¹ˆë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
        return {}

    def _calculate_word_frequencies(self, texts):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°"""
        from collections import Counter

        word_counts = Counter()
        for text in texts:
            doc = self.nlp(text)
            for token in doc:
                if (
                    len(token.text) >= 3
                    and token.is_alpha
                    and not token.is_stop
                    and token.pos_ not in ["PUNCT", "SPACE", "SYM"]
                ):
                    lemma = token.lemma_.lower()
                    if lemma != "-PRON-":
                        word_counts[lemma] += 1

        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë“±ê¸‰ ë¶€ì—¬
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        freq_tiers = {}
        total_words = len(sorted_words)

        for rank, (word, count) in enumerate(sorted_words, 1):
            if rank <= total_words * 0.05:
                tier = 1.0
            elif rank <= total_words * 0.2:
                tier = 2.0
            elif rank <= total_words * 0.5:
                tier = 3.0
            else:
                tier = 4.0 + (rank - total_words * 0.5) / (total_words * 0.5)
            freq_tiers[word] = tier

        print(f"âœ… ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì™„ë£Œ: {len(freq_tiers)}ê°œ ë‹¨ì–´")
        return freq_tiers

    def _print_initialization_summary(self):
        """ì´ˆê¸°í™” ì™„ë£Œ ìš”ì•½ ì¶œë ¥"""
        print(f"âœ… ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(
            f"   â€¢ í†µí•© ì»¨í…ìŠ¤íŠ¸ ì˜ë¯¸ ìƒì„±: {'âœ…' if self.settings['USE_INTEGRATED_CONTEXTUAL'] else 'âŒ'}"
        )
        print(
            f"   â€¢ í†µí•© ë‚œì´ë„ ë¶„ì„: {'âœ…' if self.settings['USE_INTEGRATED_DIFFICULTY'] else 'âŒ'}"
        )
        print(f"   â€¢ ê³ ê¸‰ êµ¬ë™ì‚¬ ë¶„ì„: âœ…")
        print(f"   â€¢ ë¬¸ë²• íŒ¨í„´ ë¶„ì„: âœ…")

        print(f"\nğŸ“Š ë°ì´í„° ì†ŒìŠ¤ ë¡œë”© ìƒí™©:")
        print(f"=" * 60)
        print(f"   ğŸ‘¤ ì‚¬ìš©ì ì „ì²´ ë‹¨ì–´: {len(self.user_words)}ê°œ")
        print(f"   ğŸ“ ì‚¬ìš©ì ìˆ™ì–´: {len(self.user_idioms)}ê°œ")
        print(f"   ğŸ”¤ ì‚¬ìš©ì ë‹¨ì¼ ë‹¨ì–´: {len(self.user_single_words)}ê°œ")
        print(f"   ğŸ›ï¸ ì°¸ì¡° ìˆ™ì–´ DB: {len(self.reference_idioms)}ê°œ")
        print(f"   ğŸ“š ì‰¬ìš´ ë‹¨ì–´: {len(self.easy_words)}ê°œ")
        print(f"   ğŸ“Š ë¹ˆë„ ë°ì´í„°: {len(self.freq_tiers)}ê°œ")
        print(f"=" * 60)

    def load_cache_from_file(self, cache_file=None):
        """GPT ìºì‹œ ë¡œë“œ"""
        import json

        if not cache_file:
            cache_file = self.settings["CACHE_FILE"]

        if os.path.exists(cache_file):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)

                if isinstance(cache_data, dict) and "cache" in cache_data:
                    serialized_cache = cache_data["cache"]
                    if "token_usage" in cache_data:
                        self.gpt_token_usage = cache_data["token_usage"]
                    if "call_count" in cache_data:
                        self.gpt_call_count = cache_data["call_count"]
                else:
                    serialized_cache = cache_data

                for k, v in serialized_cache.items():
                    try:
                        self.gpt_cache[k] = v
                    except Exception as e:
                        if self.verbose:
                            print(f"âš ï¸ ìºì‹œ í•­ëª© ë³€í™˜ ì‹¤íŒ¨: {k} - {e}")

                print(f"âœ… GPT ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.gpt_cache)}ê°œ í•­ëª©")
            except Exception as e:
                print(f"âš ï¸ GPT ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.gpt_cache = {}

    def save_cache_to_file(self, cache_file=None):
        """GPT ìºì‹œ ì €ì¥"""
        import json

        if not cache_file:
            cache_file = self.settings["CACHE_FILE"]

        serializable_cache = {str(k): v for k, v in self.gpt_cache.items()}
        cache_data = {
            "cache": serializable_cache,
            "token_usage": self.gpt_token_usage,
            "call_count": self.gpt_call_count,
        }

        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… GPT ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.gpt_cache)}ê°œ í•­ëª©")
        except Exception as e:
            print(f"âš ï¸ GPT ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_passage_info(self, text_input):
        """í…ìŠ¤íŠ¸ ì…ë ¥ì—ì„œ ì§€ë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            # text_inputì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸ (data_loaderì—ì„œ ì˜¨ ê²½ìš°)
            if isinstance(text_input, dict):
                return {
                    "textbook_id": text_input.get("textbook_id"),
                    "book_title": text_input.get("book_title", "Advanced Vocabulary"),
                    "textbook_studio_passage_id": text_input.get(
                        "textbook_studio_passage_id"
                    ),
                    "textbook_unit_id": text_input.get("textbook_unit_id"),
                    "studio_title": text_input.get("studio_title"),
                    "studio_series": text_input.get("studio_series"),
                    "studio_title2": text_input.get("studio_title2"),
                    "textbook_studio_passage_title": text_input.get(
                        "textbook_studio_passage_title"
                    ),
                    "passage_order": text_input.get("passage_order"),
                    "content": text_input.get("content", ""),
                }

            # ê¸°ë³¸ê°’ ë°˜í™˜ (í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°)
            return {
                "textbook_id": None,
                "book_title": "Advanced Vocabulary",
                "textbook_studio_passage_id": None,
                "textbook_unit_id": None,
                "studio_title": "",
                "studio_series": "",
                "studio_title2": "",
                "textbook_studio_passage_title": "",
                "passage_order": 1,
                "content": str(text_input) if isinstance(text_input, str) else "",
            }

        except Exception as e:
            print(f"âš ï¸ ì§€ë¬¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "textbook_id": None,
                "book_title": "Advanced Vocabulary",
                "textbook_studio_passage_id": None,
                "textbook_unit_id": None,
                "studio_title": "",
                "studio_series": "",
                "studio_title2": "",
                "textbook_studio_passage_title": "",
                "passage_order": 1,
                "content": "",
            }

    def generate_vocabulary_workbook(
        self, texts, output_file="vocabulary_advanced.xlsx", **kwargs
    ):
        """ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± - ìˆ˜ì •ëœ ë²„ì „"""
        start_time = time.time()

        print(f"ğŸš€ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ì‹œì‘")
        print(f"   â€¢ ë©”íƒ€ë°ì´í„° í¬í•¨ ì²˜ë¦¬: âœ…")
        print(f"   â€¢ ì‚¬ìš©ì DB ìˆ™ì–´: âœ… {len(self.user_idioms)}ê°œ í™œìš©")
        print(f"   â€¢ ì‚¬ìš©ì DB ë‹¨ì–´: âœ… {len(self.user_single_words)}ê°œ í™œìš©")

        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        results = []
        for idx, text_input in enumerate(
            tqdm(texts, desc="ğŸ“ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ (ë©”íƒ€ë°ì´í„° í¬í•¨)", unit="ì§€ë¬¸")
        ):
            try:
                # âœ… í…ìŠ¤íŠ¸ ì…ë ¥ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
                if isinstance(text_input, dict):
                    # data_loaderì—ì„œ ì˜¨ ë©”íƒ€ë°ì´í„° í¬í•¨ í…ìŠ¤íŠ¸
                    text_content = text_input.get("content", "")
                    text_id = text_input.get("id", f"text_{idx + 1}")
                elif isinstance(text_input, str):
                    # ë‹¨ìˆœ í…ìŠ¤íŠ¸
                    text_content = text_input
                    text_id = f"text_{idx + 1}"
                else:
                    print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸ í˜•ì‹: {type(text_input)}")
                    continue

                # âœ… process_text_with_metadata í˜¸ì¶œ (ìƒˆ ë©”ì„œë“œ)
                result = self.process_text_with_metadata(
                    text_input,  # ì „ì²´ ì…ë ¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
                    text_id,
                    self.easy_words,
                    set(),
                    self.freq_tiers,
                )

                if result is not None and isinstance(result, (list, tuple)):
                    results.extend(result)
                else:
                    print(f"âš ï¸ í…ìŠ¤íŠ¸ {idx + 1}: ìœ íš¨í•˜ì§€ ì•Šì€ ê²°ê³¼")

            except Exception as e:
                print(f"âŒ í…ìŠ¤íŠ¸ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        if not results:
            print("âš ï¸ ì¶”ì¶œëœ ë‹¨ì–´/ìˆ™ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        df = pd.DataFrame(results)

        # í’ˆì§ˆ ê²€ì‚¬
        quality_results = None
        if kwargs.get("enable_quality_check", True):
            df, quality_results = self.run_quality_check_and_fix(df, output_file)

        # ì‚¬ìš©ì DB ë§¤ì¹­ ë¶„ì„
        if len(df) > 0:
            self._analyze_user_db_matching(df)

        # Excel ì €ì¥
        try:
            df.to_excel(output_file, index=False)
            print(f"âœ… ë‹¨ì–´ì¥ ì €ì¥ ì™„ë£Œ: {output_file}")
        except Exception as e:
            print(f"âŒ Excel ì €ì¥ ì‹¤íŒ¨: {e}")

        # ìºì‹œ ì €ì¥
        self.save_cache_to_file()

        # ê²°ê³¼ ìš”ì•½
        processing_time = time.time() - start_time
        print(f"\nğŸ¯ ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ ë‹¨ì–´ì¥ ìƒì„± ê²°ê³¼:")
        print(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        print(f"   ğŸ“Š ì´ í•­ëª© ìˆ˜: {len(df)}ê°œ")
        print(f"   ğŸ“Š GPT í˜¸ì¶œ íšŸìˆ˜: {self.gpt_call_count}íšŒ")
        print(f"   ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: {self.gpt_token_usage['total_tokens']}ê°œ")

        if self.settings["USE_INTEGRATED_CONTEXTUAL"]:
            print(f"   âœ¨ í†µí•© ì»¨í…ìŠ¤íŠ¸ ì˜ë¯¸ ìƒì„±ìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ")
        if self.settings["USE_INTEGRATED_DIFFICULTY"]:
            print(f"   âœ¨ í†µí•© ë‚œì´ë„ ë¶„ì„ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ")

        print(f"   ğŸ”¥ ì‚¬ìš©ì DB ìš°ì„  + ê³ ê¸‰ íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ì •ë°€í•œ ë‹¨ì–´ì¥ ìƒì„±")

        # íŒ¨í„´ë³„ í†µê³„ ì¶œë ¥
        if "ë§¤ì¹­ë°©ì‹" in df.columns:
            pattern_stats = df["ë§¤ì¹­ë°©ì‹"].value_counts()
            print(f"\nğŸ“Š íŒ¨í„´ë³„ ì¶”ì¶œ í†µê³„:")
            for pattern, count in pattern_stats.items():
                if pattern:
                    print(f"   â€¢ {pattern}: {count}ê°œ")

        return df

    def _analyze_user_db_matching(self, df):
        """ì‚¬ìš©ì DB ë§¤ì¹­ ë¶„ì„"""
        try:
            if "ì‚¬ìš©ìDBë§¤ì¹­" in df.columns:
                user_matched = df["ì‚¬ìš©ìDBë§¤ì¹­"].sum()
                total_items = len(df)
                match_ratio = (
                    (user_matched / total_items * 100) if total_items > 0 else 0
                )

                print(f"   ğŸ‘¤ ì‚¬ìš©ì DB ë§¤ì¹­ í•­ëª©: {user_matched}ê°œ")
                print(f"   ğŸ“Š ì „ì²´ ëŒ€ë¹„ ë¹„ìœ¨: {match_ratio:.1f}%")

                # ë§¤ì¹­ ë°©ì‹ë³„ ë¶„í¬
                if "ë§¤ì¹­ë°©ì‹" in df.columns and user_matched > 0:
                    user_df = df[df["ì‚¬ìš©ìDBë§¤ì¹­"] == True]
                    match_types = user_df["ë§¤ì¹­ë°©ì‹"].value_counts()
                    print(f"   ğŸ” ë§¤ì¹­ ë°©ì‹ë³„ ë¶„í¬:")
                    for match_type, count in match_types.items():
                        if match_type:
                            print(f"      â€¢ {match_type}: {count}ê°œ")

                # í¬í•¨ ì´ìœ ë³„ ë¶„í¬
                if "í¬í•¨ì´ìœ " in df.columns:
                    inclusion_reasons = df["í¬í•¨ì´ìœ "].value_counts()
                    print(f"   ğŸ“‹ í¬í•¨ ì´ìœ ë³„ ë¶„í¬:")
                    for reason, count in inclusion_reasons.items():
                        if reason:
                            print(f"      â€¢ {reason}: {count}ê°œ")

        except Exception as e:
            print(f"   âŒ ì‚¬ìš©ì DB ë§¤ì¹­ ë¶„ì„ ì‹¤íŒ¨: {e}")

    def run_quality_check_and_fix(self, df, output_file):
        """í’ˆì§ˆ ê²€ì‚¬ ë° ìë™ ìˆ˜ì •"""
        print("\nğŸ” í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘...")

        try:
            temp_file = output_file.replace(".xlsx", "_temp.xlsx")
            df.to_excel(temp_file, index=False)

            checker = quality_checker.ContextualVocabularyQualityChecker(
                temp_file, self.client
            )
            results = checker.print_detailed_quality_report()  # âœ… ì˜¬ë°”ë¥¸ ë©”ì„œë“œ

            print("ğŸ”§ í’ˆì§ˆ ë¬¸ì œ ìë™ ìˆ˜ì • ì¤‘...")
            fixed_df = checker.fix_quality_issues(apply_fixes=True)  # âœ… ì˜¬ë°”ë¥¸ ë©”ì„œë“œ
            if os.path.exists(temp_file):
                os.remove(temp_file)

            print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {results['quality_score']:.1f}/100")
            print(f"ğŸ“Š ë°œê²¬ëœ ë¬¸ì œ: {results['quality_breakdown']['total_issues']}ê°œ")

            return fixed_df, results

        except Exception as e:
            print(f"âŒ í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return df, None

    def debug_gpt_filter_status(self):
        """GPT í•„í„° ìƒíƒœ ë””ë²„ê¹…"""
        print("\nğŸ” GPT í•„í„° ìƒíƒœ í™•ì¸:")
        print(f"   â€¢ hasattr(self, 'gpt_filter'): {hasattr(self, 'gpt_filter')}")
        if hasattr(self, "gpt_filter"):
            print(f"   â€¢ gpt_filter is not None: {self.gpt_filter is not None}")
            if self.gpt_filter:
                print(f"   â€¢ user_words ìˆ˜: {len(self.gpt_filter.user_words)}")
                print(f"   â€¢ baseline ì„¤ì •: {self.gpt_filter.user_db_baseline}")
                print(f"   â€¢ GPT í˜¸ì¶œ ìˆ˜: {self.gpt_filter.gpt_calls}")
        print()

    def test_gpt_filter_integration(self):
        """GPT í•„í„° í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª GPT í•„í„° í†µí•© í…ŒìŠ¤íŠ¸:")
        print(
            f"   â€¢ gpt_filter ì¡´ì¬: {hasattr(self, 'gpt_filter') and self.gpt_filter is not None}"
        )
        if hasattr(self, "gpt_filter") and self.gpt_filter:
            print(f"   â€¢ ì‚¬ìš©ì ë‹¨ì–´ ìˆ˜: {len(self.gpt_filter.user_words)}ê°œ")
            print(f"   â€¢ ê¸°ì¤€ì  ì„¤ì •: {self.gpt_filter.user_db_baseline is not None}")
            if self.gpt_filter.user_db_baseline:
                baseline = self.gpt_filter.user_db_baseline
                print(f"   â€¢ í‰ê·  ì ìˆ˜: {baseline['average_score']:.1f}")
                print(f"   â€¢ ìµœì†Œ ì„ê³„ê°’: {baseline['min_threshold']:.1f}")
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")

    def enhanced_extract_user_db_idioms_with_separable(self, text):
        """ë¶„ë¦¬í˜• ê°ì§€ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ì‚¬ìš©ì DB ìˆ™ì–´ ì¶”ì¶œ"""
        results = []
        text_str = force_extract_text(text)
        found_positions = set()

        print(
            f"   ğŸ” ì‚¬ìš©ì DB ìˆ™ì–´ ë§¤ì¹­ ê²€ì‚¬ (ë¶„ë¦¬í˜• í¬í•¨): {len(self.user_idioms)}ê°œ"
        )

        # 1. ë¶„ë¦¬í˜• ìˆ™ì–´ ê°ì§€ (ìµœìš°ì„ )
        if hasattr(self, "separable_detector"):
            separable_results = self.separable_detector.detect_separable_idioms_in_text(
                text_str
            )

            for sep_result in separable_results:
                context = get_sentence_context(
                    text_str, sep_result["start"], sep_result["end"]
                )
                meaning = self.enhanced_korean_definition(
                    sep_result["display_form"], context, is_phrase=True
                )

                results.append(
                    {
                        "original": sep_result["original"],
                        "base_form": sep_result["display_form"],  # ë¶„ë¦¬í˜• í‘œì‹œ í¬í•¨
                        "meaning": meaning,
                        "context": context,
                        "type": "user_db_separable_idiom",
                        "is_separated": sep_result["is_separated"],
                        "confidence": sep_result["confidence"],
                        "user_db_match": True,
                        "match_type": f"ì‚¬ìš©ìDBë¶„ë¦¬í˜•_{sep_result['description']}",
                        "separable_info": sep_result["separable_info"],
                    }
                )

                found_positions.add((sep_result["start"], sep_result["end"]))

        # 2. ì¼ë°˜ ìˆ™ì–´ ì¶”ì¶œ (ë¶„ë¦¬í˜•ì´ ì•„ë‹Œ ê²ƒë“¤)
        non_separable_idioms = self.user_idioms
        if hasattr(self, "user_separable_idioms"):
            non_separable_idioms = self.user_idioms - set(
                self.user_separable_idioms.keys()
            )

        # ê¸¸ì´ìˆœ ì •ë ¬ (ê¸´ ìˆ™ì–´ë¶€í„° ë§¤ì¹­í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
        sorted_regular_idioms = sorted(non_separable_idioms, key=len, reverse=True)

        for idiom in sorted_regular_idioms:
            # ì •í™•í•œ ë§¤ì¹­ (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤)
            import re

            pattern = r"\b" + re.escape(idiom) + r"\b"
            matches = re.finditer(pattern, text_str, re.IGNORECASE)

            for match in matches:
                start, end = match.span()

                # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                if any(abs(start - pos[0]) < 5 for pos in found_positions):
                    continue

                context = get_sentence_context(text_str, start, end)
                original_text = text_str[start:end]

                meaning = self.enhanced_korean_definition(
                    idiom, context, is_phrase=True
                )

                results.append(
                    {
                        "original": original_text,
                        "base_form": idiom,
                        "meaning": meaning,
                        "context": context,
                        "type": "user_db_idiom",
                        "is_separated": False,
                        "confidence": 0.95,
                        "user_db_match": True,
                        "match_type": "ì‚¬ìš©ìDBì¼ë°˜ìˆ™ì–´",
                    }
                )
                found_positions.add((start, end))

        # ê²°ê³¼ í†µê³„
        separable_count = len(
            [r for r in results if r.get("type") == "user_db_separable_idiom"]
        )
        regular_count = len([r for r in results if r.get("type") == "user_db_idiom"])

        print(
            f"   ğŸ“Š ì‚¬ìš©ì DB ë§¤ì¹­ ê²°ê³¼: ë¶„ë¦¬í˜• {separable_count}ê°œ, ì¼ë°˜ {regular_count}ê°œ"
        )

        return results

    def is_word_appropriate_for_extraction(self, word, context="", pos=""):
        """í†µí•©ëœ ë‹¨ì–´ ì í•©ì„± íŒë³„"""
        word_lower = word.lower()

        # ì‚¬ìš©ì DBì— ì´ë¯¸ ìˆëŠ” ë‹¨ì–´ëŠ” ë¬´ì¡°ê±´ ì í•©
        if word_lower in self.user_words:
            return True, "ì‚¬ìš©ìDBí¬í•¨"

        # ê¸°ë³¸ ë‹¨ì–´ ê°•ë ¥ ì°¨ë‹¨
        from config import BASIC_VERBS, BASIC_ADJECTIVES, BASIC_NOUNS

        if (
            word_lower in BASIC_VERBS
            or word_lower in BASIC_ADJECTIVES
            or word_lower in BASIC_NOUNS
        ):
            return False, "ê¸°ë³¸ë‹¨ì–´ì œì™¸"

        # ì™¸ë¶€ DB ê¸°ë³¸ ì–´íœ˜ ì°¨ë‹¨
        from external_vocab_db import is_basic_by_external_db

        if is_basic_by_external_db(word_lower):
            return False, "ì™¸ë¶€DBê¸°ë³¸ì–´íœ˜"

        # ì‚¬ìš©ì DB ìš°ì„  í¬í•¨
        if word_lower in self.user_single_words:
            return True, "ì‚¬ìš©ìDBìš°ì„ í¬í•¨"

        # GPT ë¬¸ë§¥ë³„ ë‚œì´ë„ ë¶„ì„
        if hasattr(self, "gpt_filter") and self.gpt_filter:
            appropriate, reason = self.gpt_filter.is_word_appropriate_for_user_db(
                word, context, pos
            )
            return appropriate, reason

        # ê¸°ë³¸ íŒë³„ ë¡œì§
        if len(word) < 4:
            return False, "ê¸¸ì´ë¶€ì¡±"

        return True, "ê¸°ë³¸í†µê³¼"
# main_extractor.py íŒŒì¼ ëë¶€ë¶„ì— ë‹¤ìŒ ë©”ì„œë“œë“¤ì„ ì¶”ê°€í•˜ì„¸ìš”:

    def enhanced_korean_definition(self, word, context, is_phrase=False):
        """AIë¥¼ ì‚¬ìš©í•œ í•œê¸€ ëœ» ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = f"korean_def_{word}_{hash(context[:100])}_{is_phrase}"
        if cache_key in self.gpt_cache:
            return self.gpt_cache[cache_key]

        # GPT í˜¸ì¶œ ì œí•œ í™•ì¸
        if self.gpt_call_count >= self.GPT_CALL_LIMIT:
            return f"{word}ì˜ ì˜ë¯¸"

        try:
            if is_phrase:
                prompt = f"""Please provide a simple and clear Korean meaning for the following English idiom/phrase.

**Idiom/Phrase**: {word}
**Context**: {context[:200]}

Requirements:
1. Answer in Korean only (no English words)
2. Keep it simple and clear (within 5 words)
3. Match the contextual meaning

Korean meaning:"""
            else:
                prompt = f"""Please provide a simple and clear Korean meaning for the following English word.

**Word**: {word}
**Context**: {context[:200]}

Requirements:
1. Answer in Korean only (no English words)
2. Keep it simple and clear (within 3 words)
3. Match the contextual meaning

Korean meaning:"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50,
                temperature=0.1,
            )

            result = response.choices[0].message.content.strip()
            
            # ê²°ê³¼ ì •ë¦¬
            result = result.replace("í•œêµ­ì–´ ëœ»:", "").strip()
            result = result.strip('"\'')
            
            # ì˜ì–´ê°€ í¬í•¨ë˜ê±°ë‚˜ ë„ˆë¬´ ê¸¸ë©´ ê¸°ë³¸ê°’
            if any(char.isalpha() and ord(char) < 128 for char in result) or len(result) > 20:
                result = f"{word}ì˜ ì˜ë¯¸"
            
            # ìºì‹œ ì €ì¥
            self.gpt_cache[cache_key] = result
            self.gpt_call_count += 1
            
            return result

        except Exception as e:
            print(f"âš ï¸ í•œê¸€ ëœ» ìƒì„± ì‹¤íŒ¨ ({word}): {e}")
            return f"{word}ì˜ ì˜ë¯¸"

    def generate_synonyms_and_antonyms(self, word, context=""):
        """ë™ì˜ì–´ì™€ ë°˜ì˜ì–´ ìƒì„±"""
        # ë‚´ì¥ AI ê¸°ë°˜ ë™ì˜ì–´/ë°˜ì˜ì–´ ìƒì„±ë§Œ ì‚¬ìš©
        cache_key = f"synonyms_{word}_{hash(context[:100])}"
        if cache_key in self.gpt_cache:
            cached = self.gpt_cache[cache_key]
            return cached.get('synonyms', ''), cached.get('antonyms', '')

        if self.gpt_call_count >= self.GPT_CALL_LIMIT:
            return "", ""

        try:
            prompt = f"""Find synonyms and antonyms for the following English word.

**Word**: {word}
**Context**: {context[:200]}

Requirements:
1. Up to 3 synonyms (separated by commas)
2. Up to 2 antonyms (separated by commas)
3. Only words that fit the context
4. Empty string if none available

Format:
Synonyms: word1, word2, word3
Antonyms: word1, word2"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=0.1,
            )

            result = response.choices[0].message.content.strip()
            
            # ê²°ê³¼ íŒŒì‹±
            synonyms = ""
            antonyms = ""
            
            lines = result.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('Synonyms:'):
                    synonyms = line.split(':', 1)[1].strip().replace(' ', '')
                elif line.startswith('Antonyms:'):
                    antonyms = line.split(':', 1)[1].strip().replace(' ', '')
            
            # ìºì‹œ ì €ì¥
            self.gpt_cache[cache_key] = {'synonyms': synonyms, 'antonyms': antonyms}
            self.gpt_call_count += 1
            
            return synonyms, antonyms

        except Exception as e:
            print(f"âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ìƒì„± ì‹¤íŒ¨ ({word}): {e}")
            return "", ""
    def process_text_with_metadata(self, text_input, text_id, easy_words, excluded_words, freq_tiers):
        """ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ - í•œê¸€ ëœ» ë° ë™ì˜ì–´ ìƒì„± í¬í•¨"""
        results = []
        
        try:
            # ì§€ë¬¸ ì •ë³´ ì¶”ì¶œ
            passage_info = self.get_passage_info(text_input)
            text_content = passage_info["content"]
            
            if not text_content.strip():
                return results

            # 1. ì‚¬ìš©ì DB ìˆ™ì–´ ì¶”ì¶œ (ê¸°ì¡´ ë©”ì„œë“œ í™œìš©)
            user_idiom_results = self.enhanced_extract_user_db_idioms_with_separable(text_content)
            
            for result in user_idiom_results:
                # ë™ì˜ì–´ ìƒì„±
                synonyms, antonyms = self.generate_synonyms_and_antonyms(
                    result['base_form'], 
                    result['context']
                )
                
                # ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
                vocab_entry = {
                    "êµì¬ID": passage_info.get("textbook_id"),
                    "êµì¬ëª…": passage_info.get("book_title", "Advanced Vocabulary"),
                    "ì§€ë¬¸ID": passage_info.get("textbook_studio_passage_id", text_id),
                    "ìˆœì„œ": len(results) + 1,
                    "ì§€ë¬¸": text_content[:100] + "..." if len(text_content) > 100 else text_content,
                    "ë‹¨ì–´": result["original"],
                    "ì›í˜•": result["base_form"],
                    "í’ˆì‚¬": "",
                    "ëœ»(í•œê¸€)": result["meaning"],
                    "ëœ»(ì˜ì–´)": "",
                    "ë™ì˜ì–´": synonyms,
                    "ë°˜ì˜ì–´": antonyms,
                    "ë¬¸ë§¥": result["context"],
                    "ë¶„ë¦¬í˜•ì—¬ë¶€": result.get("is_separated", False),
                    "ì‹ ë¢°ë„": result.get("confidence", 0.95),
                    "ì‚¬ìš©ìDBë§¤ì¹­": result.get("user_db_match", True),
                    "ë§¤ì¹­ë°©ì‹": result.get("match_type", "ì‚¬ìš©ìDBìˆ™ì–´"),
                    "íŒ¨í„´ì •ë³´": f"Studio: {passage_info.get('studio_title', '')}, Unit: {passage_info.get('textbook_unit_id', '')}",
                    "ë¬¸ë§¥ì ì˜ë¯¸": "",
                    "ë™ì˜ì–´ì‹ ë¢°ë„": 0.8 if synonyms else 0,
                    "ì²˜ë¦¬ë°©ì‹": "",
                    "í¬í•¨ì´ìœ ": f"ì§€ë¬¸ '{passage_info.get('textbook_studio_passage_title', 'Unknown')}' ì—ì„œ ì¶”ì¶œ"
                }
                results.append(vocab_entry)

            # 2. ì‚¬ìš©ì DB ë‹¨ì¼ ë‹¨ì–´ ì¶”ì¶œ
            user_word_results = self._extract_user_single_words(text_content)
            
            for result in user_word_results:
                # í•œê¸€ ëœ» ìƒì„±
                if result['meaning'] == f"{result['base_form']}ì˜ ì˜ë¯¸":
                    result['meaning'] = self.enhanced_korean_definition(
                        result['base_form'], 
                        result['context'], 
                        is_phrase=False
                    )
                
                # ë™ì˜ì–´ ìƒì„±
                synonyms, antonyms = self.generate_synonyms_and_antonyms(
                    result['base_form'], 
                    result['context']
                )
                
                vocab_entry = {
                    "êµì¬ID": passage_info.get("textbook_id"),
                    "êµì¬ëª…": passage_info.get("book_title", "Advanced Vocabulary"),
                    "ì§€ë¬¸ID": passage_info.get("textbook_studio_passage_id", text_id),
                    "ìˆœì„œ": len(results) + 1,
                    "ì§€ë¬¸": text_content[:100] + "..." if len(text_content) > 100 else text_content,
                    "ë‹¨ì–´": result["original"],
                    "ì›í˜•": result["base_form"],
                    "í’ˆì‚¬": self._get_pos_from_context(result['base_form'], result['context']),
                    "ëœ»(í•œê¸€)": result["meaning"],
                    "ëœ»(ì˜ì–´)": "",
                    "ë™ì˜ì–´": synonyms,
                    "ë°˜ì˜ì–´": antonyms,
                    "ë¬¸ë§¥": result["context"],
                    "ë¶„ë¦¬í˜•ì—¬ë¶€": False,
                    "ì‹ ë¢°ë„": result.get("confidence", 1.0),
                    "ì‚¬ìš©ìDBë§¤ì¹­": result.get("user_db_match", True),
                    "ë§¤ì¹­ë°©ì‹": result.get("match_type", "ì‚¬ìš©ìDBë‹¨ì–´"),
                    "íŒ¨í„´ì •ë³´": f"Studio: {passage_info.get('studio_title', '')}, Unit: {passage_info.get('textbook_unit_id', '')}",
                    "ë¬¸ë§¥ì ì˜ë¯¸": "",
                    "ë™ì˜ì–´ì‹ ë¢°ë„": 0.8 if synonyms else 0,
                    "ì²˜ë¦¬ë°©ì‹": "",
                    "í¬í•¨ì´ìœ ": f"ì§€ë¬¸ '{passage_info.get('textbook_studio_passage_title', 'Unknown')}' ì—ì„œ ì¶”ì¶œ"
                }
                results.append(vocab_entry)

            return results

        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return results

    def _extract_user_single_words(self, text_content):
        """ì‚¬ìš©ì DB ë‹¨ì¼ ë‹¨ì–´ ì¶”ì¶œ"""
        results = []
        text_str = force_extract_text(text_content)
        found_positions = set()

        for word in self.user_single_words:
            import re
            pattern = r"\b" + re.escape(word) + r"\b"
            matches = re.finditer(pattern, text_str, re.IGNORECASE)

            for match in matches:
                start, end = match.span()
                
                # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                if any(abs(start - pos[0]) < 3 for pos in found_positions):
                    continue

                context = get_sentence_context(text_str, start, end)
                original_text = text_str[start:end]

                results.append({
                    "original": original_text,
                    "base_form": word,
                    "meaning": f"{word}ì˜ ì˜ë¯¸",  # ë‚˜ì¤‘ì— AIë¡œ ëŒ€ì²´
                    "context": context,
                    "type": "user_db_word",
                    "confidence": 1.0,
                    "user_db_match": True,
                    "match_type": "ì‚¬ìš©ìDBë‹¨ì–´",
                })
                
                found_positions.add((start, end))

        return results

    def _get_pos_from_context(self, word, context):
        """ë¬¸ë§¥ì—ì„œ í’ˆì‚¬ ì¶”ì¶œ"""
        try:
            doc = self.nlp(context)
            for token in doc:
                if token.lemma_.lower() == word.lower():
                    return get_simple_pos(token.pos_)
            return ""
        except:
            return ""
        
