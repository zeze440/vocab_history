# quality_checker.py - ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ê²€ì‚¬ + í•œê¸€ ëœ» ë° ë™ì˜ì–´ ë³´ì™„ ê¸°ëŠ¥ ì¶”ê°€

import pandas as pd
import os
import re
from collections import defaultdict
import json


class ContextualVocabularyQualityChecker:
    def __init__(
        self, vocabulary_file, openai_client, cache_file="quality_check_cache.json"
    ):
        self.vocabulary_file = vocabulary_file
        self.client = openai_client
        self.cache_file = cache_file
        self.cache = self.load_cache()
        self.ai_call_count = 0

        # DataFrame ë¡œë“œ
        self.df = (
            pd.read_excel(vocabulary_file)
            if vocabulary_file.endswith(".xlsx")
            else pd.read_csv(vocabulary_file)
        )

        # ì»¬ëŸ¼ëª… ë§¤í•‘
        self.column_mapping = {
            "word": ["ë‹¨ì–´", "ì›í˜•", "word", "base_form", "original"],
            "meaning": ["ëœ»(í•œê¸€)", "ì˜ë¯¸", "í•œê¸€ëœ»", "meaning", "korean_meaning"],
            "context": ["ë¬¸ë§¥", "ì˜ˆë¬¸", "context", "sentence", "example"],
            "passage_id": ["ì§€ë¬¸ID", "passage_id", "text_id", "source_id"],
            "original_text": ["ì›ë¬¸", "original", "found_text"],
            "type": ["ìœ í˜•", "type", "entry_type", "item_type"],
            "is_idiom": ["ìˆ™ì–´ì—¬ë¶€", "is_idiom", "is_phrase", "idiom_flag"],
            "synonyms": ["ë™ì˜ì–´", "synonyms", "synonym"],
            "antonyms": ["ë°˜ì˜ì–´", "antonyms", "antonym"],
        }

        self.actual_columns = self._find_actual_columns()
        print(f"ğŸ” ì¢…í•© í’ˆì§ˆ ê²€ì‚¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ (í•œê¸€ ëœ» ë° ë™ì˜ì–´ ë³´ì™„ í¬í•¨)")

    def _find_actual_columns(self):
        """ì‹¤ì œ ì»¬ëŸ¼ëª… ì°¾ê¸°"""
        actual = {}
        df_columns_lower = [col.lower() for col in self.df.columns]

        for standard_name, possible_names in self.column_mapping.items():
            for possible in possible_names:
                if possible.lower() in df_columns_lower:
                    actual_col = self.df.columns[
                        df_columns_lower.index(possible.lower())
                    ]
                    actual[standard_name] = actual_col
                    break

        return actual

    def load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_cache(self):
        """ìºì‹œ ì €ì¥"""
        try:
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def generate_contextual_quality_report(self):
        """ì¢…í•© ë¬¸ë§¥ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ” ì¢…í•© ë¬¸ë§¥ í’ˆì§ˆ ë¶„ì„ ì‹œì‘...")

        results = {
            "total_entries": len(self.df),
            "ai_calls_used": 0,
            "issues_found": [],
            "quality_breakdown": {
                "within_passage_duplicates": 0,
                "context_meaning_mismatches": 0,
                "word_meaning_mismatches": 0,
                "idiom_word_conflicts": 0,
                "missing_korean_meanings": 0,  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
                "missing_synonyms": 0,  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
                "total_issues": 0,
            },
        }

        # 1. ì§€ë¬¸ ë‚´ ì¤‘ë³µ ê²€ì‚¬
        print("ğŸ“ ì§€ë¬¸ ë‚´ ì¤‘ë³µ ë‹¨ì–´ ê²€ì‚¬...")
        duplicate_issues = self._check_within_passage_duplicates()
        results["issues_found"].extend(duplicate_issues)
        results["quality_breakdown"]["within_passage_duplicates"] = len(
            duplicate_issues
        )

        # 2. ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ê²€ì‚¬
        print("ğŸ”— ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ê²€ì‚¬...")
        conflict_issues = self._check_idiom_word_conflicts()
        results["issues_found"].extend(conflict_issues)
        results["quality_breakdown"]["idiom_word_conflicts"] = len(conflict_issues)

        # 3. ğŸ”¥ í•œê¸€ ëœ» ëˆ„ë½/ì˜¤ë¥˜ ê²€ì‚¬ (ìƒˆë¡œ ì¶”ê°€)
        print("ğŸ‡°ğŸ‡· í•œê¸€ ëœ» í’ˆì§ˆ ê²€ì‚¬...")
        korean_meaning_issues = self._check_korean_meaning_quality()
        results["issues_found"].extend(korean_meaning_issues)
        results["quality_breakdown"]["missing_korean_meanings"] = len(korean_meaning_issues)

        # 4. ğŸ”¥ ë™ì˜ì–´/ë°˜ì˜ì–´ ëˆ„ë½ ê²€ì‚¬ (ìƒˆë¡œ ì¶”ê°€)
        print("ğŸ”— ë™ì˜ì–´/ë°˜ì˜ì–´ í’ˆì§ˆ ê²€ì‚¬...")
        synonym_issues = self._check_synonym_quality()
        results["issues_found"].extend(synonym_issues)
        results["quality_breakdown"]["missing_synonyms"] = len(synonym_issues)

        # 5. ë¬¸ë§¥-ì˜ë¯¸ ì í•©ì„± ê²€ì‚¬ (AI ì‚¬ìš©)
        print("ğŸ¤– ë¬¸ë§¥-ì˜ë¯¸ ì í•©ì„± ê²€ì‚¬...")
        context_issues = self._check_context_meaning_alignment()
        results["issues_found"].extend(context_issues)
        results["quality_breakdown"]["context_meaning_mismatches"] = len(context_issues)

        # 6. ë‹¨ì–´-ì˜ë¯¸ ì •í™•ì„± ê²€ì‚¬ (AI ì‚¬ìš©)
        print("ğŸ” ë‹¨ì–´-ì˜ë¯¸ ì •í™•ì„± ê²€ì‚¬...")
        word_meaning_issues = self._check_word_meaning_accuracy()
        results["issues_found"].extend(word_meaning_issues)
        results["quality_breakdown"]["word_meaning_mismatches"] = len(
            word_meaning_issues
        )

        # ì´ ë¬¸ì œ ìˆ˜ ê³„ì‚°
        results["quality_breakdown"]["total_issues"] = len(results["issues_found"])
        results["ai_calls_used"] = self.ai_call_count

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        results["quality_score"] = self._calculate_contextual_quality_score(results)

        # ìºì‹œ ì €ì¥
        self.save_cache()

        return results

    def _check_korean_meaning_quality(self):
        """í•œê¸€ ëœ» í’ˆì§ˆ ê²€ì‚¬"""
        issues = []

        if "meaning" not in self.actual_columns or "word" not in self.actual_columns:
            print("âš ï¸ í•œê¸€ ëœ» ê²€ì‚¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return issues

        meaning_col = self.actual_columns["meaning"]
        word_col = self.actual_columns["word"]

        for idx, row in self.df.iterrows():
            word = str(row[word_col]).strip()
            meaning = str(row[meaning_col]).strip()

            # ë¬¸ì œ íŒ¨í„´ ê°ì§€
            is_problematic = False
            problem_type = ""

            # 1. "ë‹¨ì–´ì˜ ì˜ë¯¸" íŒ¨í„´
            if meaning.endswith("ì˜ ì˜ë¯¸") or meaning.endswith("ì˜ë¯¸"):
                is_problematic = True
                problem_type = "generic_meaning_pattern"

            # 2. ì˜ì–´ê°€ í¬í•¨ëœ ê²½ìš°
            elif any(char.isalpha() and ord(char) < 128 for char in meaning):
                is_problematic = True
                problem_type = "contains_english"

            # 3. ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš°
            elif len(meaning) < 2 or meaning in ["", "nan", "None"]:
                is_problematic = True
                problem_type = "missing_or_too_short"

            # 4. ë„ˆë¬´ ê¸´ ê²½ìš° (30ì ì´ˆê³¼)
            elif len(meaning) > 30:
                is_problematic = True
                problem_type = "too_long"

            if is_problematic:
                issues.append({
                    "type": "missing_korean_meaning",
                    "severity": "high" if problem_type in ["missing_or_too_short", "generic_meaning_pattern"] else "medium",
                    "index": idx,
                    "word": word,
                    "current_meaning": meaning,
                    "problem_type": problem_type,
                    "description": f"'{word}'ì˜ í•œê¸€ ëœ»ì´ ë¶€ì ì ˆí•¨: {meaning}",
                    "recommendation": "AIë¡œ ì˜¬ë°”ë¥¸ í•œê¸€ ëœ» ìƒì„± í•„ìš”"
                })

        print(f"   ğŸ“Š í•œê¸€ ëœ» ë¬¸ì œ ë°œê²¬: {len(issues)}ê°œ")
        return issues

    def _check_synonym_quality(self):
        """ë™ì˜ì–´/ë°˜ì˜ì–´ í’ˆì§ˆ ê²€ì‚¬"""
        issues = []

        if "synonyms" not in self.actual_columns or "word" not in self.actual_columns:
            print("âš ï¸ ë™ì˜ì–´ ê²€ì‚¬ë¥¼ ìœ„í•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì„ íƒì‚¬í•­)")
            return issues

        synonyms_col = self.actual_columns["synonyms"]
        word_col = self.actual_columns["word"]

        empty_synonyms = 0
        for idx, row in self.df.iterrows():
            word = str(row[word_col]).strip()
            synonyms = str(row[synonyms_col]).strip()

            # ë™ì˜ì–´ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
            if not synonyms or synonyms in ["", "nan", "None", "NaN"]:
                empty_synonyms += 1
                issues.append({
                    "type": "missing_synonyms",
                    "severity": "low",
                    "index": idx,
                    "word": word,
                    "description": f"'{word}'ì˜ ë™ì˜ì–´ê°€ ë¹„ì–´ìˆìŒ",
                    "recommendation": "AIë¡œ ë™ì˜ì–´ ìƒì„± ê¶Œì¥"
                })

        print(f"   ğŸ“Š ë™ì˜ì–´ ëˆ„ë½: {empty_synonyms}ê°œ")
        return issues

    def _check_idiom_word_conflicts(self):
        """ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ê²€ì‚¬"""
        issues = []

        required_cols = ["word", "passage_id"]
        if not all(col in self.actual_columns for col in required_cols):
            print("âš ï¸ ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ê²€ì‚¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return issues

        word_col = self.actual_columns["word"]
        passage_col = self.actual_columns["passage_id"]
        type_col = self.actual_columns.get("type", None)

        # ìˆ™ì–´ì™€ ë‹¨ì–´ êµ¬ë¶„
        idioms = set()
        single_words = set()

        # ì§€ë¬¸ë³„ë¡œ ê·¸ë£¹í™”
        for passage_id, group in self.df.groupby(passage_col):
            passage_idioms = set()
            passage_words = set()

            for _, row in group.iterrows():
                word = str(row[word_col]).strip().lower()
                entry_type = (
                    str(row[type_col]).lower()
                    if type_col and pd.notna(row[type_col])
                    else ""
                )

                # ìˆ™ì–´ íŒë³„: ê³µë°±ì´ ìˆê±°ë‚˜ typeì— idiom/phrase í¬í•¨
                if (
                    " " in word
                    or "idiom" in entry_type
                    or "phrase" in entry_type
                    or "ìˆ™ì–´" in entry_type
                ):
                    passage_idioms.add(word)
                    idioms.add(word)
                else:
                    passage_words.add(word)
                    single_words.add(word)

            # ì´ ì§€ë¬¸ì—ì„œ ì¶©ëŒ ê²€ì‚¬
            conflicts = self._find_idiom_word_conflicts_in_passage(
                passage_id, passage_idioms, passage_words, group
            )
            issues.extend(conflicts)

        print(f"   ğŸ“Š ì „ì²´ ìˆ™ì–´: {len(idioms)}ê°œ, ë‹¨ì¼ ë‹¨ì–´: {len(single_words)}ê°œ")
        print(f"   ğŸ“Š ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ë°œê²¬: {len(issues)}ê°œ")

        return issues

    def _find_idiom_word_conflicts_in_passage(
        self, passage_id, idioms, words, group_df
    ):
        """íŠ¹ì • ì§€ë¬¸ì—ì„œ ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ì°¾ê¸°"""
        conflicts = []

        word_col = self.actual_columns["word"]

        for idiom in idioms:
            # ìˆ™ì–´ë¥¼ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ ì¶”ì¶œ
            idiom_words = set(idiom.split())

            # ì´ ì§€ë¬¸ì˜ ë‹¨ì¼ ë‹¨ì–´ë“¤ê³¼ ë¹„êµ
            conflicting_words = idiom_words.intersection(words)

            if conflicting_words:
                for conflicting_word in conflicting_words:
                    # ì¶©ëŒí•˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ë“¤ ì°¾ê¸°
                    word_indices = group_df[
                        group_df[word_col].str.lower() == conflicting_word
                    ].index.tolist()
                    idiom_indices = group_df[
                        group_df[word_col].str.lower() == idiom
                    ].index.tolist()

                    conflicts.append(
                        {
                            "type": "idiom_word_conflict",
                            "severity": "high",
                            "passage_id": passage_id,
                            "idiom": idiom,
                            "conflicting_word": conflicting_word,
                            "word_indices": word_indices,
                            "idiom_indices": idiom_indices,
                            "description": f"ì§€ë¬¸ {passage_id}ì—ì„œ ìˆ™ì–´ '{idiom}'ê³¼ ë‹¨ì¼ ë‹¨ì–´ '{conflicting_word}'ê°€ ì¤‘ë³µ ì¶”ì¶œë¨",
                            "recommendation": f"ìˆ™ì–´ '{idiom}'ì´ ìˆìœ¼ë¯€ë¡œ ë‹¨ì¼ ë‹¨ì–´ '{conflicting_word}' ì œê±° ê¶Œì¥",
                        }
                    )

        return conflicts

    def _check_within_passage_duplicates(self):
        """ì§€ë¬¸ ë‚´ ì¤‘ë³µ ë‹¨ì–´ ê²€ì‚¬ (ê¸°ì¡´ ì½”ë“œ)"""
        issues = []

        if "passage_id" not in self.actual_columns or "word" not in self.actual_columns:
            print("âš ï¸ ì§€ë¬¸ID ë˜ëŠ” ë‹¨ì–´ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return issues

        passage_col = self.actual_columns["passage_id"]
        word_col = self.actual_columns["word"]

        # ì§€ë¬¸ë³„ë¡œ ê·¸ë£¹í™”
        passage_groups = self.df.groupby(passage_col)

        for passage_id, group in passage_groups:
            if len(group) <= 1:
                continue

            # ê° ì§€ë¬¸ ë‚´ì—ì„œ ì¤‘ë³µ ë‹¨ì–´ ì°¾ê¸°
            word_counts = group[word_col].value_counts()
            duplicates = word_counts[word_counts > 1]

            for word, count in duplicates.items():
                duplicate_indices = group[group[word_col] == word].index.tolist()
                issues.append(
                    {
                        "type": "within_passage_duplicate",
                        "severity": "high",
                        "passage_id": passage_id,
                        "word": word,
                        "count": count,
                        "indices": duplicate_indices,
                        "description": f"ì§€ë¬¸ {passage_id}ì—ì„œ '{word}' ë‹¨ì–´ê°€ {count}ë²ˆ ì¤‘ë³µë¨",
                    }
                )

        print(f"   ğŸ“Š ì§€ë¬¸ ë‚´ ì¤‘ë³µ ë°œê²¬: {len(issues)}ê°œ")
        return issues

    def _check_context_meaning_alignment(self):
        """ë¬¸ë§¥-ì˜ë¯¸ ì í•©ì„± ê²€ì‚¬ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
        issues = []

        required_cols = ["word", "meaning", "context"]
        if not all(col in self.actual_columns for col in required_cols):
            print("âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼(ë‹¨ì–´, ì˜ë¯¸, ë¬¸ë§¥)ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return issues

        word_col = self.actual_columns["word"]
        meaning_col = self.actual_columns["meaning"]
        context_col = self.actual_columns["context"]

        # ìƒ˜í”Œë§
        check_df = self.df.dropna(subset=[word_col, meaning_col, context_col])
        sample_size = min(50, len(check_df))  # ìµœëŒ€ 50ê°œë§Œ ê²€ì‚¬
        check_df = check_df.sample(n=sample_size, random_state=42)

        print(f"   ğŸ¤– {len(check_df)}ê°œ í•­ëª©ì˜ ë¬¸ë§¥-ì˜ë¯¸ ì í•©ì„± ê²€ì‚¬ ì¤‘...")

        for idx, row in check_df.iterrows():
            word = str(row[word_col]).strip()
            meaning = str(row[meaning_col]).strip()
            context = str(row[context_col]).strip()

            # ìºì‹œ í™•ì¸
            cache_key = f"context_meaning_{word}_{hash(context)}_{hash(meaning)}"
            if cache_key in self.cache:
                result = self.cache[cache_key]
            else:
                result = self._check_context_meaning_with_ai(word, meaning, context)
                if result:
                    self.cache[cache_key] = result
                    self.ai_call_count += 1

            if result and not result.get("is_appropriate", True):
                issues.append(
                    {
                        "type": "context_meaning_mismatch",
                        "severity": result.get("severity", "medium"),
                        "index": idx,
                        "word": word,
                        "meaning": meaning,
                        "context": (
                            context[:100] + "..." if len(context) > 100 else context
                        ),
                        "ai_analysis": result.get("explanation", ""),
                        "description": f"'{word}'ì˜ í•œê¸€ ëœ» '{meaning}'ì´ ë¬¸ë§¥ì— ë¶€ì ì ˆí•¨",
                    }
                )

        print(f"   ğŸ“Š ë¬¸ë§¥-ì˜ë¯¸ ë¶€ì í•© ë°œê²¬: {len(issues)}ê°œ")
        return issues

    def _check_word_meaning_accuracy(self):
        """ë‹¨ì–´-ì˜ë¯¸ ì •í™•ì„± ê²€ì‚¬ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
        issues = []
        
        required_cols = ["word", "meaning"]
        if not all(col in self.actual_columns for col in required_cols):
            print("âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼(ë‹¨ì–´, ì˜ë¯¸)ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return issues

        word_col = self.actual_columns["word"]
        meaning_col = self.actual_columns["meaning"]

        check_df = self.df.dropna(subset=[word_col, meaning_col])
        sample_size = min(30, len(check_df))  # ìµœëŒ€ 30ê°œë§Œ ê²€ì‚¬
        check_df = check_df.sample(n=sample_size, random_state=42)

        print(f"   ğŸ¤– {len(check_df)}ê°œ í•­ëª©ì˜ ë‹¨ì–´-ì˜ë¯¸ ì •í™•ì„± ê²€ì‚¬ ì¤‘...")

        for idx, row in check_df.iterrows():
            word = str(row[word_col]).strip()
            meaning = str(row[meaning_col]).strip()

            # ìºì‹œ í™•ì¸
            cache_key = f"word_meaning_{word}_{hash(meaning)}"
            if cache_key in self.cache:
                result = self.cache[cache_key]
            else:
                result = self._check_word_meaning_with_ai(word, meaning)
                if result:
                    self.cache[cache_key] = result
                    self.ai_call_count += 1

            if result and not result.get("is_accurate", True):
                issues.append(
                    {
                        "type": "word_meaning_mismatch",
                        "severity": result.get("severity", "medium"),
                        "index": idx,
                        "word": word,
                        "meaning": meaning,
                        "ai_analysis": result.get("explanation", ""),
                        "suggested_meaning": result.get("suggested_meaning", ""),
                        "description": f"'{word}'ì˜ í•œê¸€ ëœ» '{meaning}'ì´ ì›í˜• ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ë¶ˆì¼ì¹˜",
                    }
                )

        print(f"   ğŸ“Š ë‹¨ì–´-ì˜ë¯¸ ë¶ˆì¼ì¹˜ ë°œê²¬: {len(issues)}ê°œ")
        return issues

    def _check_context_meaning_with_ai(self, word, meaning, context):
        """AIë¥¼ ì‚¬ìš©í•œ ë¬¸ë§¥-ì˜ë¯¸ ì í•©ì„± ê²€ì‚¬"""
        prompt_template = """Please evaluate whether the provided Korean meaning is appropriate for the English word in the given context.

**Word**: WORD_PLACEHOLDER
**Korean Meaning**: MEANING_PLACEHOLDER  
**Context**: CONTEXT_PLACEHOLDER

Evaluation Criteria:
1. Does the Korean meaning fit the word's usage in this context?
2. Is it contextually appropriate?

Please respond in the following JSON format:
{
    "is_appropriate": true,
    "severity": "low",
    "explanation": "Reason for the evaluation"
}"""
        
        prompt = (prompt_template
                 .replace("WORD_PLACEHOLDER", word)
                 .replace("MEANING_PLACEHOLDER", meaning)
                 .replace("CONTEXT_PLACEHOLDER", context[:200]))

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.1,
            )

            result_text = response.choices[0].message.content.strip()
            
            try:
                return json.loads(result_text)
            except json.JSONDecodeError:
                is_appropriate = ("true" in result_text.lower() and 
                                "is_appropriate" in result_text.lower())
                return {
                    "is_appropriate": is_appropriate,
                    "severity": "medium", 
                    "explanation": result_text,
                }

        except Exception as e:
            print(f"âš ï¸ AI ë¬¸ë§¥-ì˜ë¯¸ ê²€ì‚¬ ì‹¤íŒ¨ ({word}): {e}")
            return None

    def _check_word_meaning_with_ai(self, word, meaning):
        """AIë¥¼ ì‚¬ìš©í•œ ë‹¨ì–´-ì˜ë¯¸ ì •í™•ì„± ê²€ì‚¬"""
        prompt_template = """Please evaluate whether the provided Korean meaning is appropriate for the English word.

**Word**: WORD_PLACEHOLDER
**Provided Korean Meaning**: MEANING_PLACEHOLDER

Evaluation Criteria:
1. Does the Korean meaning match the word's basic dictionary meaning?
2. Is it accurate and appropriate?
3. Is it neither too general nor too specific?

Please respond in the following JSON format:
{
    "is_accurate": true,
    "severity": "low",
    "explanation": "Reason why it's inappropriate or appropriate",
    "suggested_meaning": "More appropriate Korean meaning (only if inappropriate)"
}"""

        prompt = (prompt_template
                 .replace("WORD_PLACEHOLDER", word)
                 .replace("MEANING_PLACEHOLDER", meaning))

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.1,
            )

            result_text = response.choices[0].message.content.strip()

            try:
                return json.loads(result_text)
            except json.JSONDecodeError:
                is_accurate = (
                    "true" in result_text.lower()
                    and "is_accurate" in result_text.lower()
                )
                return {
                    "is_accurate": is_accurate,
                    "severity": "medium",
                    "explanation": result_text,
                }

        except Exception as e:
            print(f"âš ï¸ AI ë‹¨ì–´-ì˜ë¯¸ ê²€ì‚¬ ì‹¤íŒ¨ ({word}): {e}")
            return None

    def fix_quality_issues(self, apply_fixes=True):
        """í’ˆì§ˆ ë¬¸ì œ ìë™ ìˆ˜ì • (í•œê¸€ ëœ» ë° ë™ì˜ì–´ ë³´ì™„ í¬í•¨)"""
        if not apply_fixes:
            print("âš ï¸ ìë™ ìˆ˜ì •ì´ ë¹„í™œì„±í™”ë¨")
            return self.df

        print("ğŸ”§ ì¢…í•© í’ˆì§ˆ ë¬¸ì œ ìë™ ìˆ˜ì • ì‹œì‘...")
        fixed_df = self.df.copy()
        fix_count = 0

        # 1. ğŸ”¥ í•œê¸€ ëœ» ë³´ì™„ (ìµœìš°ì„ )
        korean_fixes = self._fix_korean_meanings(fixed_df)
        fixed_df = korean_fixes["df"]
        fix_count += korean_fixes["fixed_count"]

        # 2. ğŸ”¥ ë™ì˜ì–´/ë°˜ì˜ì–´ ë³´ì™„
        synonym_fixes = self._fix_synonyms_antonyms(fixed_df)
        fixed_df = synonym_fixes["df"]
        fix_count += synonym_fixes["fixed_count"]

        # 3. ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ í•´ê²°
        conflict_fixes = self._fix_idiom_word_conflicts(fixed_df)
        fixed_df = conflict_fixes["df"]
        fix_count += conflict_fixes["fixed_count"]

        # 4. ì§€ë¬¸ ë‚´ ì¤‘ë³µ ì œê±°
        duplicate_fixes = self._fix_within_passage_duplicates(fixed_df)
        fixed_df = duplicate_fixes["df"]
        fix_count += duplicate_fixes["fixed_count"]

        print(f"ğŸ¯ ì´ {fix_count}ê°œ í•­ëª© ìˆ˜ì • ì™„ë£Œ")
        return fixed_df

    def _fix_korean_meanings(self, df):
        """í•œê¸€ ëœ» ìë™ ë³´ì™„"""
        print("ğŸ‡°ğŸ‡· í•œê¸€ ëœ» ìë™ ë³´ì™„ ì¤‘...")

        if "meaning" not in self.actual_columns or "word" not in self.actual_columns:
            return {"df": df, "fixed_count": 0}

        fixed_df = df.copy()
        fix_count = 0
        meaning_col = self.actual_columns["meaning"]
        word_col = self.actual_columns["word"]
        context_col = self.actual_columns.get("context", None)

        for idx, row in fixed_df.iterrows():
            word = str(row[word_col]).strip()
            meaning = str(row[meaning_col]).strip()
            context = str(row[context_col]).strip() if context_col else ""

            # ë¬¸ì œ ìˆëŠ” í•œê¸€ ëœ» ê°ì§€
            needs_fix = (
                meaning.endswith("ì˜ ì˜ë¯¸") or 
                meaning.endswith("ì˜ë¯¸") or
                any(char.isalpha() and ord(char) < 128 for char in meaning) or
                len(meaning) < 2 or 
                meaning in ["", "nan", "None", "NaN"]
            )

            if needs_fix:
                # AIë¡œ í•œê¸€ ëœ» ìƒì„±
                new_meaning = self._generate_korean_meaning_with_ai(word, context)
                if new_meaning and new_meaning != f"{word}ì˜ ì˜ë¯¸":
                    fixed_df.at[idx, meaning_col] = new_meaning
                    fix_count += 1
                    print(f"   âœ… '{word}': '{meaning}' â†’ '{new_meaning}'")

        print(f"   ğŸ“Š í•œê¸€ ëœ» ìˆ˜ì •: {fix_count}ê°œ")
        return {"df": fixed_df, "fixed_count": fix_count}

    def _generate_korean_meaning_with_ai(self, word, context=""):
        """AIë¥¼ ì‚¬ìš©í•œ í•œê¸€ ëœ» ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = f"korean_meaning_{word}_{hash(context[:100])}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        # ìˆ™ì–´ì¸ì§€ ë‹¨ì–´ì¸ì§€ íŒë³„
        is_phrase = " " in word

        try:
            if is_phrase:
                prompt = f"""ë‹¤ìŒ ì˜ì–´ ìˆ™ì–´/êµ¬ë¬¸ì˜ í•œêµ­ì–´ ëœ»ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.

**ìˆ™ì–´/êµ¬ë¬¸**: {word}
**ë¬¸ë§¥**: {context[:200]}

ì¡°ê±´:
1. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€ (ì˜ì–´ ë‹¨ì–´ í¬í•¨ ê¸ˆì§€)
2. ê°„ë‹¨ëª…ë£Œí•˜ê²Œ (5ë‹¨ì–´ ì´ë‚´)
3. ë¬¸ë§¥ì— ë§ëŠ” ì˜ë¯¸
4. "~í•˜ë‹¤", "~ë˜ë‹¤" ë“± ë™ì‚¬í˜•ìœ¼ë¡œ ëë‚˜ëŠ” ê²ƒì´ ì¢‹ìŒ

ì˜ˆì‹œ:
- "give up" â†’ "í¬ê¸°í•˜ë‹¤"
- "look forward to" â†’ "ê¸°ëŒ€í•˜ë‹¤"
- "a lot of" â†’ "ë§ì€"

í•œêµ­ì–´ ëœ»:"""
            else:
                prompt = f"""ë‹¤ìŒ ì˜ì–´ ë‹¨ì–´ì˜ í•œêµ­ì–´ ëœ»ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.

**ë‹¨ì–´**: {word}
**ë¬¸ë§¥**: {context[:200]}

ì¡°ê±´:
1. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€ (ì˜ì–´ ë‹¨ì–´ í¬í•¨ ê¸ˆì§€)
2. ê°„ë‹¨ëª…ë£Œí•˜ê²Œ (3ë‹¨ì–´ ì´ë‚´)
3. ë¬¸ë§¥ì— ë§ëŠ” ì˜ë¯¸
4. ê¸°ë³¸ ì‚¬ì „ì  ì˜ë¯¸ ìœ„ì£¼

ì˜ˆì‹œ:
- "efficient" â†’ "íš¨ìœ¨ì ì¸"
- "remarkable" â†’ "ë†€ë¼ìš´"
- "analysis" â†’ "ë¶„ì„"

í•œêµ­ì–´ ëœ»:"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50,
                temperature=0.1,
            )

            result = response.choices[0].message.content.strip()
            
            # ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬
            korean_meaning = self._clean_korean_meaning(result, word)
            
            # ìºì‹œ ì €ì¥
            self.cache[cache_key] = korean_meaning
            self.ai_call_count += 1
            
            return korean_meaning

        except Exception as e:
            print(f"âš ï¸ í•œê¸€ ëœ» ìƒì„± ì‹¤íŒ¨ ({word}): {e}")
            return f"{word}ì˜ ì˜ë¯¸"

    def _clean_korean_meaning(self, result, word):
        """í•œê¸€ ëœ» ê²°ê³¼ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        result = result.replace("í•œêµ­ì–´ ëœ»:", "").strip()
        result = result.replace("ë‹µë³€:", "").strip()
        result = result.replace("ì˜ë¯¸:", "").strip()
        
        # ë”°ì˜´í‘œ ì œê±°
        result = result.strip('"\'')
        
        # ì˜ì–´ ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš° ì²˜ë¦¬
        if any(char.isalpha() and ord(char) < 128 for char in result):
            # ì˜ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            return f"{word}ì˜ ì˜ë¯¸"
        
        # ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
        if len(result) > 20 or len(result) < 1:
            return f"{word}ì˜ ì˜ë¯¸"
        
        return result

    def _generate_synonyms_with_ai(self, word, context=""):
        """AIë¥¼ ì‚¬ìš©í•œ ë™ì˜ì–´/ë°˜ì˜ì–´ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = f"synonyms_{word}_{hash(context[:100])}"
        if cache_key in self.cache:
            cached = self.cache[cache_key]
            return cached.get('synonyms', ''), cached.get('antonyms', '')

        try:
            prompt = f"""ë‹¤ìŒ ì˜ì–´ ë‹¨ì–´ì˜ ë™ì˜ì–´ì™€ ë°˜ì˜ì–´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

**ë‹¨ì–´**: {word}
**ë¬¸ë§¥**: {context[:200]}

ì¡°ê±´:
1. ë™ì˜ì–´ 3ê°œ ì´ë‚´ (ì‰¼í‘œë¡œ êµ¬ë¶„)
2. ë°˜ì˜ì–´ 2ê°œ ì´ë‚´ (ì‰¼í‘œë¡œ êµ¬ë¶„)
3. ë¬¸ë§¥ì— ë§ëŠ” ë‹¨ì–´ë“¤ë§Œ
4. ë„ˆë¬´ ì–´ë ¤ìš´ ë‹¨ì–´ëŠ” ì œì™¸
5. ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ë‹µë³€

í˜•ì‹:
ë™ì˜ì–´: word1, word2, word3
ë°˜ì˜ì–´: word1, word2

ì˜ˆì‹œ:
**ë‹¨ì–´**: happy
ë™ì˜ì–´: glad, joyful, cheerful
ë°˜ì˜ì–´: sad, unhappy"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=0.1,
            )

            result = response.choices[0].message.content.strip()
            
            # ê²°ê³¼ íŒŒì‹±
            synonyms, antonyms = self._parse_synonyms_result(result)
            
            # ìºì‹œ ì €ì¥
            self.cache[cache_key] = {
                'synonyms': synonyms,
                'antonyms': antonyms
            }
            self.ai_call_count += 1
            
            return synonyms, antonyms

        except Exception as e:
            print(f"âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ìƒì„± ì‹¤íŒ¨ ({word}): {e}")
            return "", ""

    def _parse_synonyms_result(self, result):
        """ë™ì˜ì–´/ë°˜ì˜ì–´ ê²°ê³¼ íŒŒì‹±"""
        synonyms = ""
        antonyms = ""
        
        lines = result.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('ë™ì˜ì–´:') or line.startswith('Synonyms:'):
                synonyms = line.split(':', 1)[1].strip()
            elif line.startswith('ë°˜ì˜ì–´:') or line.startswith('Antonyms:'):
                antonyms = line.split(':', 1)[1].strip()
        
        # ì •ë¦¬
        synonyms = synonyms.replace(' ', '').strip() if synonyms else ""
        antonyms = antonyms.replace(' ', '').strip() if antonyms else ""
        
        return synonyms, antonyms

    def _fix_idiom_word_conflicts(self, df):
        """ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ìë™ ìˆ˜ì •"""
        print("ğŸ”— ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ ìë™ ìˆ˜ì • ì¤‘...")

        fixed_df = df.copy()
        fix_count = 0

        required_cols = ["word", "passage_id"]
        if not all(col in self.actual_columns for col in required_cols):
            return {"df": fixed_df, "fixed_count": 0}

        word_col = self.actual_columns["word"]
        passage_col = self.actual_columns["passage_id"]
        type_col = self.actual_columns.get("type", None)

        indices_to_remove = set()

        # ì§€ë¬¸ë³„ë¡œ ì²˜ë¦¬
        for passage_id, group in fixed_df.groupby(passage_col):
            passage_idioms = []
            passage_single_words = []

            # ìˆ™ì–´ì™€ ë‹¨ì¼ ë‹¨ì–´ ë¶„ë¥˜
            for idx, row in group.iterrows():
                word = str(row[word_col]).strip().lower()
                entry_type = (
                    str(row[type_col]).lower()
                    if type_col and pd.notna(row[type_col])
                    else ""
                )

                if (
                    " " in word
                    or "idiom" in entry_type
                    or "phrase" in entry_type
                    or "ìˆ™ì–´" in entry_type
                ):
                    passage_idioms.append((idx, word))
                else:
                    passage_single_words.append((idx, word))

            # ì¶©ëŒ í•´ê²°: ìˆ™ì–´ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë‹¨ì¼ ë‹¨ì–´ ì œê±°
            for idiom_idx, idiom in passage_idioms:
                idiom_words = set(idiom.split())

                for word_idx, single_word in passage_single_words:
                    if single_word in idiom_words:
                        indices_to_remove.add(word_idx)
                        fix_count += 1
                        print(f"   âœ… ì œê±°: '{single_word}' (ìˆ™ì–´ '{idiom}' ë•Œë¬¸ì—)")

        # ì¶©ëŒí•˜ëŠ” ë‹¨ì¼ ë‹¨ì–´ë“¤ ì œê±°
        if indices_to_remove:
            fixed_df = fixed_df.drop(indices_to_remove).reset_index(drop=True)

        print(f"   ğŸ“Š ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ í•´ê²°: {fix_count}ê°œ ë‹¨ì¼ ë‹¨ì–´ ì œê±°")
        return {"df": fixed_df, "fixed_count": fix_count}

    def _fix_within_passage_duplicates(self, df):
        """ì§€ë¬¸ ë‚´ ì¤‘ë³µ ì œê±° (ê¸°ì¡´ ì½”ë“œ ê°œì„ )"""
        print("ğŸ“ ì§€ë¬¸ ë‚´ ì¤‘ë³µ ì œê±° ì¤‘...")

        fixed_df = df.copy()
        fix_count = 0

        if "passage_id" not in self.actual_columns or "word" not in self.actual_columns:
            return {"df": fixed_df, "fixed_count": 0}

        passage_col = self.actual_columns["passage_id"]
        word_col = self.actual_columns["word"]
        context_col = self.actual_columns.get("context", None)

        groups_to_keep = []

        for passage_id, group in fixed_df.groupby(passage_col):
            for word, word_group in group.groupby(word_col):
                if len(word_group) > 1:
                    # ê°€ì¥ ì¢‹ì€ ê²ƒ ì„ íƒ
                    if context_col and context_col in fixed_df.columns:
                        # ë¬¸ë§¥ ê¸¸ì´ë¡œ íŒë‹¨
                        best_idx = (
                            word_group[context_col].astype(str).str.len().idxmax()
                        )
                        groups_to_keep.append(word_group.loc[[best_idx]])
                        fix_count += len(word_group) - 1
                    else:
                        # ì²« ë²ˆì§¸ ê²ƒë§Œ ìœ ì§€
                        groups_to_keep.append(word_group.iloc[[0]])
                        fix_count += len(word_group) - 1
                else:
                    groups_to_keep.append(word_group)

        if groups_to_keep:
            fixed_df = pd.concat(groups_to_keep, ignore_index=True)

        print(f"   ğŸ“Š ì§€ë¬¸ ë‚´ ì¤‘ë³µ ì œê±°: {fix_count}ê°œ")
        return {"df": fixed_df, "fixed_count": fix_count}

    def _calculate_contextual_quality_score(self, results):
        """ì¢…í•© ë¬¸ë§¥ ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        total_entries = results["total_entries"]
        if total_entries == 0:
            return 0

        breakdown = results["quality_breakdown"]

        score = 100

        # ì§€ë¬¸ ë‚´ ì¤‘ë³µ (25ì )
        duplicate_penalty = (
            breakdown["within_passage_duplicates"] / total_entries
        ) * 25
        score -= duplicate_penalty

        # ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ (20ì )
        conflict_penalty = (breakdown["idiom_word_conflicts"] / total_entries) * 20
        score -= conflict_penalty

        # ğŸ”¥ í•œê¸€ ëœ» ëˆ„ë½/ì˜¤ë¥˜ (25ì ) - ìƒˆë¡œ ì¶”ê°€
        korean_meaning_penalty = (breakdown["missing_korean_meanings"] / total_entries) * 25
        score -= korean_meaning_penalty

        # ğŸ”¥ ë™ì˜ì–´ ëˆ„ë½ (10ì ) - ìƒˆë¡œ ì¶”ê°€  
        synonym_penalty = (breakdown["missing_synonyms"] / total_entries) * 10
        score -= synonym_penalty

        # ë¬¸ë§¥-ì˜ë¯¸ ë¶€ì í•© (15ì )
        context_penalty = (breakdown["context_meaning_mismatches"] / total_entries) * 15
        score -= context_penalty

        # ë‹¨ì–´-ì˜ë¯¸ ë¶€ì •í™• (5ì )
        word_penalty = (breakdown["word_meaning_mismatches"] / total_entries) * 5
        score -= word_penalty

        return round(max(0, score), 1)
    def _fix_synonyms_antonyms(self, df):
        """ë¬¸ë§¥ì ìœ¼ë¡œ ì •í™•í•œ ë™ì˜ì–´/ë°˜ì˜ì–´ë§Œ ë³´ì™„"""
        print("ğŸ”— ì •í™•í•œ ë™ì˜ì–´/ë°˜ì˜ì–´ ë³´ì™„ ì¤‘...")
        
        fixed_df = df.copy()
        fix_count = 0
        
        # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
        if "word" not in self.actual_columns:
            return {"df": fixed_df, "fixed_count": 0}
        
        word_col = self.actual_columns["word"]
        meaning_col = self.actual_columns.get("meaning", None)
        context_col = self.actual_columns.get("context", None)
        synonyms_col = self.actual_columns.get("synonyms", None)
        antonyms_col = self.actual_columns.get("antonyms", None)
        
        # ë™ì˜ì–´ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
        if not synonyms_col:
            print("   ğŸ’¡ ë™ì˜ì–´ ì»¬ëŸ¼ ì—†ìŒ - ê±´ë„ˆëœ€")
            return {"df": fixed_df, "fixed_count": 0}
        
        # ë™ì˜ì–´ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¶€ì‹¤í•œ í•­ëª©ë“¤ ì°¾ê¸°
        candidates = []
        for idx, row in fixed_df.iterrows():
            word = str(row[word_col]).strip()
            synonyms = str(row[synonyms_col]).strip() if pd.notna(row[synonyms_col]) else ""
            
            # ìˆ™ì–´ëŠ” ê±´ë„ˆëœ€
            if " " in word:
                continue
                
            # ë™ì˜ì–´ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            if (not synonyms or 
                synonyms in ["", "nan", "NaN", "None"] or
                len(synonyms.split(',')) < 2):  # ë™ì˜ì–´ê°€ 1ê°œ ì´í•˜ì¸ ê²½ìš°
                candidates.append(idx)
        
        # ìµœëŒ€ 20ê°œê¹Œì§€ë§Œ ì²˜ë¦¬ (ì •í™•ì„± ìœ„í•´ ì ì€ ìˆ˜)
        max_items = min(20, len(candidates))
        selected_candidates = candidates[:max_items]
        
        print(f"   ğŸ“Š ì •í™•í•œ ë™ì˜ì–´ ìƒì„± ëŒ€ìƒ: {len(candidates)}ê°œ ì¤‘ {max_items}ê°œ ì²˜ë¦¬")
        
        for idx in selected_candidates:
            row = fixed_df.loc[idx]
            word = str(row[word_col]).strip()
            meaning = str(row[meaning_col]).strip() if meaning_col else ""
            context = str(row[context_col]).strip() if context_col else ""
            
            # ì •í™•í•œ ë™ì˜ì–´/ë°˜ì˜ì–´ ìƒì„±
            result = self._generate_contextual_synonyms_antonyms(word, meaning, context)
            
            if result:
                synonyms = result.get('synonyms', '')
                antonyms = result.get('antonyms', '')
                
                if synonyms:
                    fixed_df.at[idx, synonyms_col] = synonyms
                    fix_count += 1
                    print(f"   âœ… '{word}': ë™ì˜ì–´ '{synonyms}' ì¶”ê°€")
                
                if antonyms_col and antonyms:
                    fixed_df.at[idx, antonyms_col] = antonyms
                    print(f"   âœ… '{word}': ë°˜ì˜ì–´ '{antonyms}' ì¶”ê°€")
        
        print(f"   ğŸ“Š ì •í™•í•œ ë™ì˜ì–´/ë°˜ì˜ì–´ ë³´ì™„: {fix_count}ê°œ")
        return {"df": fixed_df, "fixed_count": fix_count}

    def _generate_contextual_synonyms_antonyms(self, word, meaning, context):
        """ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ë™ì˜ì–´/ë°˜ì˜ì–´ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = f"contextual_synonyms_{word}_{hash(meaning)}_{hash(context[:100])}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        try: 
            prompt = f"""Find **exact synonyms and antonyms** that can be substituted for the given English word in this specific context.
**Word**: {word}
**Korean Meaning**: {meaning}
**Context**: {context[:300]}

Strict Requirements:
1. Synonyms: Words that can **exactly replace** '{word}' in this context with identical meaning
2. Antonyms: Words that can **perfectly substitute** '{word}' with opposite meaning in this context
3. **Identical part of speech** required (nounâ†’noun, verbâ†’verb, adjectiveâ†’adjective)
4. **Exact same meaning** (not just similar)
5. **Grammatically perfect** in this specific context
6. If there's ANY doubt, leave empty
7. Maximum 2 words each (only absolutely certain ones)

Response format:
{{
    "synonyms": "word1, word2",
    "antonyms": "word1, word2",
    "explanation": "reasoning for selection"
}}

Examples:
- Context: "The task was very difficult to complete"
difficult (adjective) â†’ synonyms: "hard, challenging" (adjectives only), antonyms: "easy" (adjective only)

- Context: "He decided to analyze the data carefully"  
analyze (verb) â†’ synonyms: "examine" (verb only), antonyms: "" (not certain)

- If ANY uncertainty: synonyms: "", antonyms: ""
"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.1,
            )

            result_text = response.choices[0].message.content.strip()
            
            try:
                # JSON íŒŒì‹± ì‹œë„
                result = json.loads(result_text)
                
                # ê²°ê³¼ ê²€ì¦ ë° ì •ë¦¬
                synonyms = self._clean_synonym_list(result.get('synonyms', ''))
                antonyms = self._clean_synonym_list(result.get('antonyms', ''))
                
                final_result = {
                    'synonyms': synonyms,
                    'antonyms': antonyms,
                    'explanation': result.get('explanation', '')
                }
                
                # ìºì‹œ ì €ì¥
                self.cache[cache_key] = final_result
                self.ai_call_count += 1
                
                return final_result
                
            except json.JSONDecodeError:
                # JSONì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                return self._parse_synonym_text_response(result_text, word)

        except Exception as e:
            print(f"âš ï¸ ë¬¸ë§¥ì  ë™ì˜ì–´/ë°˜ì˜ì–´ ìƒì„± ì‹¤íŒ¨ ({word}): {e}")
            return None

    def _clean_synonym_list(self, synonym_string):
        """ë™ì˜ì–´ ë¦¬ìŠ¤íŠ¸ ì •ë¦¬"""
        if not synonym_string or synonym_string.strip() == "":
            return ""
        
        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
        words = [w.strip() for w in synonym_string.split(',')]
        
        # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ì›ë˜ ë‹¨ì–´ëŠ” ì œì™¸
        cleaned = [w for w in words if w and len(w) > 1]
        
        # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
        cleaned = cleaned[:3]
        
        return ', '.join(cleaned) if cleaned else ""

    def _parse_synonym_text_response(self, text, original_word):
        """í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ"""
        synonyms = ""
        antonyms = ""
        
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if 'synonyms' in line.lower() and ':' in line:
                synonyms = line.split(':', 1)[1].strip().strip('"')
            elif 'antonyms' in line.lower() and ':' in line:
                antonyms = line.split(':', 1)[1].strip().strip('"')
        
        return {
            'synonyms': self._clean_synonym_list(synonyms),
            'antonyms': self._clean_synonym_list(antonyms),
            'explanation': 'Parsed from text response'
        }
    def print_detailed_quality_report(self):
        """ìƒì„¸ í’ˆì§ˆ ë³´ê³ ì„œ ì¶œë ¥ (í•œê¸€ ëœ» ë° ë™ì˜ì–´ ë³´ì™„ í¬í•¨)"""
        results = self.generate_contextual_quality_report()

        print(f"\nğŸ“Š ì¢…í•© ë¬¸ë§¥ í’ˆì§ˆ ë³´ê³ ì„œ")
        print(f"=" * 70)
        print(f"ğŸ“ íŒŒì¼: {os.path.basename(self.vocabulary_file)}")
        print(f"ğŸ“Š ì´ í•­ëª© ìˆ˜: {results['total_entries']}ê°œ")
        print(f"ğŸ¯ ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {results['quality_score']:.1f}/100")
        print(f"ğŸ¤– AI í˜¸ì¶œ íšŸìˆ˜: {results['ai_calls_used']}íšŒ")

        # í’ˆì§ˆ í‰ê°€
        score = results["quality_score"]
        if score >= 90:
            print(f"   âœ… ìš°ìˆ˜í•œ í’ˆì§ˆ! êµìœ¡ìš©ìœ¼ë¡œ ì™„ë²½")
        elif score >= 75:
            print(f"   ğŸŸ¡ ì–‘í˜¸í•œ í’ˆì§ˆ")
        elif score >= 60:
            print(f"   ğŸŸ  í’ˆì§ˆ ê°œì„  í•„ìš”")
        else:
            print(f"   ğŸ”´ ì‹¬ê°í•œ í’ˆì§ˆ ë¬¸ì œ")

        # ë¬¸ì œ ìœ í˜•ë³„ ìš”ì•½
        breakdown = results["quality_breakdown"]
        print(f"\nğŸ” ë°œê²¬ëœ ë¬¸ì œ:")
        print(f"   ğŸ“ ì§€ë¬¸ ë‚´ ì¤‘ë³µ: {breakdown['within_passage_duplicates']}ê°œ")
        print(f"   ğŸ”— ìˆ™ì–´-ë‹¨ì–´ ì¶©ëŒ: {breakdown['idiom_word_conflicts']}ê°œ")
        print(f"   ğŸ‡°ğŸ‡· í•œê¸€ ëœ» ëˆ„ë½/ì˜¤ë¥˜: {breakdown['missing_korean_meanings']}ê°œ")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
        print(f"   ğŸ”— ë™ì˜ì–´ ëˆ„ë½: {breakdown['missing_synonyms']}ê°œ")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
        print(f"   ğŸ¯ ë¬¸ë§¥-ì˜ë¯¸ ë¶€ì í•©: {breakdown['context_meaning_mismatches']}ê°œ")
        print(f"   ğŸ” ë‹¨ì–´-ì˜ë¯¸ ë¶ˆì¼ì¹˜: {breakdown['word_meaning_mismatches']}ê°œ")
        print(f"   ğŸ“Š ì´ ë¬¸ì œ ìˆ˜: {breakdown['total_issues']}ê°œ")

        # ì£¼ìš” ë¬¸ì œ ìƒì„¸ ì •ë³´ (ìƒìœ„ 5ê°œ)
        if results["issues_found"]:
            print(f"\nâš ï¸ ì£¼ìš” ë¬¸ì œ ìƒì„¸:")
            for i, issue in enumerate(results["issues_found"][:5], 1):
                print(f"\n{i}. {issue['description']}")
                if issue["type"] == "idiom_word_conflict":
                    print(f"   ğŸ’¡ ê¶Œì¥ì‚¬í•­: {issue.get('recommendation', '')}")
                elif issue["type"] == "missing_korean_meaning":
                    print(f"   ğŸ’¡ ê¶Œì¥ì‚¬í•­: {issue.get('recommendation', '')}")
                elif issue["type"] == "missing_synonyms":
                    print(f"   ğŸ’¡ ê¶Œì¥ì‚¬í•­: {issue.get('recommendation', '')}")
                elif "ai_analysis" in issue and issue["ai_analysis"]:
                    print(f"   ğŸ¤– AI ë¶„ì„: {issue['ai_analysis']}")
                if "suggested_meaning" in issue and issue["suggested_meaning"]:
                    print(f"   ğŸ’¡ ì œì•ˆ ì˜ë¯¸: {issue['suggested_meaning']}")

            print(f"=" * 70)
        return results

    def export_quality_issues(self, output_file="quality_issues.xlsx"):
        """í’ˆì§ˆ ë¬¸ì œë¥¼ Excelë¡œ ë‚´ë³´ë‚´ê¸° (í•œê¸€ ëœ» ë° ë™ì˜ì–´ ë³´ì™„ í¬í•¨)"""
        results = self.generate_contextual_quality_report()

        if not results["issues_found"]:
            print("âœ… ë‚´ë³´ë‚¼ í’ˆì§ˆ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        # ë¬¸ì œë“¤ì„ DataFrameìœ¼ë¡œ ë³€í™˜
        issues_data = []
        for issue in results["issues_found"]:
            issues_data.append(
                {
                    "ë¬¸ì œìœ í˜•": issue["type"],
                    "ì‹¬ê°ë„": issue["severity"],
                    "ì§€ë¬¸ID": issue.get("passage_id", ""),
                    "ë‹¨ì–´": issue.get("word", ""),
                    "ìˆ™ì–´": issue.get("idiom", ""),
                    "ì¶©ëŒë‹¨ì–´": issue.get("conflicting_word", ""),
                    "í˜„ì¬ì˜ë¯¸": issue.get("current_meaning", ""),  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
                    "ë¬¸ì œìœ í˜•ìƒì„¸": issue.get("problem_type", ""),  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
                    "ì˜ë¯¸": issue.get("meaning", ""),
                    "ë¬¸ë§¥": issue.get("context", ""),
                    "ë¬¸ì œì„¤ëª…": issue["description"],
                    "ê¶Œì¥ì‚¬í•­": issue.get("recommendation", ""),
                    "AIë¶„ì„": issue.get("ai_analysis", ""),
                    "ì œì•ˆì˜ë¯¸": issue.get("suggested_meaning", ""),
                    "í–‰ë²ˆí˜¸": issue.get("index", ""),
                }
            )

        issues_df = pd.DataFrame(issues_data)
        issues_df.to_excel(output_file, index=False)
        print(f"ğŸ“Š í’ˆì§ˆ ë¬¸ì œ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        print(f"   ğŸ“‹ ì´ {len(issues_data)}ê°œ ë¬¸ì œ ê¸°ë¡ë¨")

        # ë¬¸ì œ ìœ í˜•ë³„ í†µê³„
        type_counts = issues_df["ë¬¸ì œìœ í˜•"].value_counts()
        print(f"   ğŸ“ˆ ë¬¸ì œ ìœ í˜•ë³„ ë¶„í¬:")
        for issue_type, count in type_counts.items():
            type_name = {
                "within_passage_duplicate": "ì§€ë¬¸ë‚´ì¤‘ë³µ",
                "idiom_word_conflict": "ìˆ™ì–´-ë‹¨ì–´ì¶©ëŒ",
                "missing_korean_meaning": "í•œê¸€ëœ»ëˆ„ë½/ì˜¤ë¥˜",  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
                "missing_synonyms": "ë™ì˜ì–´ëˆ„ë½",  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
                "context_meaning_mismatch": "ë¬¸ë§¥-ì˜ë¯¸ë¶€ì í•©",
                "word_meaning_mismatch": "ë‹¨ì–´-ì˜ë¯¸ë¶ˆì¼ì¹˜",
            }.get(issue_type, issue_type)
            print(f"      â€¢ {type_name}: {count}ê°œ")

    # ğŸ”¥ ìƒˆë¡œìš´ ê¸°ëŠ¥: í•œê¸€ ëœ» ë° ë™ì˜ì–´ ì¼ê´„ ìƒì„±
    def enhance_vocabulary_meanings_and_synonyms(self, max_items=50):
        """í•œê¸€ ëœ» ë° ë™ì˜ì–´ ì¼ê´„ ìƒì„±/ë³´ì™„"""
        print(f"ğŸš€ í•œê¸€ ëœ» ë° ë™ì˜ì–´ ì¼ê´„ ë³´ì™„ ì‹œì‘ (ìµœëŒ€ {max_items}ê°œ)...")
        
        if "meaning" not in self.actual_columns or "word" not in self.actual_columns:
            print("âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return self.df

        enhanced_df = self.df.copy()
        meaning_col = self.actual_columns["meaning"]
        word_col = self.actual_columns["word"]
        context_col = self.actual_columns.get("context", None)
        synonyms_col = self.actual_columns.get("synonyms", None)

        # 1. í•œê¸€ ëœ» ë³´ì™„ ëŒ€ìƒ ì°¾ê¸°
        problematic_meanings = enhanced_df[
            enhanced_df[meaning_col].astype(str).str.endswith("ì˜ ì˜ë¯¸") |
            enhanced_df[meaning_col].astype(str).str.endswith("ì˜ë¯¸") |
            enhanced_df[meaning_col].isna() |
            (enhanced_df[meaning_col].astype(str).str.len() < 2)
        ].head(max_items // 2)

        print(f"ğŸ“ í•œê¸€ ëœ» ë³´ì™„ ëŒ€ìƒ: {len(problematic_meanings)}ê°œ")

        for idx, row in problematic_meanings.iterrows():
            word = str(row[word_col]).strip()
            context = str(row[context_col]).strip() if context_col else ""
            
            new_meaning = self._generate_korean_meaning_with_ai(word, context)
            if new_meaning and new_meaning != f"{word}ì˜ ì˜ë¯¸":
                enhanced_df.at[idx, meaning_col] = new_meaning
                print(f"   âœ… '{word}': í•œê¸€ ëœ» ìƒì„± ì™„ë£Œ")

        # 2. ë™ì˜ì–´ ë³´ì™„ (ë™ì˜ì–´ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
        if synonyms_col:
            empty_synonyms = enhanced_df[
                enhanced_df[synonyms_col].isna() |
                (enhanced_df[synonyms_col].astype(str).str.strip() == "") |
                (enhanced_df[synonyms_col].astype(str).str.strip() == "nan")
            ].head(max_items // 2)

            print(f"ğŸ”— ë™ì˜ì–´ ë³´ì™„ ëŒ€ìƒ: {len(empty_synonyms)}ê°œ")

            for idx, row in empty_synonyms.iterrows():
                word = str(row[word_col]).strip()
                context = str(row[context_col]).strip() if context_col else ""
                
                synonyms, antonyms = self._generate_synonyms_with_ai(word, context)
                if synonyms:
                    enhanced_df.at[idx, synonyms_col] = synonyms
                    print(f"   âœ… '{word}': ë™ì˜ì–´ ìƒì„± ì™„ë£Œ")

        print(f"ğŸ¯ ì¼ê´„ ë³´ì™„ ì™„ë£Œ! AI í˜¸ì¶œ: {self.ai_call_count}íšŒ")
        return enhanced_df