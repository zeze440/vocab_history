#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ì œ ì‹œìŠ¤í…œ
- VOCABULARY.XLSXì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì¶œë ¥
- ë¬´ë£Œ ì‚¬ì „ APIë“¤ë¡œ êµì°¨ ê²€ì¦í•˜ì—¬ í›„ë³´ ìˆ˜ì§‘
- AI ê¸°ë°˜ ì›í˜• ë‹¨ì–´ íŒë³„
- GPTë¡œ ë¬¸ë§¥ ì í•©ì„± ë° ìˆ˜ì¤€ ê²€ì¦
- ìµœëŒ€ 3ê°œê¹Œì§€ ì—„ì„ 
"""

import pandas as pd
import numpy as np
import re
import argparse
import json
import os
import time
import requests
import openai
from typing import Dict, List, Tuple, Optional, Set
from collections import Counter, defaultdict
from datetime import datetime
import warnings
from dotenv import load_dotenv
import nltk
from nltk.corpus import wordnet

warnings.filterwarnings("ignore")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


def check_dependencies():
    """í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸"""
    missing = []

    try:
        import nltk
    except ImportError:
        missing.append("nltk")

    try:
        import openai
    except ImportError:
        missing.append("openai")

    if missing:
        print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ: {', '.join(missing)}")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install " + " ".join(missing))
        return False
    return True


class ContextualSynonymExtractor:
    """ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì •ë°€ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸°"""

    def __init__(self, client, verbose=False):
        self.client = client
        self.verbose = verbose

        # ğŸ”¥ ì´ ë¶€ë¶„ì—ì„œ nltkë¥¼ import ì—†ì´ ì‚¬ìš©
        try:
            nltk.data.find("tokenizers/punkt")
            nltk.data.find("corpora/wordnet")
        except LookupError:
            nltk.download("punkt", quiet=True)
            nltk.download("wordnet", quiet=True)

    def extract_contextual_meaning_with_gpt(
        self, word: str, context: str, pos: str = ""
    ) -> Dict:
        """GPTë¡œ ë¬¸ë§¥ì—ì„œì˜ êµ¬ì²´ì  ì˜ë¯¸ ì¶”ì¶œ"""

        prompt = f"""
Analyze the word "{word}" in this specific context and determine its EXACT meaning.

Context: "{context}"
Part of Speech: {pos}

Task: Identify the specific meaning/sense of "{word}" as used in THIS context.

Instructions:
1. Look at HOW the word is used in this sentence
2. Determine the specific meaning (not all possible meanings)
3. Consider the surrounding words and overall meaning
4. Focus on the contextual definition

Respond in JSON format:
{{
    "contextual_meaning": "specific meaning of the word in this context",
    "semantic_field": "the domain/field this meaning belongs to (e.g., 'finance', 'nature', 'emotion', 'physical')",
    "usage_type": "how it's used (e.g., 'literal', 'metaphorical', 'technical', 'casual')",
    "meaning_certainty": 0.0-1.0,
    "key_context_clues": ["words or phrases that help determine this meaning"]
}}

Example:
- "bank" in "I went to the bank" â†’ financial institution
- "bank" in "river bank" â†’ edge of water
- "light" in "light weight" â†’ not heavy  
- "light" in "turn on the light" â†’ illumination
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert linguist who identifies specific word meanings in context.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=400,
                temperature=0.1,
            )

            content = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            import json

            result = json.loads(content)
            return result

        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ GPT ì˜ë¯¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "contextual_meaning": "",
                "semantic_field": "general",
                "usage_type": "literal",
                "meaning_certainty": 0.5,
                "key_context_clues": [],
            }

    def get_wordnet_synsets_by_context(
        self, word: str, contextual_meaning: str, pos: str = ""
    ) -> List:
        """ë¬¸ë§¥ì  ì˜ë¯¸ì— ë§ëŠ” WordNet synsetë“¤ë§Œ ì„ ë³„"""

        # í’ˆì‚¬ ë§¤í•‘
        pos_mapping = {
            "n": wordnet.NOUN,
            "v": wordnet.VERB,
            "a": wordnet.ADJ,
            "r": wordnet.ADV,
        }
        wordnet_pos = pos_mapping.get(pos.lower(), None)

        # ëª¨ë“  synset ê°€ì ¸ì˜¤ê¸°
        if wordnet_pos:
            synsets = wordnet.synsets(word, pos=wordnet_pos)
        else:
            synsets = wordnet.synsets(word)

        if not synsets or not contextual_meaning:
            return synsets[:2]  # ê¸°ë³¸ê°’: ì²˜ìŒ 2ê°œë§Œ

        # ë¬¸ë§¥ì  ì˜ë¯¸ì™€ synset ì •ì˜ ë¹„êµ
        relevant_synsets = []
        contextual_keywords = set(contextual_meaning.lower().split())

        for synset in synsets:
            definition = synset.definition().lower()
            examples = [ex.lower() for ex in synset.examples()]

            # ì •ì˜ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            definition_words = set(definition.split())
            example_words = set()
            for ex in examples:
                example_words.update(ex.split())

            # ìœ ì‚¬ë„ ê³„ì‚°
            definition_overlap = len(contextual_keywords & definition_words) / max(
                len(contextual_keywords), 1
            )
            example_overlap = (
                len(contextual_keywords & example_words)
                / max(len(contextual_keywords), 1)
                if example_words
                else 0
            )

            total_score = definition_overlap + (example_overlap * 0.5)

            relevant_synsets.append((synset, total_score))

            if self.verbose:
                print(f"   ğŸ“ {synset.name()}: {definition} (ì ìˆ˜: {total_score:.3f})")

        # ì ìˆ˜ìˆœ ì •ë ¬í•˜ê³  ìƒìœ„ 2ê°œë§Œ ì„ íƒ
        relevant_synsets.sort(key=lambda x: x[1], reverse=True)
        selected_synsets = [
            synset for synset, score in relevant_synsets[:2] if score > 0.1
        ]

        if self.verbose:
            print(f"   âœ… ì„ íƒëœ synset: {len(selected_synsets)}ê°œ")

        return selected_synsets if selected_synsets else synsets[:1]

    def extract_synonyms_from_synsets(
        self, synsets: List, original_word: str
    ) -> List[str]:
        """ì„ ë³„ëœ synsetë“¤ì—ì„œë§Œ ë™ì˜ì–´ ì¶”ì¶œ"""

        synonyms = set()
        original_lower = original_word.lower()

        for synset in synsets:
            for lemma in synset.lemmas():
                lemma_name = lemma.name().replace("_", " ")

                # ê¸°ë³¸ í•„í„°ë§
                if (
                    lemma_name.lower() != original_lower
                    and " " not in lemma_name  # ë‹¨ì¼ ë‹¨ì–´ë§Œ
                    and "-" not in lemma_name  # í•˜ì´í”ˆ ì—†ìŒ
                    and len(lemma_name) >= 3  # ìµœì†Œ 3ê¸€ì
                    and len(lemma_name) <= 12  # ìµœëŒ€ 12ê¸€ì
                    and lemma_name.isalpha()  # ì•ŒíŒŒë²³ë§Œ
                ):
                    synonyms.add(lemma_name)

        return list(synonyms)

    def extract_antonyms_from_synsets(
        self, synsets: List, original_word: str
    ) -> List[str]:
        """ì„ ë³„ëœ synsetë“¤ì—ì„œë§Œ ë°˜ì˜ì–´ ì¶”ì¶œ"""

        antonyms = set()
        original_lower = original_word.lower()

        for synset in synsets:
            for lemma in synset.lemmas():
                for antonym in lemma.antonyms():
                    antonym_name = antonym.name().replace("_", " ")

                    # ê¸°ë³¸ í•„í„°ë§
                    if (
                        antonym_name.lower() != original_lower
                        and " " not in antonym_name  # ë‹¨ì¼ ë‹¨ì–´ë§Œ
                        and "-" not in antonym_name  # í•˜ì´í”ˆ ì—†ìŒ
                        and len(antonym_name) >= 3  # ìµœì†Œ 3ê¸€ì
                        and len(antonym_name) <= 12  # ìµœëŒ€ 12ê¸€ì
                        and antonym_name.isalpha()  # ì•ŒíŒŒë²³ë§Œ
                    ):
                        antonyms.add(antonym_name)

        return list(antonyms)

    def validate_synonyms_with_context(
        self, word: str, synonyms: List[str], context: str, contextual_meaning: str
    ) -> List[str]:
        """ë¬¸ë§¥ê³¼ ì˜ë¯¸ë¥¼ ê³ ë ¤í•œ ë™ì˜ì–´ ê²€ì¦"""

        if not synonyms or len(synonyms) <= 2:
            return synonyms

        prompt = f"""
Validate these synonym candidates for the word "{word}" in this specific context.

Original word: "{word}"
Context: "{context}"
Specific meaning in context: "{contextual_meaning}"
Synonym candidates: {', '.join(synonyms)}

Task: Select ONLY the synonyms that:
1. Have the SAME specific meaning as "{word}" in this context
2. Can replace "{word}" in this sentence without changing the meaning
3. Are appropriate for Korean high school students
4. Are single words (no phrases)

Test each candidate by substitution:
- Original: "{context}"
- With synonym: Would the meaning stay exactly the same?

Respond in JSON format:
{{
    "validated_synonyms": ["synonym1", "synonym2"],
    "rejected_synonyms": [
        {{"word": "rejected_word", "reason": "why it doesn't fit this context"}},
    ],
    "context_analysis": "brief explanation of the word's meaning in this context"
}}

Be VERY strict - when in doubt, reject the synonym.
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a strict English teacher who only approves synonyms that perfectly match the contextual meaning.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=600,
                temperature=0.1,
            )

            content = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                if json_end != -1:
                    content = content[json_start:json_end].strip()

            import json

            result = json.loads(content)
            validated = result.get("validated_synonyms", [])

            if self.verbose:
                rejected = result.get("rejected_synonyms", [])
                print(f"   âœ… ê²€ì¦ í†µê³¼: {validated}")
                if rejected:
                    print(f"   âŒ ê²€ì¦ ì‹¤íŒ¨: {[r['word'] for r in rejected]}")

            return validated[:3]  # ìµœëŒ€ 3ê°œ

        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ ë™ì˜ì–´ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return synonyms[:2]  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ìŒ 2ê°œë§Œ

    def extract_contextual_synonyms_antonyms(
        self, word: str, context: str, pos: str = ""
    ) -> Dict:
        """ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ"""
        # ë¨¼ì € ê¸°ë³¸ ì´ˆê¸°í™”
        self.client = openai.OpenAI()
        # ë‚˜ì¤‘ì— contextual extractor ì´ˆê¸°í™”
        self.contextual_extractor = ContextualSynonymExtractor(
            client=self.client, verbose=self.verbose
        )
        # 1ë‹¨ê³„: GPTë¡œ ë¬¸ë§¥ì  ì˜ë¯¸ ì¶”ì¶œ
        meaning_analysis = self.extract_contextual_meaning_with_gpt(word, context, pos)
        contextual_meaning = meaning_analysis.get("contextual_meaning", "")

        if self.verbose:
            print(f"   ğŸ“ ë¬¸ë§¥ì  ì˜ë¯¸: {contextual_meaning}")

        # 2ë‹¨ê³„: ë¬¸ë§¥ì— ë§ëŠ” WordNet synset ì„ ë³„
        relevant_synsets = self.get_wordnet_synsets_by_context(
            word, contextual_meaning, pos
        )

        # 3ë‹¨ê³„: ì„ ë³„ëœ synsetì—ì„œë§Œ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ
        raw_synonyms = self.extract_synonyms_from_synsets(relevant_synsets, word)
        raw_antonyms = self.extract_antonyms_from_synsets(relevant_synsets, word)

        if self.verbose:
            print(f"   ğŸ” í›„ë³´ ë™ì˜ì–´: {raw_synonyms}")
            print(f"   ğŸ” í›„ë³´ ë°˜ì˜ì–´: {raw_antonyms}")

        # 4ë‹¨ê³„: ë¬¸ë§¥ ê¸°ë°˜ ê²€ì¦ (ë™ì˜ì–´ë§Œ, ë°˜ì˜ì–´ëŠ” ì´ë¯¸ ì •ë°€í•¨)
        if raw_synonyms:
            validated_synonyms = self.validate_synonyms_with_context(
                word, raw_synonyms, context, contextual_meaning
            )
        else:
            validated_synonyms = []

        # 5ë‹¨ê³„: ì–´ê·¼ ì¤‘ë³µ ì œê±°
        final_synonyms = self.remove_root_duplicates(validated_synonyms, word)
        final_antonyms = self.remove_root_duplicates(
            raw_antonyms[:2], word
        )  # ë°˜ì˜ì–´ëŠ” ìµœëŒ€ 2ê°œ

        return {
            "synonyms": final_synonyms,
            "antonyms": final_antonyms,
            "contextual_meaning": contextual_meaning,
            "semantic_field": meaning_analysis.get("semantic_field", ""),
            "meaning_certainty": meaning_analysis.get("meaning_certainty", 0.5),
            "method": "contextual_precise",
        }

    def remove_root_duplicates(self, words: List[str], original_word: str) -> List[str]:
        """ì–´ê·¼ ì¤‘ë³µ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)"""
        if not words:
            return words

        # ë¶ˆê·œì¹™ ì–´ê·¼ ë§¤í•‘
        irregular_roots = {
            "better": "good",
            "best": "good",
            "worse": "bad",
            "worst": "bad",
            "more": "much",
            "most": "much",
            "less": "little",
            "least": "little",
            "further": "far",
            "furthest": "far",
            "older": "old",
            "oldest": "old",
        }

        def get_root(word):
            word_lower = word.lower()
            if word_lower in irregular_roots:
                return irregular_roots[word_lower]

            # ê°„ë‹¨í•œ ì ‘ë¯¸ì‚¬ ì œê±°
            suffixes = ["ing", "ed", "er", "est", "ly", "ness", "ment", "tion"]
            for suffix in suffixes:
                if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 2:
                    return word_lower[: -len(suffix)]
            return word_lower

        filtered = []
        seen_roots = set()
        original_root = get_root(original_word)

        for word in words:
            word_root = get_root(word)

            # ì›ë³¸ê³¼ ê°™ì€ ì–´ê·¼ì´ê±°ë‚˜ ì´ë¯¸ ë³¸ ì–´ê·¼ì´ë©´ ì œì™¸
            if word_root == original_root or word_root in seen_roots:
                continue

            seen_roots.add(word_root)
            filtered.append(word)

        return filtered

    def integrate_contextual_extractor(self):
        """ê¸°ì¡´ í´ë˜ìŠ¤ì— ë¬¸ë§¥ ê¸°ë°˜ ì¶”ì¶œê¸° í†µí•©"""
        # ğŸ”¥ ìˆ˜ì •: clientê°€ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ì‹¤í–‰
        if hasattr(self, "client"):
            self.contextual_extractor = ContextualSynonymExtractor(
                client=self.client, verbose=self.verbose
            )
            print("ğŸ¯ ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ì¶”ì¶œê¸° í™œì„±í™”")
        else:
            print("âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")


class ImprovedSynonymRefiner:
    """ê°œì„ ëœ ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ì œê¸° (VOCABULARY.XLSX êµ¬ì¡° ì¶œë ¥)"""

    def __init__(
        self,
        vocab_file: str,
        api_key: str = None,
        verbose: bool = False,
        enable_ai_base_form: bool = True,
    ):

        self.vocab_file = vocab_file
        self.verbose = verbose
        self.enable_ai_base_form = enable_ai_base_form

        # ğŸ”¥ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ë¨¼ì € ì´ˆê¸°í™”
        if api_key:
            openai.api_key = api_key
        elif os.getenv("OPENAI_API_KEY"):
            openai.api_key = os.getenv("OPENAI_API_KEY")
        else:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        self.client = openai.OpenAI()  # í•œ ë²ˆë§Œ ì´ˆê¸°í™”

        # ğŸ”¥ í’ˆì‚¬ ë§¤í•‘ ì¶”ê°€
        self.pos_mapping = {
            "noun": "n",
            "n": "n",
            "NN": "n",
            "NNS": "n",
            "NNP": "n",
            "NNPS": "n",
            "verb": "v",
            "v": "v",
            "VB": "v",
            "VBD": "v",
            "VBG": "v",
            "VBN": "v",
            "VBP": "v",
            "VBZ": "v",
            "adjective": "a",
            "adj": "a",
            "a": "a",
            "JJ": "a",
            "JJR": "a",
            "JJS": "a",
            "adverb": "r",
            "adv": "r",
            "r": "r",
            "RB": "r",
            "RBR": "r",
            "RBS": "r",
        }

        # ë°ì´í„° ë¡œë“œ
        self.load_vocabulary()

        # ë‹¨ì–´ì¥ ìˆ˜ì¤€ ë¶„ì„
        self.vocabulary_level = self.analyze_vocabulary_level()

        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.gpt_calls = 0
        self.api_calls = {"datamuse": 0, "wordsapi": 0, "merriam": 0}
        self.total_tokens = 0
        self.cost_estimate = 0.0
        self.word_root_cache = {}

        # ìºì‹œ
        self.cache = {}
        self.api_cache = {}
        self.base_form_cache = {}
        self.load_cache()

        print("âœ… ê°œì„ ëœ ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ì œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print("ğŸ“š ë‹¨ì–´ì¥: {}ê°œ í•­ëª©".format(len(self.vocab_df)))
        print("ğŸ¯ ê°ì§€ëœ ìˆ˜ì¤€: {}".format(self.vocabulary_level))
        print("ğŸ”„ ì‚¬ìš© API: Datamuse, WordsAPI, Merriam-Webster")
        if self.enable_ai_base_form:
            print("ğŸ¤– AI ì›í˜• íŒë³„: í™œì„±í™”")
        else:
            print("ğŸ“ ì›í˜• íŒë³„: ê¸°ì¡´ íŒ¨í„´ ë°©ì‹")

    def _normalize_pos(self, pos: str) -> str:
        """í’ˆì‚¬ë¥¼ WordNet í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
        if not pos:
            return ""
        pos_clean = pos.strip().lower()
        return self.pos_mapping.get(pos_clean, pos_clean)

    def _verify_pos_with_wordnet(self, word: str, target_pos: str) -> bool:
        """WordNetìœ¼ë¡œ ë‹¨ì–´ì˜ í’ˆì‚¬ ê²€ì¦"""
        try:
            target_pos_norm = self._normalize_pos(target_pos)
            if not target_pos_norm:
                return True  # í’ˆì‚¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ í†µê³¼

            synsets = wordnet.synsets(word, pos=target_pos_norm)
            return len(synsets) > 0
        except:
            return True  # ì˜¤ë¥˜ ì‹œ í†µê³¼

    def _filter_candidates_by_pos(
        self, candidates: List[str], target_pos: str
    ) -> List[str]:
        """í’ˆì‚¬ê°€ ì¼ì¹˜í•˜ëŠ” í›„ë³´ë“¤ë§Œ í•„í„°ë§"""
        if not candidates or not target_pos:
            return candidates

        filtered = []
        for candidate in candidates:
            if self._verify_pos_with_wordnet(candidate, target_pos):
                filtered.append(candidate)
            elif self.verbose:
                print(f"   âš ï¸ í’ˆì‚¬ ë¶ˆì¼ì¹˜ë¡œ ì œì™¸: {candidate} (ëª©í‘œ: {target_pos})")

        return filtered

    def load_vocabulary(self):
        """ë‹¨ì–´ì¥ ë¡œë“œ - VOCABULARY.XLSX êµ¬ì¡° ìœ ì§€"""
        try:
            if self.vocab_file.endswith(".xlsx"):
                self.vocab_df = pd.read_excel(self.vocab_file, engine="openpyxl")
            else:
                self.vocab_df = pd.read_csv(self.vocab_file, encoding="utf-8")

            # VOCABULARY.XLSXì˜ í•„ìˆ˜ ì»¬ëŸ¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
            expected_columns = [
                "êµì¬ID",
                "êµì¬ëª…",
                "ì§€ë¬¸ID",
                "ìˆœì„œ",
                "ì§€ë¬¸",
                "ë‹¨ì–´",
                "ì›í˜•",
                "í’ˆì‚¬",
                "ëœ»(í•œê¸€)",
                "ëœ»(ì˜ì–´)",
                "ë™ì˜ì–´",
                "ë°˜ì˜ì–´",
                "ë¬¸ë§¥",
                "ë¶„ë¦¬í˜•ì—¬ë¶€",
                "ì‹ ë¢°ë„",
                "ì‚¬ìš©ìDBë§¤ì¹­",
                "ë§¤ì¹­ë°©ì‹",
                "íŒ¨í„´ì •ë³´",
                "ë¬¸ë§¥ì ì˜ë¯¸",
                "ë™ì˜ì–´ì‹ ë¢°ë„",
                "ì²˜ë¦¬ë°©ì‹",
                "í¬í•¨ì´ìœ ",
            ]

            missing_columns = [
                col for col in expected_columns if col not in self.vocab_df.columns
            ]
            if missing_columns:
                print(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤: {missing_columns}")
                print("ğŸ’¡ ê¸°ë³¸ê°’ìœ¼ë¡œ ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.")

                # ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶”ê°€
                for col in missing_columns:
                    if col in ["êµì¬ID"]:
                        self.vocab_df[col] = 1
                    elif col in ["ìˆœì„œ"]:
                        self.vocab_df[col] = range(1, len(self.vocab_df) + 1)
                    elif col in ["ë¶„ë¦¬í˜•ì—¬ë¶€", "ì‚¬ìš©ìDBë§¤ì¹­"]:
                        self.vocab_df[col] = False
                    elif col in ["ì‹ ë¢°ë„", "ë™ì˜ì–´ì‹ ë¢°ë„"]:
                        self.vocab_df[col] = 0.8
                    else:
                        self.vocab_df[col] = ""

            # ë™ì˜ì–´/ë°˜ì˜ì–´ê°€ ìˆëŠ” í•­ëª©ë§Œ í•„í„°ë§ (ê°œì„ í•  ëŒ€ìƒ)
            mask = (self.vocab_df.get("ë™ì˜ì–´", "").fillna("").str.strip() != "") | (
                self.vocab_df.get("ë°˜ì˜ì–´", "").fillna("").str.strip() != ""
            )

            if mask.sum() == 0:
                print("âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ê°€ ìˆëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í•­ëª©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                self.process_df = self.vocab_df.copy()
            else:
                self.process_df = self.vocab_df[mask].reset_index(drop=True)
                print("ğŸ“– ë™ì˜ì–´/ë°˜ì˜ì–´ê°€ ìˆëŠ” í•­ëª©: {}ê°œ".format(len(self.process_df)))

        except Exception as e:
            raise Exception("ë‹¨ì–´ì¥ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))

    def analyze_vocabulary_level(self) -> str:
        """ë‹¨ì–´ì¥ ì „ì²´ ìˆ˜ì¤€ ë¶„ì„"""
        if len(self.vocab_df) == 0:
            return "intermediate"

        words = self.vocab_df.get("ë‹¨ì–´", "").fillna("").str.strip()
        word_lengths = [len(w) for w in words if w]
        avg_length = np.mean(word_lengths) if word_lengths else 5

        complex_patterns = 0
        total_words = 0

        for word in words:
            if not word or len(word) < 2:
                continue
            total_words += 1

            if (
                word.endswith(
                    ("tion", "sion", "ment", "ness", "ity", "ism", "ology", "graphy")
                )
                or word.startswith(
                    ("pre", "post", "anti", "auto", "inter", "trans", "sub", "super")
                )
                or len(word) > 8
            ):
                complex_patterns += 1

        complexity_ratio = complex_patterns / total_words if total_words > 0 else 0

        if avg_length > 7 and complexity_ratio > 0.3:
            return "advanced"
        elif avg_length > 5.5 and complexity_ratio > 0.15:
            return "intermediate"
        else:
            return "elementary"

    def estimate_word_difficulty(self, word: str) -> str:
        """ê°œë³„ ë‹¨ì–´ ë‚œì´ë„ ì¶”ì •"""
        if not word:
            return "elementary"

        length = len(word)
        score = 0

        if length <= 4:
            score += 1
        elif length <= 6:
            score += 2
        elif length <= 8:
            score += 3
        else:
            score += 4

        if re.search(r"(tion|sion|ment|ness|ity|ism|ology|graphy)$", word):
            score += 2
        if re.search(r"^(pre|post|anti|auto|inter|trans|sub|super)", word):
            score += 1
        if re.search(r"(duct|struct|spect|ject|port|form)", word):
            score += 1

        if score <= 2:
            return "elementary"
        elif score <= 4:
            return "intermediate"
        else:
            return "advanced"

    def check_base_form_batch(self, words: List[str]) -> Dict[str, bool]:
        """AIë¥¼ ì‚¬ìš©í•´ ì—¬ëŸ¬ ë‹¨ì–´ì˜ ì›í˜• ì—¬ë¶€ë¥¼ í•œ ë²ˆì— í™•ì¸"""
        if not words or not self.enable_ai_base_form:
            return {word: True for word in words}

        uncached_words = []
        results = {}

        for word in words:
            if word.lower() in self.base_form_cache:
                results[word] = self.base_form_cache[word.lower()]
            else:
                uncached_words.append(word)

        if not uncached_words:
            return results

        # ë°°ì¹˜ë¡œ AIì—ê²Œ ì§ˆì˜ (ìµœëŒ€ 20ê°œì”©)
        batch_size = 20
        for i in range(0, len(uncached_words), batch_size):
            batch = uncached_words[i : i + batch_size]
            batch_results = self._ai_check_base_forms(batch)
            results.update(batch_results)

            # ìºì‹œì— ì €ì¥
            for word, is_base in batch_results.items():
                self.base_form_cache[word.lower()] = is_base

        return results

    def _ai_check_base_forms(self, words: List[str]) -> Dict[str, bool]:
        """AIë¡œ ë‹¨ì–´ë“¤ì˜ ì›í˜• ì—¬ë¶€ í™•ì¸"""
        words_str = ", ".join(words)
        # ğŸ”¥ í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë‹¨ì–´ì˜ í’ˆì‚¬ ì •ë³´ í¬í•¨
        current_pos = getattr(self, "_current_pos", "")
        pos_info = f" (ëª©í‘œ í’ˆì‚¬: {current_pos})" if current_pos else ""

        prompt = f"""
Please analyze the following English words and determine if each word is in its BASE FORM{pos_info}.

Words to analyze: {words_str}

STRICT Rules - Mark as FALSE if:
- Plurals (cats, dogs, children, mice)
- Past tense (walked, ran, went, was) 
- Gerunds/Present participles (running, walking, being, going)
- Past participles (broken, written, done)
- Comparatives/Superlatives (better, best, worse, worst, more, most)
- Any inflected form
{f"- Words that are not {current_pos}" if current_pos else ""}

Mark as TRUE only if:
- The exact dictionary base form{f" that is {current_pos}" if current_pos else ""}
- Root form you would look up in a dictionary

Respond in JSON format only:
{{
    "word1": true/false,
    "word2": true/false,
    ...
}}
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert English linguist who determines if words are in their base/root form.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=300,
                temperature=0.1,
            )

            self.gpt_calls += 1
            if hasattr(response, "usage"):
                usage = response.usage
                self.total_tokens += usage.total_tokens
                cost = (
                    usage.prompt_tokens * 0.15 + usage.completion_tokens * 0.60
                ) / 1000000
                self.cost_estimate += cost

            content = response.choices[0].message.content.strip()

            try:
                if "```json" in content:
                    start = content.find("```json") + 7
                    end = content.find("```", start)
                    if end != -1:
                        content = content[start:end].strip()

                result = json.loads(content)

                validated_result = {}
                for word in words:
                    if word in result:
                        validated_result[word] = bool(result[word])
                    else:
                        validated_result[word] = False
                        if self.verbose:
                            print(f"âš ï¸ AI ì‘ë‹µì— '{word}' ì—†ìŒ - Falseë¡œ ì„¤ì •")

                if self.verbose:
                    base_count = sum(validated_result.values())
                    print(f"ğŸ¤– AI ì›í˜• íŒë³„: {len(words)}ê°œ ì¤‘ {base_count}ê°œê°€ ì›í˜•")

                return validated_result

            except json.JSONDecodeError as e:
                if self.verbose:
                    print(f"âŒ AI ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return {word: False for word in words}

        except Exception as e:
            if self.verbose:
                print(f"âŒ AI ì›í˜• íŒë³„ ì‹¤íŒ¨: {e}")
            return {word: False for word in words}

    def get_datamuse_synonyms_antonyms(self, word: str) -> Tuple[List[str], List[str]]:
        """Datamuse APIì—ì„œ ë™ì˜ì–´/ë°˜ì˜ì–´ ê°€ì ¸ì˜¤ê¸°"""
        cache_key = f"datamuse_{word.lower()}"
        if cache_key in self.api_cache:
            cached = self.api_cache[cache_key]
            return cached.get("synonyms", []), cached.get("antonyms", [])

        synonyms, antonyms = [], []

        try:
            # ë™ì˜ì–´ ìš”ì²­
            syn_data = self._safe_api_call(
                "https://api.datamuse.com/words",
                params={"rel_syn": word, "max": 20},
                timeout=15,
                max_retries=2,
            )
            if syn_data and isinstance(syn_data, list):
                synonyms = [
                    item.get("word", "")
                    for item in syn_data
                    if isinstance(item, dict) and "word" in item
                ]
                synonyms = [w for w in synonyms if w and isinstance(w, str)]

            # ë°˜ì˜ì–´ ìš”ì²­
            ant_data = self._safe_api_call(
                "https://api.datamuse.com/words",
                params={"rel_ant": word, "max": 20},
                timeout=15,
                max_retries=2,
            )
            if ant_data and isinstance(ant_data, list):
                antonyms = [
                    item.get("word", "")
                    for item in ant_data
                    if isinstance(item, dict) and "word" in item
                ]
                antonyms = [w for w in antonyms if w and isinstance(w, str)]

            self.api_calls["datamuse"] += 1
            self.api_cache[cache_key] = {"synonyms": synonyms, "antonyms": antonyms}

            if self.verbose:
                print(
                    f"ğŸ“Š Datamuse: {word} â†’ ë™ì˜ì–´ {len(synonyms)}ê°œ, ë°˜ì˜ì–´ {len(antonyms)}ê°œ"
                )

        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ Datamuse API ì˜¤ë¥˜ ({word}): {e}")

        return synonyms or [], antonyms or []

    def get_wordsapi_synonyms_antonyms(self, word: str) -> Tuple[List[str], List[str]]:
        """WordsAPIì—ì„œ ë™ì˜ì–´/ë°˜ì˜ì–´ ê°€ì ¸ì˜¤ê¸°"""
        cache_key = f"wordsapi_{word.lower()}"
        if cache_key in self.api_cache:
            cached = self.api_cache[cache_key]
            return cached.get("synonyms", []), cached.get("antonyms", [])

        synonyms, antonyms = [], []
        rapidapi_key = os.getenv("RAPIDAPI_KEY")

        if not rapidapi_key:
            if self.verbose:
                print("âš ï¸ WordsAPI: RAPIDAPI_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ê±´ë„ˆëœ€")
            return [], []

        try:
            headers = {
                "X-RapidAPI-Key": rapidapi_key,
                "X-RapidAPI-Host": "wordsapiv1.p.rapidapi.com",
            }

            # ë™ì˜ì–´ ìš”ì²­
            syn_data = self._safe_api_call(
                f"https://wordsapiv1.p.rapidapi.com/words/{word}/synonyms",
                headers=headers,
            )
            if syn_data and isinstance(syn_data, dict) and "synonyms" in syn_data:
                synonyms = syn_data["synonyms"]
                if synonyms and isinstance(synonyms, list):
                    synonyms = [w for w in synonyms if w and isinstance(w, str)]
                else:
                    synonyms = []

            # ë°˜ì˜ì–´ ìš”ì²­
            ant_data = self._safe_api_call(
                f"https://wordsapiv1.p.rapidapi.com/words/{word}/antonyms",
                headers=headers,
            )
            if ant_data and isinstance(ant_data, dict) and "antonyms" in ant_data:
                antonyms = ant_data["antonyms"]
                if antonyms and isinstance(antonyms, list):
                    antonyms = [w for w in antonyms if w and isinstance(w, str)]
                else:
                    antonyms = []

            self.api_calls["wordsapi"] += 1
            self.api_cache[cache_key] = {"synonyms": synonyms, "antonyms": antonyms}

            if self.verbose:
                print(
                    f"ğŸ“Š WordsAPI: {word} â†’ ë™ì˜ì–´ {len(synonyms)}ê°œ, ë°˜ì˜ì–´ {len(antonyms)}ê°œ"
                )

        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ WordsAPI ì˜¤ë¥˜ ({word}): {e}")

        return synonyms or [], antonyms or []

    def get_merriam_webster_synonyms_antonyms(
        self, word: str
    ) -> Tuple[List[str], List[str]]:
        """Merriam-Webster Thesaurus APIì—ì„œ ë™ì˜ì–´/ë°˜ì˜ì–´ ê°€ì ¸ì˜¤ê¸°"""
        cache_key = "merriam_{}".format(word.lower())
        if cache_key in self.api_cache:
            return (
                self.api_cache[cache_key]["synonyms"],
                self.api_cache[cache_key]["antonyms"],
            )

        synonyms = []
        antonyms = []

        mw_key = os.getenv("MERRIAM_WEBSTER_KEY")
        if not mw_key:
            if self.verbose:
                print(
                    "âš ï¸ Merriam-Webster: MERRIAM_WEBSTER_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ê±´ë„ˆëœ€"
                )
            return synonyms, antonyms

        try:
            url = "https://www.dictionaryapi.com/api/v3/references/thesaurus/json/{}".format(
                word
            )
            params = {"key": mw_key}

            response = requests.get(url, params=params, timeout=5)
            if response.status_code == 200:
                data = response.json()

                if (
                    isinstance(data, list)
                    and len(data) > 0
                    and isinstance(data[0], dict)
                ):
                    entry = data[0]

                    # ë™ì˜ì–´ ì¶”ì¶œ
                    if "meta" in entry and "syns" in entry["meta"]:
                        syns_lists = entry["meta"]["syns"]
                        for syn_list in syns_lists:
                            synonyms.extend(syn_list)

                    # ë°˜ì˜ì–´ ì¶”ì¶œ
                    if "meta" in entry and "ants" in entry["meta"]:
                        ants_lists = entry["meta"]["ants"]
                        for ant_list in ants_lists:
                            antonyms.extend(ant_list)

            self.api_calls["merriam"] += 1
            self.api_cache[cache_key] = {"synonyms": synonyms, "antonyms": antonyms}

            if self.verbose:
                print(
                    "ğŸ“Š Merriam-Webster: {} â†’ ë™ì˜ì–´ {}ê°œ, ë°˜ì˜ì–´ {}ê°œ".format(
                        word, len(synonyms), len(antonyms)
                    )
                )

        except Exception as e:
            if self.verbose:
                print("âš ï¸ Merriam-Webster API ì˜¤ë¥˜ ({}): {}".format(word, e))

        return synonyms, antonyms

    def collect_api_candidates(
        self, word: str, pos: str = ""
    ) -> Tuple[List[str], List[str]]:
        """ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ í›„ë³´ ìˆ˜ì§‘ (ê¸°ì¡´ ë©”ì„œë“œ ëŒ€ì²´)"""
        if not word or not isinstance(word, str):
            return [], []

        if self.verbose:
            print(f"ğŸ” {word} ì •ë°€ í›„ë³´ ìˆ˜ì§‘ ì‹œì‘...")

        try:
            # 1ë‹¨ê³„: ê¸°ì¡´ API ê²°ê³¼ ìˆ˜ì§‘ (ë¹ ë¥¸ í›„ë³´ í™•ë³´)
            datamuse_syns, datamuse_ants = self.get_datamuse_synonyms_antonyms(word)
            wordsapi_syns, wordsapi_ants = self.get_wordsapi_synonyms_antonyms(word)
            merriam_syns, merriam_ants = self.get_merriam_webster_synonyms_antonyms(
                word
            )

            # 2ë‹¨ê³„: ê¸°ë³¸ í•„í„°ë§
            api_synonyms = self.basic_filter_candidates_for_synonyms(
                datamuse_syns + wordsapi_syns + merriam_syns, word
            )
            api_antonyms = self.basic_filter_candidates_for_synonyms(
                datamuse_ants + wordsapi_ants + merriam_ants, word
            )

            # 3ë‹¨ê³„: ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ì¶”ì¶œ (ë©”ì¸ ë¡œì§)
            if hasattr(self, "contextual_extractor") and hasattr(
                self, "_current_context"
            ):
                context = getattr(self, "_current_context", "")
                if context:
                    contextual_result = (
                        self.contextual_extractor.extract_contextual_synonyms_antonyms(
                            word, context, pos
                        )
                    )

                    # ë¬¸ë§¥ ê¸°ë°˜ ê²°ê³¼ ìš°ì„  ì‚¬ìš©
                    final_synonyms = contextual_result["synonyms"]
                    final_antonyms = contextual_result["antonyms"]

                    # API ê²°ê³¼ì™€ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                    combined_synonyms = list(set(final_synonyms + api_synonyms[:2]))[:3]
                    combined_antonyms = list(set(final_antonyms + api_antonyms[:1]))[:2]

                    if self.verbose:
                        print(
                            f"ğŸ¯ ë¬¸ë§¥ ê¸°ë°˜ ê²°ê³¼: ë™ì˜ì–´ {len(final_synonyms)}ê°œ, ë°˜ì˜ì–´ {len(final_antonyms)}ê°œ"
                        )
                        print(
                            f"ğŸ”— API ë³‘í•© í›„: ë™ì˜ì–´ {len(combined_synonyms)}ê°œ, ë°˜ì˜ì–´ {len(combined_antonyms)}ê°œ"
                        )

                    return combined_synonyms, combined_antonyms

            # 4ë‹¨ê³„: ë¬¸ë§¥ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ (ì•ˆì „ì¥ì¹˜)
            if self.verbose:
                print("âš ï¸ ë¬¸ë§¥ ì •ë³´ ì—†ìŒ, ê¸°ì¡´ API ë°©ì‹ ì‚¬ìš©")

            # AI ì›í˜• íŒë³„
            if self.enable_ai_base_form and (api_synonyms or api_antonyms):
                all_candidates = list(set(api_synonyms + api_antonyms))
                base_form_results = self.check_base_form_batch(all_candidates)

                api_synonyms = [
                    w for w in api_synonyms if base_form_results.get(w, False)
                ]
                api_antonyms = [
                    w for w in api_antonyms if base_form_results.get(w, False)
                ]

            # ì–´ê·¼ ì¤‘ë³µ ì œê±° ë° ê°œìˆ˜ ì œí•œ
            final_synonyms = self.enhanced_filter_synonyms_antonyms(
                api_synonyms, word, max_count=3
            )
            final_antonyms = self.enhanced_filter_synonyms_antonyms(
                api_antonyms, word, max_count=2
            )

            return final_synonyms, final_antonyms

        except Exception as e:
            if self.verbose:
                print(f"âŒ ì •ë°€ í›„ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({word}): {e}")
            return [], []

    def is_high_quality_candidate(self, candidate: str, original_word: str) -> bool:
        """ê³ í’ˆì§ˆ í›„ë³´ì¸ì§€ í™•ì¸"""
        # ê¸¸ì´ê°€ ì ë‹¹í•´ì•¼ í•¨ (3-10ê¸€ì)
        if len(candidate) < 3 or len(candidate) > 10:
            return False

        # ì¼ë°˜ì ì¸ ì˜ì–´ ë‹¨ì–´ íŒ¨í„´
        if not re.match(r"^[a-zA-Z]+$", candidate):
            return False

        # ì›ë³¸ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ì–´ì—¬ì•¼ í•¨
        if (
            candidate.lower() in original_word.lower()
            or original_word.lower() in candidate.lower()
        ):
            return False

        # ë§¤ìš° ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤ (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸)
        common_words = {
            "big",
            "small",
            "large",
            "tiny",
            "huge",
            "little",
            "good",
            "bad",
            "great",
            "poor",
            "fine",
            "nice",
            "awful",
            "happy",
            "sad",
            "glad",
            "upset",
            "joyful",
            "fast",
            "slow",
            "quick",
            "rapid",
            "hot",
            "cold",
            "warm",
            "cool",
            "easy",
            "hard",
            "difficult",
            "simple",
            "new",
            "old",
            "fresh",
            "recent",
            "important",
            "crucial",
            "vital",
            "significant",
            "beautiful",
            "pretty",
            "lovely",
            "ugly",
            "strong",
            "weak",
            "powerful",
            "rich",
            "poor",
            "wealthy",
            "clean",
            "dirty",
            "messy",
            "loud",
            "quiet",
            "noisy",
            "silent",
        }

        return candidate.lower() in common_words

    def filter_candidates(self, candidates: List[str], original_word: str) -> List[str]:
        """í›„ë³´ ë‹¨ì–´ë“¤ ì—„ê²© í•„í„°ë§"""
        if not candidates or candidates is None:
            return []

        filtered = []
        original_lower = original_word.lower() if original_word else ""

        for candidate in candidates:
            if not candidate or not isinstance(candidate, str):
                continue

            candidate = candidate.strip()
            candidate_lower = candidate.lower()

            # 1. í•œ ë‹¨ì–´ë§Œ í—ˆìš© (ê³µë°±, í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ í¬í•¨ëœ ê²ƒ ì œì™¸)
            if " " in candidate or "-" in candidate or "_" in candidate:
                continue

            # 2. ìê¸° ìì‹  ì œì™¸
            if candidate_lower == original_lower:
                continue

            # 3. ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸ (2ê¸€ì ì´í•˜)
            if len(candidate) <= 2:
                continue

            # 4. ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ì–´ ì œì™¸
            if any(char.isdigit() for char in candidate):
                continue

            # 5. íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ë‹¨ì–´ ì œì™¸ (ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ì œì™¸)
            if re.search(r"[^\w\']", candidate):
                continue

            # 6. ëª¨ë‘ ëŒ€ë¬¸ìì¸ ë‹¨ì–´ ì œì™¸ (ì•½ì–´ì¼ ê°€ëŠ¥ì„±)
            if candidate.isupper() and len(candidate) > 1:
                continue

            # 7. ì›í˜•ê³¼ ë„ˆë¬´ ìœ ì‚¬í•œ ë‹¨ì–´ ì œì™¸
            if self.is_too_similar(candidate_lower, original_lower):
                continue

            # 8. ì¼ë°˜ì ì´ì§€ ì•Šì€ ë‹¨ì–´ ì œì™¸ (ë§¤ìš° ê¸´ ë‹¨ì–´)
            if len(candidate) > 12:
                continue

            filtered.append(candidate)

        if filtered:
            # í•©ì„±ì–´ ì œê±°
            filtered = [w for w in filtered if not self.is_compound_word(w)]
            # ì–´ê·¼ ì¤‘ë³µ ì œê±°
            filtered = self.remove_same_root_duplicates(filtered, original_word)

        return filtered or []

    def is_too_similar(self, word1: str, word2: str) -> bool:
        """ë‘ ë‹¨ì–´ê°€ ë„ˆë¬´ ìœ ì‚¬í•œì§€ í™•ì¸ (ë‹¨ìˆœ edit distance)"""
        if abs(len(word1) - len(word2)) > 3:
            return False

        # ê°„ë‹¨í•œ edit distance (Levenshtein distance)
        m, n = len(word1), len(word2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j

        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if word1[i - 1] == word2[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1]
                else:
                    dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])

        edit_distance = dp[m][n]
        max_len = max(len(word1), len(word2))

        # í¸ì§‘ ê±°ë¦¬ê°€ ë‹¨ì–´ ê¸¸ì´ì˜ 30% ì´í•˜ë©´ ë„ˆë¬´ ìœ ì‚¬
        return edit_distance / max_len < 0.3

    def is_compound_word(self, word: str) -> bool:
        """2ê°œ ì´ìƒ í•©ì„±ì–´ì¸ì§€ í™•ì¸"""
        if not word or len(word) < 4:
            return False

        # ëª…í™•í•œ í•©ì„±ì–´ íŒ¨í„´
        compound_patterns = [
            r"\w+\s+\w+",  # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ í•©ì„±ì–´
            r"\w+-\w+",  # í•˜ì´í”ˆìœ¼ë¡œ ì—°ê²°ëœ í•©ì„±ì–´
            r"\w+_\w+",  # ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ì—°ê²°ëœ í•©ì„±ì–´
        ]

        for pattern in compound_patterns:
            if re.search(pattern, word):
                return True

        return False

    def extract_word_root(self, word: str) -> str:
        """ë‹¨ì–´ì˜ ì–´ê·¼ ì¶”ì¶œ"""
        if word.lower() in self.word_root_cache:
            return self.word_root_cache[word.lower()]

        original_word = word.lower()

        # ì¼ë°˜ì ì¸ ì ‘ë¯¸ì‚¬ ì œê±° (ìˆœì„œ ì¤‘ìš” - ê¸´ ê²ƒë¶€í„°)
        suffixes_to_remove = [
            "ness",
            "ment",
            "tion",
            "sion",
            "ible",
            "able",
            "ical",
            "ful",
            "less",
            "ish",
            "ous",
            "ive",
            "ing",
            "ed",
            "er",
            "est",
            "ly",
            "al",
            "ic",
            "y",
        ]

        # ë¶ˆê·œì¹™ ì–´ê·¼ ë§¤í•‘
        irregular_roots = {
            "better": "good",
            "worse": "bad",
            "best": "good",
            "worst": "bad",
            "more": "much",
            "most": "much",
            "less": "little",
            "least": "little",
            "further": "far",
            "furthest": "far",
            "older": "old",
            "oldest": "old",
        }

        if original_word in irregular_roots:
            root = irregular_roots[original_word]
        else:
            root = original_word
            for suffix in suffixes_to_remove:
                if root.endswith(suffix) and len(root) > len(suffix) + 2:
                    potential_root = root[: -len(suffix)]
                    if len(potential_root) >= 3:
                        root = potential_root
                        break

        self.word_root_cache[word.lower()] = root
        return root

    def have_same_root(self, word1: str, word2: str) -> bool:
        """ë‘ ë‹¨ì–´ê°€ ê°™ì€ ì–´ê·¼ì„ ê°€ì§€ëŠ”ì§€ í™•ì¸"""
        root1 = self.extract_word_root(word1)
        root2 = self.extract_word_root(word2)

        if root1 == root2:
            return True

        if len(root1) >= 4 and len(root2) >= 4:
            if root1 in root2 or root2 in root1:
                return True

        return False

    def remove_same_root_duplicates(
        self, words: List[str], original_word: str
    ) -> List[str]:
        """ì–´ê·¼ì´ ê°™ì€ ë‹¨ì–´ë“¤ ì¤‘ë³µ ì œê±°"""
        if not words:
            return words

        filtered = []
        seen_roots = set()
        original_root = self.extract_word_root(original_word)

        for word in words:
            if not word:
                continue

            word_root = self.extract_word_root(word)

            # ì›ë³¸ ë‹¨ì–´ì™€ ê°™ì€ ì–´ê·¼ì´ë©´ ì œì™¸
            if word_root == original_root:
                continue

            # ì´ë¯¸ ê°™ì€ ì–´ê·¼ì˜ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì œì™¸
            if word_root in seen_roots:
                continue

            seen_roots.add(word_root)
            filtered.append(word)

        return filtered

    def _safe_api_call(
        self,
        url: str,
        params: Dict = None,
        headers: Dict = None,
        timeout: int = 10,
        max_retries: int = 3,
    ) -> Optional[List]:
        """ì•ˆì „í•œ API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)"""

        for attempt in range(max_retries):
            try:
                current_timeout = timeout + (attempt * 5)
                response = requests.get(
                    url, params=params, headers=headers, timeout=current_timeout
                )
                response.raise_for_status()

                data = response.json()

                if data is None:
                    return []

                if isinstance(data, list):
                    return data
                elif isinstance(data, dict):
                    return data  # dict ê·¸ëŒ€ë¡œ ë°˜í™˜
                else:
                    if self.verbose:
                        print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ íƒ€ì…: {type(data)}")
                    return []

            except requests.exceptions.Timeout as e:
                if self.verbose:
                    print(
                        f"â° API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ ({url}), ì‹œë„ {attempt + 1}/{max_retries}: {e}"
                    )
                if attempt < max_retries - 1:
                    wait_time = 2**attempt
                    if self.verbose:
                        print(f"ğŸ”„ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    if self.verbose:
                        print(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼, API í˜¸ì¶œ í¬ê¸°: {url}")
                    return []

            except requests.exceptions.RequestException as e:
                if self.verbose:
                    print(
                        f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨ ({url}), ì‹œë„ {attempt + 1}/{max_retries}: {e}"
                    )
                if attempt < max_retries - 1:
                    wait_time = 2**attempt
                    if self.verbose:
                        print(f"ğŸ”„ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    return []

            except json.JSONDecodeError as e:
                if self.verbose:
                    print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ ({url}): {e}")
                return []

            except Exception as e:
                if self.verbose:
                    print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ({url}): {e}")
                return []

        return []

    def safe_gpt_call(
        self, messages: List[Dict], max_tokens: int = 400, temperature: float = 0.1
    ) -> Tuple[Optional[str], Optional[str]]:
        """ì•ˆì „í•œ GPT API í˜¸ì¶œ"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            self.gpt_calls += 1
            if hasattr(response, "usage"):
                usage = response.usage
                self.total_tokens += usage.total_tokens
                cost = (
                    usage.prompt_tokens * 0.15 + usage.completion_tokens * 0.60
                ) / 1000000
                self.cost_estimate += cost

            return response.choices[0].message.content.strip(), None

        except Exception as e:
            return None, str(e)

    def parse_gpt_json(self, content: str) -> Dict:
        """GPT ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±"""
        try:
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            result = json.loads(content)
            return result if isinstance(result, dict) else {}

        except json.JSONDecodeError:
            try:
                lines = content.split("\n")
                partial_result = {}
                for line in lines:
                    if ":" in line and not line.strip().startswith("//"):
                        key_part, value_part = line.split(":", 1)
                        key = key_part.strip().strip('"').strip("'")
                        value = value_part.strip().rstrip(",").strip('"').strip("'")

                        if key in ["final_synonyms", "final_antonyms"]:
                            if "[" in value and "]" in value:
                                try:
                                    import ast

                                    partial_result[key] = ast.literal_eval(value)
                                except:
                                    items = value.strip("[]").split(",")
                                    partial_result[key] = [
                                        item.strip().strip('"').strip("'")
                                        for item in items
                                        if item.strip()
                                    ]
                            else:
                                partial_result[key] = []
                        else:
                            partial_result[key] = value

                return partial_result
            except:
                return {}
        except Exception:
            return {}

    def gpt_validate_and_select(
        self,
        word: str,
        meaning: str,
        context: str,
        api_synonyms: List[str],
        api_antonyms: List[str],
    ) -> Dict:
        """GPTë¡œ ë¬¸ë§¥ ì í•©ì„± ê²€ì¦ ë° ìµœì¢… ì„ ë³„ (ìˆ˜ì •ëœ ë²„ì „)"""

        cache_key = "gpt_{}:{}:{}".format(word, context[:50], self.vocabulary_level)
        if cache_key in self.cache:
            return self.cache[cache_key]

        word_level = self.estimate_word_difficulty(word)

        # ğŸ”¥ ì´ë¯¸ í•„í„°ë§ëœ í›„ë³´ë“¤ (ìµœëŒ€ 3ê°œì”©)
        api_syns_str = ", ".join(api_synonyms) if api_synonyms else "None"
        api_ants_str = ", ".join(api_antonyms) if api_antonyms else "None"

        # ğŸ”¥ í’ˆì‚¬ ì •ë³´ ì¶”ì¶œ ë° ì •ê·œí™”
        pos = getattr(self, "_current_pos", "")  # ì²˜ë¦¬ ì¤‘ì¸ ë‹¨ì–´ì˜ í’ˆì‚¬
        pos_normalized = self._normalize_pos(pos)

        pos_description = {
            "n": "noun (ëª…ì‚¬)",
            "v": "verb (ë™ì‚¬)",
            "a": "adjective (í˜•ìš©ì‚¬)",
            "r": "adverb (ë¶€ì‚¬)",
        }.get(pos_normalized, f"{pos} (ê¸°íƒ€)")

        prompt = f"""
    You are an English education expert. Apply EXTREMELY STRICT criteria for synonym/antonym validation.

    **Analysis Target:**
    - Word: "{word}"
    - Part of Speech: {pos_description}
    - Meaning: {meaning}
    - Context: {context}
    - Synonym candidates: {api_syns_str}
    - Antonym candidates: {api_ants_str}
    - Vocabulary Level: {self.vocabulary_level}

    **ğŸ”¥ CRITICAL REQUIREMENTS:**
    1. **EXACT Part of Speech Match:**
    - ALL synonyms and antonyms must be {pos_description} ONLY
    - Do NOT include words from different parts of speech
    - Example: If "{word}" is {pos_description}, provide only {pos_description} synonyms/antonyms

    2. **Single Words ONLY:**
    - NO phrases or multi-word expressions
    - NO hyphenated words
    - NO compound expressions

    3. **Base Forms ONLY:**
    - Reject ALL inflected forms (plurals, past tense, gerunds, comparatives)
    - Accept only dictionary base forms that are {pos_description}

    4. **Perfect Context Match:**
    - Must fit the exact meaning and context
    - Must be appropriate for {self.vocabulary_level} level students

    5. **Conservative Selection:**
    - When in doubt, EXCLUDE rather than include
    - Maximum 2 synonyms, 1 antonym
    - Quality over quantity

    **Response in JSON format:**
    {{
        "final_synonyms": ["max 2 perfect single-word {pos_description} synonyms"],
        "final_antonyms": ["max 1 perfect single-word {pos_description} antonym"],
        "pos_verified": "{pos_normalized}",
        "rejected_synonyms": [
            {{"word": "word", "reason": "specific rejection reason including POS/form issues"}}
        ],
        "rejected_antonyms": [
            {{"word": "word", "reason": "specific rejection reason including POS/form issues"}}
        ],
        "contextual_confidence": 0.0-1.0,
        "reasoning": "detailed explanation including POS verification and selection criteria"
    }}
    """

        messages = [
            {
                "role": "system",
                "content": "You are an ultra-conservative English education expert. You reject most suggestions unless they are absolutely perfect matches. Many words should have NO synonyms or antonyms - this is normal and correct.",
            },
            {"role": "user", "content": prompt},
        ]

        # GPT í˜¸ì¶œ
        response_content, error = self.safe_gpt_call(
            messages, max_tokens=600, temperature=0.1
        )

        if response_content:
            result = self.parse_gpt_json(response_content)

            # ğŸ”¥ ì¶”ê°€ ì•ˆì „ì¥ì¹˜ - ê²°ê³¼ë¥¼ ë‹¤ì‹œ í•œë²ˆ í•„í„°ë§
            raw_synonyms = result.get("final_synonyms", [])
            raw_antonyms = result.get("final_antonyms", [])

            final_synonyms = self.enhanced_filter_synonyms_antonyms(
                raw_synonyms, word, max_count=2
            )
            final_antonyms = self.enhanced_filter_synonyms_antonyms(
                raw_antonyms, word, max_count=1
            )

            refined_result = {
                "api_synonyms": api_synonyms,
                "api_antonyms": api_antonyms,
                "contextual_meaning": result.get("contextual_meaning", ""),
                "part_of_speech": result.get("part_of_speech", ""),
                "final_synonyms": final_synonyms,  # ğŸ”¥ ìµœì¢… í•„í„°ë§ ì ìš©
                "final_antonyms": final_antonyms,  # ğŸ”¥ ìµœì¢… í•„í„°ë§ ì ìš©
                "rejected_synonyms": result.get("rejected_synonyms", []),
                "rejected_antonyms": result.get("rejected_antonyms", []),
                "api_confidence": (
                    "high"
                    if len(api_synonyms) >= 2 or len(api_antonyms) >= 2
                    else "medium" if api_synonyms or api_antonyms else "low"
                ),
                "contextual_confidence": result.get("contextual_confidence", 0.5),
                "reasoning": result.get("reasoning", ""),
                "word_level": word_level,
                "vocab_level": self.vocabulary_level,
                "method": (
                    "hybrid_api_gpt_ai_filtered_enhanced"
                    if self.enable_ai_base_form
                    else "hybrid_api_gpt_enhanced"
                ),
            }

            self.cache[cache_key] = refined_result

            if self.verbose:
                final_syn_count = len(refined_result["final_synonyms"])
                final_ant_count = len(refined_result["final_antonyms"])
                print(
                    f"ğŸ¤– GPT ê²€ì¦: {word} â†’ ìµœì¢… ë™ì˜ì–´ {final_syn_count}ê°œ, ë°˜ì˜ì–´ {final_ant_count}ê°œ"
                )

            return refined_result

        else:
            if self.verbose:
                print(f"âŒ GPT ê²€ì¦ ì‹¤íŒ¨: {word} - {error}")

            # GPT ì‹¤íŒ¨ ì‹œì—ë„ í•„í„°ë§ ì ìš©
            filtered_synonyms = self.enhanced_filter_synonyms_antonyms(
                api_synonyms, word, max_count=2
            )
            filtered_antonyms = self.enhanced_filter_synonyms_antonyms(
                api_antonyms, word, max_count=1
            )

            return {
                "api_synonyms": api_synonyms,
                "api_antonyms": api_antonyms,
                "contextual_meaning": "",
                "part_of_speech": "",
                "final_synonyms": filtered_synonyms,
                "final_antonyms": filtered_antonyms,
                "rejected_synonyms": [],
                "rejected_antonyms": [],
                "api_confidence": "low",
                "contextual_confidence": 0.3,
                "reasoning": f"GPT ê²€ì¦ ì‹¤íŒ¨, í•„í„°ë§ëœ API ê²°ê³¼ ì‚¬ìš©: {error}",
                "word_level": word_level,
                "vocab_level": self.vocabulary_level,
                "method": "api_only_fallback_enhanced",
            }

    def process_vocabulary(
        self, max_items: int = None, batch_size: int = 10
    ) -> pd.DataFrame:
        """ë¬¸ë§¥ ì •ë³´ë¥¼ í¬í•¨í•œ ê°œì„ ëœ ì²˜ë¦¬ (ê¸°ì¡´ ë©”ì„œë“œ ìˆ˜ì •)"""

        # ê¸°ì¡´ ì²˜ë¦¬ ë¡œì§ê³¼ ë™ì¼í•˜ì§€ë§Œ ë¬¸ë§¥ ì •ë³´ ì¶”ê°€
        if max_items:
            process_df = self.process_df.head(max_items).copy()
        else:
            process_df = self.process_df.copy()

        total_items = len(process_df)
        processed_count = 0

        print("ğŸ¯ ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ì²˜ë¦¬ ì‹œì‘")

        for idx, row in process_df.iterrows():
            word = str(row.get("ë‹¨ì–´", "")).strip()
            meaning = str(row.get("ëœ»(í•œê¸€)", "")).strip()
            context = str(row.get("ë¬¸ë§¥", "")).strip()
            pos = str(row.get("í’ˆì‚¬", "")).strip()

            if not word:
                continue

            try:
                start_time = time.time()

                # ğŸ”¥ í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë‹¨ì–´ì˜ ë¬¸ë§¥ê³¼ í’ˆì‚¬ ì €ì¥
                self._current_pos = pos
                self._current_context = context  # ë¬¸ë§¥ ì •ë³´ ì¶”ê°€

                # 1ë‹¨ê³„: ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ í›„ë³´ ìˆ˜ì§‘
                api_synonyms, api_antonyms = self.collect_api_candidates(word, pos)

                # 2ë‹¨ê³„: GPT ê²€ì¦ (ê¸°ì¡´ê³¼ ë™ì¼)
                result = self.gpt_validate_and_select(
                    word, meaning, context, api_synonyms, api_antonyms
                )

                processing_time = time.time() - start_time

                # ê²°ê³¼ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼)
                final_synonyms_str = ", ".join(result["final_synonyms"])
                final_antonyms_str = ", ".join(result["final_antonyms"])

                process_df.at[idx, "ë™ì˜ì–´"] = final_synonyms_str
                process_df.at[idx, "ë°˜ì˜ì–´"] = final_antonyms_str

                if result["contextual_meaning"]:
                    process_df.at[idx, "ë¬¸ë§¥ì ì˜ë¯¸"] = result["contextual_meaning"]

                process_df.at[idx, "ë™ì˜ì–´ì‹ ë¢°ë„"] = result["contextual_confidence"]
                process_df.at[idx, "ì²˜ë¦¬ë°©ì‹"] = "ë¬¸ë§¥ê¸°ë°˜ì •ë°€ì¶”ì¶œ"
                process_df.at[idx, "í¬í•¨ì´ìœ "] = result["reasoning"]

                processed_count += 1

                # ì§„í–‰ìƒí™© ì¶œë ¥
                if processed_count % batch_size == 0:
                    percentage = (processed_count / total_items) * 100
                    print(
                        f"ğŸ“ˆ ì§„í–‰ë¥ : {processed_count}/{total_items} ({percentage:.1f}%) - ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ì²˜ë¦¬"
                    )

            except Exception as e:
                print(f"âŒ {word} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        print(f"\nâœ… ë¬¸ë§¥ ê¸°ë°˜ ì •ë°€ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ í•­ëª©")
        if max_items:
            process_df = self.process_df.head(max_items).copy()
        else:
            process_df = self.process_df.copy()

        # ì„ì‹œ ì»¬ëŸ¼ë“¤ ì¶”ê°€ (ì²˜ë¦¬ìš©)
        process_df["_ì›ë³¸_ë™ì˜ì–´"] = process_df["ë™ì˜ì–´"].copy()
        process_df["_ì›ë³¸_ë°˜ì˜ì–´"] = process_df["ë°˜ì˜ì–´"].copy()
        process_df["_API_ë™ì˜ì–´ìˆ˜"] = 0
        process_df["_API_ë°˜ì˜ì–´ìˆ˜"] = 0
        process_df["_ìµœì¢…_ë™ì˜ì–´ìˆ˜"] = 0
        process_df["_ìµœì¢…_ë°˜ì˜ì–´ìˆ˜"] = 0
        process_df["_ì²˜ë¦¬_ì‹œê°„"] = ""
        process_df["_ê°œì„ _ë°©ì‹"] = ""

        method_description = (
            "í•˜ì´ë¸Œë¦¬ë“œ + AI ì›í˜• í•„í„°ë§" if self.enable_ai_base_form else "í•˜ì´ë¸Œë¦¬ë“œ"
        )
        print("ğŸ”„ {} ì²˜ë¦¬ ì‹œì‘: {}ê°œ í•­ëª©".format(method_description, total_items))
        print(
            "ğŸ“Š 1ë‹¨ê³„: API í›„ë³´ ìˆ˜ì§‘ â†’ 2ë‹¨ê³„: {} â†’ 3ë‹¨ê³„: GPT ë¬¸ë§¥ ê²€ì¦".format(
                "ê¸°ì¡´ í•„í„°ë§ + AI ì›í˜• íŒë³„"
                if self.enable_ai_base_form
                else "ê¸°ì¡´ í•„í„°ë§ + êµì°¨ ê²€ì¦"
            )
        )

        processed_count = 0

        for idx, row in process_df.iterrows():
            word = str(row.get("ë‹¨ì–´", "")).strip()
            meaning = str(row.get("ëœ»(í•œê¸€)", "")).strip()
            context = str(row.get("ë¬¸ë§¥", "")).strip()
            pos = str(row.get("í’ˆì‚¬", "")).strip()  # ğŸ”¥ í’ˆì‚¬ ì •ë³´ ì¶”ê°€

            if not word:
                continue

            try:
                start_time = time.time()

                # ğŸ”¥ í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë‹¨ì–´ì˜ í’ˆì‚¬ ì €ì¥ (GPT í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©)
                self._current_pos = pos

                # 1ë‹¨ê³„: APIì—ì„œ í›„ë³´ ìˆ˜ì§‘ (í’ˆì‚¬ ì •ë³´ ì „ë‹¬)
                api_synonyms, api_antonyms = self.collect_api_candidates(word, pos)

                # 2ë‹¨ê³„: GPTë¡œ ê²€ì¦ ë° ì„ ë³„ (í’ˆì‚¬ ì •ë³´ í¬í•¨)
                result = self.gpt_validate_and_select(
                    word, meaning, context, api_synonyms, api_antonyms
                )
                processing_time = time.time() - start_time

                # VOCABULARY.XLSX êµ¬ì¡°ì— ë§ê²Œ ê²°ê³¼ ì—…ë°ì´íŠ¸
                # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ìœ ì§€í•˜ê³  ë™ì˜ì–´/ë°˜ì˜ì–´ë§Œ ê°œì„ 
                final_synonyms_str = ", ".join(result["final_synonyms"])
                final_antonyms_str = ", ".join(result["final_antonyms"])

                # ë™ì˜ì–´/ë°˜ì˜ì–´ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
                process_df.at[idx, "ë™ì˜ì–´"] = final_synonyms_str
                process_df.at[idx, "ë°˜ì˜ì–´"] = final_antonyms_str

                # ë¬¸ë§¥ì ì˜ë¯¸ ì—…ë°ì´íŠ¸ (ìˆëŠ” ê²½ìš°)
                if result["contextual_meaning"]:
                    process_df.at[idx, "ë¬¸ë§¥ì ì˜ë¯¸"] = result["contextual_meaning"]

                # ë™ì˜ì–´ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
                process_df.at[idx, "ë™ì˜ì–´ì‹ ë¢°ë„"] = result["contextual_confidence"]

                # ì²˜ë¦¬ë°©ì‹ ì—…ë°ì´íŠ¸
                if self.enable_ai_base_form:
                    process_df.at[idx, "ì²˜ë¦¬ë°©ì‹"] = "ê°œì„ ëœì •ì œ_AIì›í˜•í•„í„°ë§"
                else:
                    process_df.at[idx, "ì²˜ë¦¬ë°©ì‹"] = "ê°œì„ ëœì •ì œ_í•˜ì´ë¸Œë¦¬ë“œ"

                # í¬í•¨ì´ìœ  ì—…ë°ì´íŠ¸
                process_df.at[idx, "í¬í•¨ì´ìœ "] = result["reasoning"]

                # ì„ì‹œ í†µê³„ ì»¬ëŸ¼ë“¤ ì—…ë°ì´íŠ¸
                process_df.at[idx, "_API_ë™ì˜ì–´ìˆ˜"] = len(result["api_synonyms"])
                process_df.at[idx, "_API_ë°˜ì˜ì–´ìˆ˜"] = len(result["api_antonyms"])
                process_df.at[idx, "_ìµœì¢…_ë™ì˜ì–´ìˆ˜"] = len(result["final_synonyms"])
                process_df.at[idx, "_ìµœì¢…_ë°˜ì˜ì–´ìˆ˜"] = len(result["final_antonyms"])
                process_df.at[idx, "_ì²˜ë¦¬_ì‹œê°„"] = "{:.2f}s".format(processing_time)
                process_df.at[idx, "_ê°œì„ _ë°©ì‹"] = result["method"]

                processed_count += 1

                # ì§„í–‰ìƒí™© ì¶œë ¥
                if processed_count % batch_size == 0:
                    percentage = (processed_count / total_items) * 100
                    ai_note = " + AIì›í˜•" if self.enable_ai_base_form else ""
                    print(
                        "ğŸ“ˆ ì§„í–‰ë¥ : {}/{} ({:.1f}%) - API: D{}/W{}/M{}, GPT: {}íšŒ{}, ë¹„ìš©: ${:.3f}".format(
                            processed_count,
                            total_items,
                            percentage,
                            self.api_calls["datamuse"],
                            self.api_calls["wordsapi"],
                            self.api_calls["merriam"],
                            self.gpt_calls,
                            ai_note,
                            self.cost_estimate,
                        )
                    )

                # API ì œí•œ ê³ ë ¤í•œ ë”œë ˆì´
                if processed_count % 5 == 0:
                    time.sleep(0.5)

            except Exception as e:
                print("âŒ {} ì²˜ë¦¬ ì‹¤íŒ¨: {}".format(word, e))
                continue

        # ìµœì¢… í†µê³„
        ai_note = " + AI ì›í˜• í•„í„°ë§" if self.enable_ai_base_form else ""
        print("\nâœ… ê°œì„ ëœ ì •ì œ{} ì™„ë£Œ!".format(ai_note))
        print("ğŸ“Š ì²˜ë¦¬ëœ í•­ëª©: {}ê°œ".format(processed_count))
        print(
            "ğŸ“¡ API í˜¸ì¶œ: Datamuse {}íšŒ, WordsAPI {}íšŒ, Merriam {}íšŒ".format(
                self.api_calls["datamuse"],
                self.api_calls["wordsapi"],
                self.api_calls["merriam"],
            )
        )
        print(
            "ğŸ¤– GPT í˜¸ì¶œ: {}íšŒ{}".format(
                self.gpt_calls, " (ì›í˜• íŒë³„ í¬í•¨)" if self.enable_ai_base_form else ""
            )
        )
        print("ğŸ« ì´ í† í° ì‚¬ìš©: {:,}ê°œ".format(self.total_tokens))
        print("ğŸ’° ì‹¤ì œ GPT ë¹„ìš©: ${:.3f}".format(self.cost_estimate))

        if self.enable_ai_base_form:
            print("ğŸ¤– AI ì›í˜• íŒë³„ ìºì‹œ: {}ê°œ í•­ëª©".format(len(self.base_form_cache)))

        # ì •ì œ ê²°ê³¼ í†µê³„
        self.print_refinement_statistics(process_df)

        return process_df

    def print_refinement_statistics(self, df: pd.DataFrame):
        """ì •ì œ ê²°ê³¼ í†µê³„ ì¶œë ¥"""
        ai_note = " + AI ì›í˜• í•„í„°ë§" if self.enable_ai_base_form else ""
        print("\nğŸ“ˆ ê°œì„ ëœ ì •ì œ{} ê²°ê³¼ í†µê³„:".format(ai_note))

        # API vs ìµœì¢… ë¹„êµ
        api_syn_total = df["_API_ë™ì˜ì–´ìˆ˜"].sum()
        final_syn_total = df["_ìµœì¢…_ë™ì˜ì–´ìˆ˜"].sum()
        api_ant_total = df["_API_ë°˜ì˜ì–´ìˆ˜"].sum()
        final_ant_total = df["_ìµœì¢…_ë°˜ì˜ì–´ìˆ˜"].sum()

        print("ğŸ”µ ë™ì˜ì–´:")
        print(
            "   â€¢ API ìˆ˜ì§‘{}: {}ê°œ".format(
                " (AI ì›í˜• í•„í„°ë§ í›„)" if self.enable_ai_base_form else "",
                api_syn_total,
            )
        )
        print("   â€¢ GPT ê²€ì¦ í›„: {}ê°œ".format(final_syn_total))
        if api_syn_total > 0:
            retention_rate = (final_syn_total / api_syn_total) * 100
            print("   â€¢ ì±„íƒë¥ : {:.1f}%".format(retention_rate))

        print("ğŸ”´ ë°˜ì˜ì–´:")
        print(
            "   â€¢ API ìˆ˜ì§‘{}: {}ê°œ".format(
                " (AI ì›í˜• í•„í„°ë§ í›„)" if self.enable_ai_base_form else "",
                api_ant_total,
            )
        )
        print("   â€¢ GPT ê²€ì¦ í›„: {}ê°œ".format(final_ant_total))
        if api_ant_total > 0:
            retention_rate = (final_ant_total / api_ant_total) * 100
            print("   â€¢ ì±„íƒë¥ : {:.1f}%".format(retention_rate))

        # ë¬¸ë§¥ ì‹ ë¢°ë„ ë¶„í¬
        confidence_scores = pd.to_numeric(df["ë™ì˜ì–´ì‹ ë¢°ë„"], errors="coerce").fillna(0)
        high_conf = (confidence_scores >= 0.7).sum()
        med_conf = ((confidence_scores >= 0.4) & (confidence_scores < 0.7)).sum()
        low_conf = (confidence_scores < 0.4).sum()

        print("ğŸ“Š ë¬¸ë§¥ ì‹ ë¢°ë„ ë¶„í¬:")
        print("   â€¢ ë†’ìŒ (0.7+): {}ê°œ".format(high_conf))
        print("   â€¢ ë³´í†µ (0.4-0.7): {}ê°œ".format(med_conf))
        print("   â€¢ ë‚®ìŒ (0.4ë¯¸ë§Œ): {}ê°œ".format(low_conf))

    def save_results(
        self, df: pd.DataFrame, output_prefix: str = None
    ) -> Dict[str, str]:
        """ê²°ê³¼ ì €ì¥ - VOCABULARY.XLSXì™€ ë™ì¼í•œ êµ¬ì¡°"""
        if output_prefix is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_name = os.path.splitext(os.path.basename(self.vocab_file))[0]
            ai_suffix = "_ai" if self.enable_ai_base_form else ""
            output_prefix = "{}_improved{}_{}".format(base_name, ai_suffix, timestamp)

        saved_files = {}

        try:
            # VOCABULARY.XLSXì™€ ì •í™•íˆ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì¶œë ¥
            expected_columns = [
                "êµì¬ID",
                "êµì¬ëª…",
                "ì§€ë¬¸ID",
                "ìˆœì„œ",
                "ì§€ë¬¸",
                "ë‹¨ì–´",
                "ì›í˜•",
                "í’ˆì‚¬",
                "ëœ»(í•œê¸€)",
                "ëœ»(ì˜ì–´)",
                "ë™ì˜ì–´",
                "ë°˜ì˜ì–´",
                "ë¬¸ë§¥",
                "ë¶„ë¦¬í˜•ì—¬ë¶€",
                "ì‹ ë¢°ë„",
                "ì‚¬ìš©ìDBë§¤ì¹­",
                "ë§¤ì¹­ë°©ì‹",
                "íŒ¨í„´ì •ë³´",
                "ë¬¸ë§¥ì ì˜ë¯¸",
                "ë™ì˜ì–´ì‹ ë¢°ë„",
                "ì²˜ë¦¬ë°©ì‹",
                "í¬í•¨ì´ìœ ",
            ]

            # ì„ì‹œ ì»¬ëŸ¼ë“¤ ì œê±°í•˜ê³  ì›ë³¸ êµ¬ì¡°ë§Œ ìœ ì§€
            output_df = df.copy()
            temp_columns = [col for col in output_df.columns if col.startswith("_")]
            output_df = output_df.drop(columns=temp_columns, errors="ignore")

            # ì»¬ëŸ¼ ìˆœì„œë¥¼ VOCABULARY.XLSXì™€ ë™ì¼í•˜ê²Œ ì •ë ¬
            available_columns = [
                col for col in expected_columns if col in output_df.columns
            ]
            output_df = output_df[available_columns]

            # 1. ë©”ì¸ ê²°ê³¼ íŒŒì¼ (VOCABULARY.XLSXì™€ ë™ì¼í•œ êµ¬ì¡°)
            main_file = "{}_VOCABULARY_IMPROVED.xlsx".format(output_prefix)
            output_df.to_excel(main_file, index=False, engine="openpyxl")
            saved_files["ê°œì„ ëœ_VOCABULARYíŒŒì¼"] = main_file

            # 2. ìƒì„¸ ë¶„ì„ ê²°ê³¼ (ì„ì‹œ ì»¬ëŸ¼ë“¤ í¬í•¨)
            detailed_file = "{}_detailed_analysis.xlsx".format(output_prefix)
            df.to_excel(detailed_file, index=False, engine="openpyxl")
            saved_files["ìƒì„¸ë¶„ì„ê²°ê³¼"] = detailed_file

            # 3. ë¹„êµ ë¶„ì„ ë²„ì „ (Before/After)
            if all(
                col in df.columns for col in ["ë‹¨ì–´", "_ì›ë³¸_ë™ì˜ì–´", "_ì›ë³¸_ë°˜ì˜ì–´"]
            ):
                compare_data = []
                for idx, row in df.iterrows():
                    word = row.get("ë‹¨ì–´", "")
                    original_syn = row.get("_ì›ë³¸_ë™ì˜ì–´", "")
                    original_ant = row.get("_ì›ë³¸_ë°˜ì˜ì–´", "")
                    improved_syn = row.get("ë™ì˜ì–´", "")
                    improved_ant = row.get("ë°˜ì˜ì–´", "")

                    compare_data.append(
                        {
                            "ë‹¨ì–´": word,
                            "ëœ»(í•œê¸€)": row.get("ëœ»(í•œê¸€)", ""),
                            "ë¬¸ë§¥": row.get("ë¬¸ë§¥", ""),
                            "ì›ë³¸_ë™ì˜ì–´": original_syn,
                            "ê°œì„ ëœ_ë™ì˜ì–´": improved_syn,
                            "ì›ë³¸_ë°˜ì˜ì–´": original_ant,
                            "ê°œì„ ëœ_ë°˜ì˜ì–´": improved_ant,
                            "ë™ì˜ì–´_ë³€í™”": (
                                "âœ“ ê°œì„ ë¨" if original_syn != improved_syn else "ë™ì¼"
                            ),
                            "ë°˜ì˜ì–´_ë³€í™”": (
                                "âœ“ ê°œì„ ë¨" if original_ant != improved_ant else "ë™ì¼"
                            ),
                            "ì‹ ë¢°ë„": row.get("ë™ì˜ì–´ì‹ ë¢°ë„", ""),
                            "ì²˜ë¦¬ë°©ì‹": row.get("ì²˜ë¦¬ë°©ì‹", ""),
                            "ì²˜ë¦¬ì‹œê°„": row.get("_ì²˜ë¦¬_ì‹œê°„", ""),
                        }
                    )

                compare_df = pd.DataFrame(compare_data)
                compare_file = "{}_before_after_comparison.xlsx".format(output_prefix)
                compare_df.to_excel(compare_file, index=False, engine="openpyxl")
                saved_files["ë³€í™”ë¹„êµ"] = compare_file

            # 4. í†µê³„ ë¦¬í¬íŠ¸
            stats_file = "{}_improvement_report.txt".format(output_prefix)
            with open(stats_file, "w", encoding="utf-8") as f:
                ai_note = " + AI ì›í˜• í•„í„°ë§" if self.enable_ai_base_form else ""
                f.write("ê°œì„ ëœ ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ì œ ê²°ê³¼ ë¦¬í¬íŠ¸{}\n".format(ai_note))
                f.write("=" * 70 + "\n")
                f.write(
                    "ì²˜ë¦¬ ì¼ì‹œ: {}\n".format(
                        datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    )
                )
                f.write("ì›ë³¸ íŒŒì¼: {}\n".format(self.vocab_file))
                f.write("ë‹¨ì–´ì¥ ìˆ˜ì¤€: {}\n".format(self.vocabulary_level))
                f.write("ì²˜ë¦¬ëœ í•­ëª©: {}ê°œ\n".format(len(df)))
                if self.enable_ai_base_form:
                    f.write("AI ì›í˜• í•„í„°ë§: í™œì„±í™”\n")
                f.write("\n")

                f.write("ğŸ”§ ì ìš©ëœ ê°œì„ ì‚¬í•­:\n")
                f.write("âœ… 1. ì›í˜• ë‹¨ì–´ë§Œ í—ˆìš© (ë™ëª…ì‚¬, ë³µìˆ˜í˜•, ê³¼ê±°í˜• ë“± ì œì™¸)\n")
                f.write("âœ… 2. ì–µì§€ìŠ¤ëŸ¬ìš´ ë°˜ì˜ì–´ ì œì™¸ (ì¶”ìƒê°œë…, ê³ ìœ ëª…ì‚¬ ë“±)\n")
                f.write("âœ… 3. ì–´ê·¼ ë™ì¼í•œ ë‹¨ì–´ ì¤‘ë³µ ì œê±° (order vs orderliness)\n")
                f.write("âœ… 4. 2ê°œ ì´ìƒ í•©ì„±ì–´ ì œì™¸ (free person, well-known ë“±)\n")
                f.write("âœ… 5. ë™ëª…ì‚¬, ë³µìˆ˜í˜•íƒœ ì™„ì „ ì œì™¸\n")
                f.write("âœ… 6. VOCABULARY.XLSXì™€ ë™ì¼í•œ êµ¬ì¡° ìœ ì§€\n\n")

                f.write("API ì‚¬ìš©ëŸ‰:\n")
                f.write("- Datamuse: {}íšŒ (ë¬´ë£Œ)\n".format(self.api_calls["datamuse"]))
                f.write(
                    "- WordsAPI: {}íšŒ (ë¬´ë£Œ 2500/ì›”)\n".format(
                        self.api_calls["wordsapi"]
                    )
                )
                f.write(
                    "- Merriam-Webster: {}íšŒ (ë¬´ë£Œ 1000/ì¼)\n\n".format(
                        self.api_calls["merriam"]
                    )
                )

                f.write("GPT ì‚¬ìš©ëŸ‰:\n")
                f.write(
                    "- í˜¸ì¶œ íšŸìˆ˜: {}íšŒ{}\n".format(
                        self.gpt_calls,
                        " (ì›í˜• íŒë³„ í¬í•¨)" if self.enable_ai_base_form else "",
                    )
                )
                f.write("- í† í° ì‚¬ìš©: {:,}ê°œ\n".format(self.total_tokens))
                f.write("- ë¹„ìš©: ${:.3f}\n\n".format(self.cost_estimate))

                if self.enable_ai_base_form:
                    f.write("AI ì›í˜• íŒë³„:\n")
                    f.write("- ìºì‹œëœ í•­ëª©: {}ê°œ\n\n".format(len(self.base_form_cache)))

                # ìƒì„¸ í†µê³„ ì¶”ê°€
                try:
                    api_syn_total = df["_API_ë™ì˜ì–´ìˆ˜"].sum()
                    final_syn_total = df["_ìµœì¢…_ë™ì˜ì–´ìˆ˜"].sum()
                    api_ant_total = df["_API_ë°˜ì˜ì–´ìˆ˜"].sum()
                    final_ant_total = df["_ìµœì¢…_ë°˜ì˜ì–´ìˆ˜"].sum()

                    f.write("ì²˜ë¦¬ ê²°ê³¼:\n")
                    f.write(
                        "ë™ì˜ì–´: API {}ê°œ â†’ ìµœì¢… {}ê°œ (ì±„íƒë¥  {:.1f}%)\n".format(
                            api_syn_total,
                            final_syn_total,
                            (
                                (final_syn_total / api_syn_total * 100)
                                if api_syn_total > 0
                                else 0
                            ),
                        )
                    )
                    f.write(
                        "ë°˜ì˜ì–´: API {}ê°œ â†’ ìµœì¢… {}ê°œ (ì±„íƒë¥  {:.1f}%)\n".format(
                            api_ant_total,
                            final_ant_total,
                            (
                                (final_ant_total / api_ant_total * 100)
                                if api_ant_total > 0
                                else 0
                            ),
                        )
                    )

                    # ë³€í™” í†µê³„
                    if "_ì›ë³¸_ë™ì˜ì–´" in df.columns:
                        syn_changed = (df["_ì›ë³¸_ë™ì˜ì–´"] != df["ë™ì˜ì–´"]).sum()
                        ant_changed = (df["_ì›ë³¸_ë°˜ì˜ì–´"] != df["ë°˜ì˜ì–´"]).sum()
                        f.write("\në³€í™” í†µê³„:\n")
                        f.write("- ë™ì˜ì–´ ê°œì„ ëœ í•­ëª©: {}ê°œ\n".format(syn_changed))
                        f.write("- ë°˜ì˜ì–´ ê°œì„ ëœ í•­ëª©: {}ê°œ\n".format(ant_changed))
                        f.write(
                            "- ì „ì²´ ê°œì„ ìœ¨: {:.1f}%\n".format(
                                ((syn_changed + ant_changed) / (len(df) * 2)) * 100
                            )
                        )

                except Exception as e:
                    f.write("í†µê³„ ê³„ì‚° ì˜¤ë¥˜: {}\n".format(e))

                f.write("\nğŸ“ ì¶œë ¥ íŒŒì¼:\n")
                f.write(
                    "- ë©”ì¸ ê²°ê³¼: {}_VOCABULARY_IMPROVED.xlsx (ì›ë³¸ê³¼ ë™ì¼í•œ êµ¬ì¡°)\n".format(
                        output_prefix
                    )
                )
                f.write(
                    "- ìƒì„¸ ë¶„ì„: {}_detailed_analysis.xlsx\n".format(output_prefix)
                )
                f.write(
                    "- ë³€í™” ë¹„êµ: {}_before_after_comparison.xlsx\n".format(
                        output_prefix
                    )
                )

            saved_files["í†µê³„ë¦¬í¬íŠ¸"] = stats_file

            # 5. ìºì‹œ ì €ì¥
            self.save_cache()

            print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
            for desc, filename in saved_files.items():
                print("   â€¢ {}: {}".format(desc, filename))

            print(f"\nğŸ¯ ë©”ì¸ ì¶œë ¥ íŒŒì¼: {main_file}")
            print("   â†’ VOCABULARY.XLSXì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ë™ì˜ì–´/ë°˜ì˜ì–´ê°€ ê°œì„ ë¨")

        except Exception as e:
            print("âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {}".format(e))
            if self.verbose:
                import traceback

                traceback.print_exc()

        return saved_files

    def load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        # GPT ìºì‹œ
        gpt_cache_file = "improved_gpt_cache_{}.json".format(self.vocabulary_level)
        try:
            if os.path.exists(gpt_cache_file):
                with open(gpt_cache_file, "r", encoding="utf-8") as f:
                    self.cache = json.load(f)
                print("ğŸ“¦ GPT ìºì‹œ ë¡œë“œ: {}ê°œ í•­ëª©".format(len(self.cache)))
        except Exception as e:
            print("âš ï¸ GPT ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
            self.cache = {}

        # API ìºì‹œ
        api_cache_file = "improved_api_cache.json"
        try:
            if os.path.exists(api_cache_file):
                with open(api_cache_file, "r", encoding="utf-8") as f:
                    self.api_cache = json.load(f)
                print("ğŸ“¦ API ìºì‹œ ë¡œë“œ: {}ê°œ í•­ëª©".format(len(self.api_cache)))
        except Exception as e:
            print("âš ï¸ API ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
            self.api_cache = {}

        # AI ì›í˜• íŒë³„ ìºì‹œ
        if self.enable_ai_base_form:
            base_form_cache_file = "improved_ai_base_form_cache.json"
            try:
                if os.path.exists(base_form_cache_file):
                    with open(base_form_cache_file, "r", encoding="utf-8") as f:
                        self.base_form_cache = json.load(f)
                    print(
                        "ğŸ“¦ AI ì›í˜• íŒë³„ ìºì‹œ ë¡œë“œ: {}ê°œ í•­ëª©".format(
                            len(self.base_form_cache)
                        )
                    )
            except Exception as e:
                print("âš ï¸ AI ì›í˜• íŒë³„ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
                self.base_form_cache = {}

        # ì–´ê·¼ ìºì‹œ ë¡œë“œ
        word_root_cache_file = "improved_word_root_cache.json"
        try:
            if os.path.exists(word_root_cache_file):
                with open(word_root_cache_file, "r", encoding="utf-8") as f:
                    self.word_root_cache = json.load(f)
                print(
                    "ğŸ“¦ ì–´ê·¼ ë¶„ì„ ìºì‹œ ë¡œë“œ: {}ê°œ í•­ëª©".format(
                        len(self.word_root_cache)
                    )
                )
        except Exception as e:
            print("âš ï¸ ì–´ê·¼ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
            self.word_root_cache = {}

    def basic_filter_candidates_for_synonyms(
        self, candidates: List[str], original_word: str
    ) -> List[str]:
        """ê¸°ë³¸ í•„í„°ë§ - ë‹¨ì¼ ë‹¨ì–´ë§Œ, ìœ íš¨í•œ í˜•íƒœë§Œ"""
        if not candidates:
            return []

        filtered = []
        original_lower = original_word.lower() if original_word else ""

        for candidate in candidates:
            if not candidate or not isinstance(candidate, str):
                continue

            candidate = candidate.strip()
            candidate_lower = candidate.lower()

            # ğŸ”¥ 1. ë‘ ê°œ ì´ìƒ ë‹¨ì–´ ì œì™¸ (ê³µë°±, í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´)
            if " " in candidate or "-" in candidate or "_" in candidate:
                continue

            # 2. ìê¸° ìì‹  ì œì™¸
            if candidate_lower == original_lower:
                continue

            # 3. ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸ (2ê¸€ì ì´í•˜)
            if len(candidate) <= 2:
                continue

            # 4. ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ì–´ ì œì™¸
            if any(char.isdigit() for char in candidate):
                continue

            # 5. íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ë‹¨ì–´ ì œì™¸ (ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ì œì™¸)
            if re.search(r"[^\w']", candidate):
                continue

            # 6. ëª¨ë‘ ëŒ€ë¬¸ìì¸ ë‹¨ì–´ ì œì™¸ (ì•½ì–´ì¼ ê°€ëŠ¥ì„±)
            if candidate.isupper() and len(candidate) > 1:
                continue

            # 7. ë„ˆë¬´ ê¸´ ë‹¨ì–´ ì œì™¸ (12ê¸€ì ì´ˆê³¼)
            if len(candidate) > 12:
                continue

            # 8. ìœ íš¨í•œ ì˜ì–´ ì•ŒíŒŒë²³ìœ¼ë¡œë§Œ êµ¬ì„±
            if not re.match(r"^[a-zA-Z']+$", candidate):
                continue

            filtered.append(candidate)

        return filtered

    def extract_word_root_for_synonyms(self, word: str) -> str:
        """ë‹¨ì–´ì˜ ì–´ê·¼ ì¶”ì¶œ (PorterStemmer ì‚¬ìš©)"""
        if not word:
            return ""

        # ê¸°ì¡´ ìºì‹œ í™•ì¸
        if word.lower() in self.word_root_cache:
            return self.word_root_cache[word.lower()]

        original_word = word.lower()

        # ë¶ˆê·œì¹™ ì–´ê·¼ ë§¤í•‘ (ì£¼ìš” ë¶ˆê·œì¹™ ë³€í™”ë“¤)
        irregular_roots = {
            "better": "good",
            "best": "good",
            "worse": "bad",
            "worst": "bad",
            "more": "much",
            "most": "much",
            "less": "little",
            "least": "little",
            "further": "far",
            "furthest": "far",
            "farther": "far",
            "farthest": "far",
            "older": "old",
            "oldest": "old",
            "elder": "old",
            "eldest": "old",
        }

        if original_word in irregular_roots:
            root = irregular_roots[original_word]
        else:
            # ê°„ë‹¨í•œ ì ‘ë¯¸ì‚¬ ì œê±° (ìˆœì„œ ì¤‘ìš” - ê¸´ ê²ƒë¶€í„°)
            suffixes = [
                "ness",
                "ment",
                "tion",
                "sion",
                "ible",
                "able",
                "ical",
                "ful",
                "less",
                "ish",
                "ous",
                "ive",
                "ing",
                "ed",
                "er",
                "est",
                "ly",
                "al",
                "ic",
                "y",
            ]

            root = original_word
            for suffix in suffixes:
                if root.endswith(suffix) and len(root) > len(suffix) + 2:
                    potential_root = root[: -len(suffix)]
                    if len(potential_root) >= 3:
                        root = potential_root
                        break

        # ìºì‹œì— ì €ì¥
        self.word_root_cache[word.lower()] = root
        return root

    def remove_same_root_duplicates_for_synonyms(
        self, words: List[str], original_word: str
    ) -> List[str]:
        """ì–´ê·¼ì´ ê°™ì€ ë‹¨ì–´ë“¤ ì¤‘ë³µ ì œê±°"""
        if not words:
            return words

        filtered = []
        seen_roots = set()
        original_root = self.extract_word_root_for_synonyms(original_word)

        for word in words:
            if not word:
                continue

            word_root = self.extract_word_root_for_synonyms(word)

            # ì›ë³¸ ë‹¨ì–´ì™€ ê°™ì€ ì–´ê·¼ì´ë©´ ì œì™¸
            if word_root == original_root:
                continue

            # ì´ë¯¸ ê°™ì€ ì–´ê·¼ì˜ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì œì™¸
            if word_root in seen_roots:
                continue

            # ì–´ê·¼ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ (2ê¸€ì ì´í•˜) ì‹ ë¢°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë‹¨ì–´ ìì²´ë¡œ ë¹„êµ
            if len(word_root) <= 2:
                word_for_comparison = word.lower()
                if word_for_comparison in [w.lower() for w in filtered]:
                    continue

            seen_roots.add(word_root)
            filtered.append(word)

        return filtered

    def enhanced_filter_synonyms_antonyms(
        self, candidates: List[str], original_word: str, max_count: int = 3
    ) -> List[str]:
        """
        ë™ì˜ì–´/ë°˜ì˜ì–´ í›„ë³´ë“¤ì„ ì—„ê²©í•˜ê²Œ í•„í„°ë§

        Args:
            candidates: í›„ë³´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
            original_word: ì›ë³¸ ë‹¨ì–´
            max_count: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜

        Returns:
            í•„í„°ë§ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ max_countê°œ)
        """
        if not candidates:
            return []

        # 1ë‹¨ê³„: ê¸°ë³¸ í•„í„°ë§ (ì´ë¯¸ ì ìš©ë˜ì—ˆì§€ë§Œ ì¶”ê°€ ì•ˆì „ì¥ì¹˜)
        filtered = self.basic_filter_candidates_for_synonyms(candidates, original_word)

        # 2ë‹¨ê³„: ì–´ê·¼ ì¤‘ë³µ ì œê±°
        filtered = self.remove_same_root_duplicates_for_synonyms(
            filtered, original_word
        )

        # 3ë‹¨ê³„: ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        return filtered[:max_count]

    def save_cache(self):
        """ìºì‹œ ì €ì¥"""
        # GPT ìºì‹œ ì €ì¥
        gpt_cache_file = "improved_gpt_cache_{}.json".format(self.vocabulary_level)
        try:
            with open(gpt_cache_file, "w", encoding="utf-8") as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
            if self.verbose:
                print("ğŸ’¾ GPT ìºì‹œ ì €ì¥: {}ê°œ í•­ëª©".format(len(self.cache)))
        except Exception as e:
            print("âš ï¸ GPT ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {}".format(e))

        # API ìºì‹œ ì €ì¥
        api_cache_file = "improved_api_cache.json"
        try:
            with open(api_cache_file, "w", encoding="utf-8") as f:
                json.dump(self.api_cache, f, ensure_ascii=False, indent=2)
            if self.verbose:
                print("ğŸ’¾ API ìºì‹œ ì €ì¥: {}ê°œ í•­ëª©".format(len(self.api_cache)))
        except Exception as e:
            print("âš ï¸ API ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {}".format(e))

        # AI ì›í˜• íŒë³„ ìºì‹œ ì €ì¥
        if self.enable_ai_base_form:
            base_form_cache_file = "improved_ai_base_form_cache.json"
            try:
                with open(base_form_cache_file, "w", encoding="utf-8") as f:
                    json.dump(self.base_form_cache, f, ensure_ascii=False, indent=2)
                if self.verbose:
                    print(
                        "ğŸ’¾ AI ì›í˜• íŒë³„ ìºì‹œ ì €ì¥: {}ê°œ í•­ëª©".format(
                            len(self.base_form_cache)
                        )
                    )
            except Exception as e:
                print("âš ï¸ AI ì›í˜• íŒë³„ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {}".format(e))

        # ì–´ê·¼ ìºì‹œ ì €ì¥
        word_root_cache_file = "improved_word_root_cache.json"
        try:
            with open(word_root_cache_file, "w", encoding="utf-8") as f:
                json.dump(self.word_root_cache, f, ensure_ascii=False, indent=2)
            if self.verbose:
                print(
                    "ğŸ’¾ ì–´ê·¼ ë¶„ì„ ìºì‹œ ì €ì¥: {}ê°œ í•­ëª©".format(
                        len(self.word_root_cache)
                    )
                )
        except Exception as e:
            print("âš ï¸ ì–´ê·¼ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {}".format(e))


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if not check_dependencies():
        return
    parser = argparse.ArgumentParser(
        description="ê°œì„ ëœ ë™ì˜ì–´/ë°˜ì˜ì–´ ì •ì œ ì‹œìŠ¤í…œ (VOCABULARY.XLSX êµ¬ì¡° ì¶œë ¥)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
python improved_synonym_refiner.py vocabulary.xlsx
python improved_synonym_refiner.py vocab.xlsx --max-items 50
python improved_synonym_refiner.py vocab.xlsx --api-key your_openai_key
python improved_synonym_refiner.py vocab.xlsx --verbose --output-prefix my_improved
python improved_synonym_refiner.py vocab.xlsx --disable-ai-base-form  # AI ì›í˜• íŒë³„ ë¹„í™œì„±í™”

ìƒˆë¡œìš´ ê¸°ëŠ¥:
- VOCABULARY.XLSXì™€ ì •í™•íˆ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì¶œë ¥
- AI ê¸°ë°˜ ì›í˜• ë‹¨ì–´ íŒë³„ (ê¸°ë³¸ í™œì„±í™”)
- running, cats, better ë“± ë³€í˜•ëœ í˜•íƒœ ìë™ ì œê±°
- ê¸°ì¡´ ëª¨ë“  ê¸°ëŠ¥ê³¼ ì„±ëŠ¥ ìœ ì§€
- Before/After ë¹„êµ ë¶„ì„ ì œê³µ

API í‚¤ ì„¤ì • (ì„ íƒì‚¬í•­):
export RAPIDAPI_KEY="your_rapidapi_key"          # WordsAPIìš©
export MERRIAM_WEBSTER_KEY="your_merriam_key"    # Merriam-Websterìš©

â€» API í‚¤ê°€ ì—†ì–´ë„ Datamuse APIë§Œìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤ (ë¬´ë£Œ, ë¬´ì œí•œ)

ì¶œë ¥ íŒŒì¼:
- {íŒŒì¼ëª…}_VOCABULARY_IMPROVED.xlsx : ë©”ì¸ ê²°ê³¼ (VOCABULARY.XLSXì™€ ë™ì¼í•œ êµ¬ì¡°)
- {íŒŒì¼ëª…}_detailed_analysis.xlsx : ìƒì„¸ ë¶„ì„ ê²°ê³¼
- {íŒŒì¼ëª…}_before_after_comparison.xlsx : ë³€í™” ë¹„êµ
- {íŒŒì¼ëª…}_improvement_report.txt : í†µê³„ ë¦¬í¬íŠ¸
    """,
    )

    parser.add_argument("vocab_file", help="ì²˜ë¦¬í•  VOCABULARY.XLSX íŒŒì¼")
    parser.add_argument(
        "--api-key", help="OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë„ ì‚¬ìš© ê°€ëŠ¥)"
    )
    parser.add_argument("--max-items", type=int, help="ìµœëŒ€ ì²˜ë¦¬ í•­ëª© ìˆ˜ (ë¹„ìš© ì œí•œ)")
    parser.add_argument(
        "--batch-size", type=int, default=10, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)"
    )
    parser.add_argument("--output-prefix", help="ì¶œë ¥ íŒŒì¼ ì ‘ë‘ì‚¬")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ì¶œë ¥")
    parser.add_argument(
        "--disable-ai-base-form",
        action="store_true",
        help="AI ì›í˜• íŒë³„ ë¹„í™œì„±í™” (ê¸°ì¡´ ë°©ì‹ë§Œ ì‚¬ìš©)",
    )

    args = parser.parse_args()

    try:
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(args.vocab_file):
            print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(args.vocab_file))
            return

        # API í‚¤ ìƒíƒœ í™•ì¸
        print("ğŸ”‘ API í‚¤ ìƒíƒœ í™•ì¸:")
        print(
            "   â€¢ OpenAI: {}".format(
                "ì„¤ì •ë¨" if (args.api_key or os.getenv("OPENAI_API_KEY")) else "âŒ í•„ìˆ˜"
            )
        )
        print(
            "   â€¢ RapidAPI (WordsAPI): {}".format(
                "ì„¤ì •ë¨" if os.getenv("RAPIDAPI_KEY") else "ì„ íƒì‚¬í•­"
            )
        )
        print(
            "   â€¢ Merriam-Webster: {}".format(
                "ì„¤ì •ë¨" if os.getenv("MERRIAM_WEBSTER_KEY") else "ì„ íƒì‚¬í•­"
            )
        )
        print("   â€¢ Datamuse: ë¬´ë£Œ (í‚¤ ë¶ˆìš”)")

        if not (args.api_key or os.getenv("OPENAI_API_KEY")):
            print("âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return

        # AI ì›í˜• íŒë³„ ì„¤ì •
        enable_ai_base_form = not args.disable_ai_base_form
        if enable_ai_base_form:
            print("ğŸ¤– AI ì›í˜• íŒë³„: í™œì„±í™” (--disable-ai-base-formìœ¼ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥)")
        else:
            print("ğŸ“ AI ì›í˜• íŒë³„: ë¹„í™œì„±í™” (ê¸°ì¡´ íŒ¨í„´ ë°©ì‹ë§Œ ì‚¬ìš©)")

        print("ğŸ“‹ ì¶œë ¥ êµ¬ì¡°: VOCABULARY.XLSXì™€ ë™ì¼í•œ 22ê°œ ì»¬ëŸ¼ êµ¬ì¡° ìœ ì§€")

        # ì •ì œê¸° ì´ˆê¸°í™”
        refiner = ImprovedSynonymRefiner(
            vocab_file=args.vocab_file,
            api_key=args.api_key,
            verbose=args.verbose,
            enable_ai_base_form=enable_ai_base_form,
        )

        # ì²˜ë¦¬ ì‹¤í–‰
        result_df = refiner.process_vocabulary(
            max_items=args.max_items, batch_size=args.batch_size
        )

        # ê²°ê³¼ ì €ì¥
        saved_files = refiner.save_results(result_df, args.output_prefix)

        ai_note = " + AI ì›í˜• íŒë³„" if enable_ai_base_form else ""
        print("\nğŸ‰ ê°œì„ ëœ ì •ì œ{} ì™„ë£Œ!".format(ai_note))
        print("ğŸ“‹ VOCABULARY.XLSXì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        if enable_ai_base_form:
            print("ğŸ¤– AI ì›í˜• íŒë³„ë¡œ ë³€í˜•ëœ ë‹¨ì–´ë“¤ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # ë©”ì¸ ì¶œë ¥ íŒŒì¼ ê°•ì¡°
        main_file = saved_files.get("ê°œì„ ëœ_VOCABULARYíŒŒì¼", "")
        if main_file:
            print(f"\nğŸ¯ ë©”ì¸ ê²°ê³¼ íŒŒì¼: {main_file}")
            print("   â†’ ì´ íŒŒì¼ì„ ê¸°ì¡´ VOCABULARY.XLSX ëŒ€ì‹  ì‚¬ìš©í•˜ì„¸ìš”!")

        print("\nğŸ’¡ ë” ë§ì€ APIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
        print("   export RAPIDAPI_KEY='your_key'")
        print("   export MERRIAM_WEBSTER_KEY='your_key'")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ: {}".format(e))
        if args.verbose:
            import traceback

            traceback.print_exc()


if __name__ == "__main__":
    main()
