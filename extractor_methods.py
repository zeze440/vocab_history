# extractor_methods.py - ì¶”ì¶œ ë©”ì„œë“œë“¤ (ê°„ë‹¨ ìˆ˜ì • ë²„ì „)

import re
from utils import force_extract_text, get_sentence_context, get_simple_pos


class ExtractorMethods:
    """ì¶”ì¶œê¸°ì˜ í•µì‹¬ ë©”ì„œë“œë“¤ì„ í¬í•¨í•˜ëŠ” ë¯¹ìŠ¤ì¸ í´ë˜ìŠ¤"""

    def process_text_with_metadata(
        self, text_input, text_id, easy_words, child_vocab, freq_tiers
    ):
        """ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ - ìƒˆ ë©”ì„œë“œ"""

        print(f"\nğŸ” í…ìŠ¤íŠ¸ {text_id} ì²˜ë¦¬ ì‹œì‘ (ë©”íƒ€ë°ì´í„° í¬í•¨)...")

        # âœ… ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        passage_info = self.get_passage_info(text_input)

        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        if isinstance(text_input, dict):
            text_content = text_input.get("content", "")
        elif isinstance(text_input, str):
            text_content = text_input
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸ í˜•ì‹: {type(text_input)}")
            return []

        # ğŸ”¥ ì…ë ¥ê°’ ê²€ì¦
        if not text_content or text_content is None:
            print(f"âŒ í…ìŠ¤íŠ¸ {text_id}: ë¹ˆ í…ìŠ¤íŠ¸")
            return []

        try:
            text_str = force_extract_text(text_content)
            if not text_str or len(text_str.strip()) < 10:
                print(f"âŒ í…ìŠ¤íŠ¸ {text_id}: ìœ íš¨í•˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸")
                return []
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ {text_id}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return []

        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text_str)}ì")

        # âœ… ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶œë ¥
        if passage_info.get("textbook_studio_passage_title"):
            print(f"   ğŸ“– ì§€ë¬¸ ì œëª©: {passage_info['textbook_studio_passage_title']}")
        if passage_info.get("book_title"):
            print(f"   ğŸ“š êµì¬ëª…: {passage_info['book_title']}")

        rows = []
        stats = {
            "user_db_idioms": 0,
            "grammar_patterns": 0,
            "reference_idioms": 0,
            "user_db_words": 0,
            "gpt_words": 0,
        }

        try:
            # 1. ìˆ™ì–´ ì¶”ì¶œ
            print(f"   ğŸ” ìˆ™ì–´ ì¶”ì¶œ ì¤‘...")
            try:
                idioms = self.extract_advanced_idioms(text_str)
                if idioms is None:
                    idioms = []

                print(f"   ğŸ“Š ìˆ™ì–´ ì¶”ì¶œ ê²°ê³¼: {len(idioms)}ê°œ")

                for i, idiom in enumerate(idioms):
                    if idiom and isinstance(idiom, dict):
                        try:
                            # âœ… ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ í–‰ ìƒì„±
                            row = self.create_result_row_with_metadata(
                                item=idiom,
                                passage_info=passage_info,
                                item_type="idiom",
                                order=i + 1,
                            )
                            rows.append(row)

                            # í†µê³„ ì—…ë°ì´íŠ¸
                            if idiom.get("user_db_match"):
                                stats["user_db_idioms"] += 1
                            elif idiom.get("type") == "grammar_pattern":
                                stats["grammar_patterns"] += 1
                            else:
                                stats["reference_idioms"] += 1

                        except Exception as e:
                            print(f"         âŒ ìˆ™ì–´ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            except Exception as e:
                print(f"   âŒ ìˆ™ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            # 2. ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ
            print(f"   ğŸ” ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ ì¤‘...")
            try:
                difficult_words = self.extract_difficult_words(
                    text_str, easy_words, child_vocab, freq_tiers
                )
                if difficult_words is None:
                    difficult_words = []

                print(f"   ğŸ“Š ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ ê²°ê³¼: {len(difficult_words)}ê°œ")

                for i, word in enumerate(difficult_words):
                    if word and isinstance(word, dict):
                        try:
                            # âœ… ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ í–‰ ìƒì„±
                            row = self.create_result_row_with_metadata(
                                item=word,
                                passage_info=passage_info,
                                item_type="word",
                                order=len(rows) + i + 1,  # ìˆ™ì–´ ë‹¤ìŒ ìˆœì„œ
                            )
                            rows.append(row)

                            # í†µê³„ ì—…ë°ì´íŠ¸
                            if word.get("user_db_match"):
                                stats["user_db_words"] += 1
                            else:
                                stats["gpt_words"] += 1

                        except Exception as e:
                            print(f"         âŒ ë‹¨ì–´ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            except Exception as e:
                print(f"   âŒ ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            # í†µê³„ ì¶œë ¥
            print(f"âœ… í…ìŠ¤íŠ¸ {text_id} ì²˜ë¦¬ ì™„ë£Œ:")
            print(f"   ğŸ“Š ì‚¬ìš©ì DB ìˆ™ì–´: {stats['user_db_idioms']}ê°œ")
            print(f"   ğŸ“Š ë¬¸ë²• íŒ¨í„´: {stats['grammar_patterns']}ê°œ")
            print(f"   ğŸ“Š ì°¸ì¡° DB ìˆ™ì–´: {stats['reference_idioms']}ê°œ")
            print(f"   ğŸ“Š ì‚¬ìš©ì DB ë‹¨ì–´: {stats['user_db_words']}ê°œ")
            print(f"   ğŸ“Š GPT ì„ íƒ ë‹¨ì–´: {stats['gpt_words']}ê°œ")
            print(f"   ğŸ“Š ì´ ì¶”ì¶œ: {len(rows)}ê°œ")

            # ğŸ”¥ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€
            if rows:  # ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ
                try:
                    print(f"   ğŸ” ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì¤‘...")
                    rows = self.add_synonyms_antonyms_to_results(rows)
                    print(f"   âœ… ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì™„ë£Œ: {len(rows)}ê°œ í•­ëª©")
                except Exception as e:
                    print(f"   âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")

        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ {text_id} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
            return []

        print(f"   ğŸ¯ ìµœì¢… ë°˜í™˜: {len(rows)}ê°œ í•­ëª©")
        return rows if rows else []

    def create_result_row_with_metadata(self, item, passage_info, item_type, order):
        """ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ê²°ê³¼ í–‰ ìƒì„±"""

        # âœ… 22ì»¬ëŸ¼ êµ¬ì¡°ì— ë§ì¶° ìƒì„±
        row = {
            # ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ë“¤
            "êµì¬ID": passage_info.get("textbook_id", ""),
            "êµì¬ëª…": passage_info.get("book_title", "Advanced Vocabulary"),
            "ì§€ë¬¸ID": passage_info.get("textbook_studio_passage_id", ""),
            "ìˆœì„œ": order,
            "ì§€ë¬¸": (
                passage_info.get("content", "")[:100] + "..."
                if len(passage_info.get("content", "")) > 100
                else passage_info.get("content", "")
            ),
            # ë‹¨ì–´/ìˆ™ì–´ ì •ë³´
            "ë‹¨ì–´": item.get("original", ""),
            "ì›í˜•": item.get("base_form", ""),
            "í’ˆì‚¬": item.get("pos", ""),
            "ëœ»(í•œê¸€)": item.get("meaning", item.get("korean_meaning", "")),
            "ëœ»(ì˜ì–´)": "",  # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„ì›€
            # ë™ì˜ì–´/ë°˜ì˜ì–´ (ë‚˜ì¤‘ì— ì¶”ê°€ë¨)
            "ë™ì˜ì–´": "",
            "ë°˜ì˜ì–´": "",
            # ë¬¸ë§¥ ì •ë³´
            "ë¬¸ë§¥": item.get("context", ""),
            "ë¶„ë¦¬í˜•ì—¬ë¶€": item.get("is_separated", False),
            "ì‹ ë¢°ë„": item.get("confidence", 0.0),
            "ì‚¬ìš©ìDBë§¤ì¹­": item.get("user_db_match", False),
            "ë§¤ì¹­ë°©ì‹": item.get("match_type", "ì¼ë°˜"),
            # ì¶”ê°€ ì •ë³´
            "íŒ¨í„´ì •ë³´": f"Studio: {passage_info.get('studio_title', '')}, Unit: {passage_info.get('textbook_unit_id', '')}",
            "ë¬¸ë§¥ì ì˜ë¯¸": "",
            "ë™ì˜ì–´ì‹ ë¢°ë„": 0.0,
            "ì²˜ë¦¬ë°©ì‹": "",
            "í¬í•¨ì´ìœ ": f"ì§€ë¬¸ '{passage_info.get('textbook_studio_passage_title', '')}' ì—ì„œ ì¶”ì¶œ",
        }

        return row

    def add_synonyms_antonyms_to_results(self, results):
        """ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ - ì»¬ëŸ¼ëª… ë§¤í•‘ ìˆ˜ì •"""

        if not hasattr(self, "synonym_extractor") or not self.synonym_extractor:
            print("âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤")
            # ë¹ˆ ì»¬ëŸ¼ ì¶”ê°€
            for result in results:
                result["ë™ì˜ì–´"] = ""
                result["ë°˜ì˜ì–´"] = ""
                result["ë™ì˜ì–´ì‹ ë¢°ë„"] = 0.0
            return results

        print("ğŸ” ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ì¤‘...")

        try:
            # ë‹¨ì–´ ì •ë³´ ì¤€ë¹„
            word_list = []
            for result in results:
                word_info = {
                    # âœ… ì •í™•í•œ ì»¬ëŸ¼ëª… ì‚¬ìš©
                    "word": result.get("ë‹¨ì–´", ""),
                    "context": result.get("ë¬¸ë§¥", ""),
                    "pos": result.get("í’ˆì‚¬", ""),
                    "meaning": result.get("ëœ»(í•œê¸€)", ""),
                }
                word_list.append(word_info)

            # ë°°ì¹˜ ì¶”ì¶œ
            synonym_results = self.synonym_extractor.batch_extract(word_list)

            # ê²°ê³¼ì— ì¶”ê°€
            for result in results:
                word = result.get("ë‹¨ì–´", "")
                if word in synonym_results:
                    syn_data = synonym_results[word]
                    synonyms = syn_data.get("synonyms", [])[:3]  # ìµœëŒ€ 3ê°œ
                    antonyms = syn_data.get("antonyms", [])[:2]  # ìµœëŒ€ 2ê°œ

                    # âœ… ì •í™•í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì €ì¥
                    result["ë™ì˜ì–´"] = ", ".join(synonyms)
                    result["ë°˜ì˜ì–´"] = ", ".join(antonyms)
                    result["ë™ì˜ì–´ì‹ ë¢°ë„"] = syn_data.get("confidence", 0.0)
                else:
                    result["ë™ì˜ì–´"] = ""
                    result["ë°˜ì˜ì–´"] = ""
                    result["ë™ì˜ì–´ì‹ ë¢°ë„"] = 0.0

            return results

        except Exception as e:
            print(f"âŒ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ì»¬ëŸ¼ ì¶”ê°€
            for result in results:
                result["ë™ì˜ì–´"] = ""
                result["ë°˜ì˜ì–´"] = ""
                result["ë™ì˜ì–´ì‹ ë¢°ë„"] = 0.0
            return results

    def extract_advanced_idioms(self, text):
        """ê¸°ë³¸ ìˆ™ì–´ ì¶”ì¶œ"""
        results = []

        try:
            text_str = force_extract_text(text)
            if not text_str:
                return []

            found_positions = set()

            # 1. ì‚¬ìš©ì DB ìˆ™ì–´ ìš°ì„  ê²€ì‚¬
            print(f"      ğŸ” ì‚¬ìš©ì DB ìˆ™ì–´ ê²€ì‚¬...")
            try:
                if hasattr(self, "user_idioms") and self.user_idioms:
                    extracted_user_idioms = self.extract_user_db_idioms(text_str)
                    if extracted_user_idioms:
                        results.extend(extracted_user_idioms)
                        print(
                            f"      âœ… ì‚¬ìš©ì DB ìˆ™ì–´: {len(extracted_user_idioms)}ê°œ"
                        )
                        # ìœ„ì¹˜ ê¸°ë¡
                        for idiom in extracted_user_idioms:
                            if idiom and isinstance(idiom, dict):
                                base_form = idiom.get("base_form", "")
                                if base_form:
                                    start = text_str.lower().find(base_form.lower())
                                    if start != -1:
                                        end = start + len(base_form)
                                        found_positions.add((start, end))
                    else:
                        print(f"      âš ï¸ ì‚¬ìš©ì ìˆ™ì–´ DB ì—†ìŒ")
            except Exception as e:
                print(f"      âŒ ì‚¬ìš©ì DB ìˆ™ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            # 2. ì°¸ì¡° DB ìˆ™ì–´ ê²€ì‚¬ (ê°„ë‹¨ ë²„ì „)
            print(f"      ğŸ” ì°¸ì¡° DB ìˆ™ì–´ ê²€ì‚¬...")
            try:
                if hasattr(self, "reference_idioms") and self.reference_idioms:
                    ref_count = 0
                    for idiom in self.reference_idioms[:50]:  # ì²˜ìŒ 50ê°œë§Œ í™•ì¸
                        if idiom and idiom.lower() in text_str.lower():
                            start = text_str.lower().find(idiom.lower())
                            end = start + len(idiom)

                            # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                            if any(abs(start - pos[0]) < 10 for pos in found_positions):
                                continue

                            context = get_sentence_context(text_str, start, end)
                            meaning = self.enhanced_korean_definition(
                                idiom, context, is_phrase=True
                            )

                            results.append(
                                {
                                    "original": idiom,
                                    "base_form": idiom,
                                    "meaning": meaning,
                                    "context": context,
                                    "type": "reference_idiom_db",
                                    "is_separated": False,
                                    "confidence": 0.85,
                                    "user_db_match": False,
                                    "match_type": "ì°¸ì¡°DB",
                                }
                            )
                            found_positions.add((start, end))
                            ref_count += 1

                    if ref_count > 0:
                        print(f"      âœ… ì°¸ì¡° DB ìˆ™ì–´: {ref_count}ê°œ")
                else:
                    print(f"      âš ï¸ ì°¸ì¡° ìˆ™ì–´ DB ì—†ìŒ")
            except Exception as e:
                print(f"      âŒ ì°¸ì¡° DB ìˆ™ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        except Exception as e:
            print(f"âŒ ìˆ™ì–´ ì¶”ì¶œ ì „ì²´ ì‹¤íŒ¨: {e}")
            return []

        return results

    def extract_user_db_idioms(self, text):
        """ì‚¬ìš©ì DBì—ì„œ ìˆ™ì–´ ì¶”ì¶œ"""
        results = []

        try:
            text_str = force_extract_text(text)
            text_lower = text_str.lower()
            found_positions = set()

            if not hasattr(self, "user_idioms") or not self.user_idioms:
                return results

            # ê¸¸ì´ìˆœ ì •ë ¬
            sorted_user_idioms = sorted(self.user_idioms, key=len, reverse=True)

            for idiom in sorted_user_idioms:
                try:  # ê°œë³„ ìˆ™ì–´ ì²˜ë¦¬ìš© try-except
                    pattern = r"\b" + re.escape(idiom) + r"\b"
                    matches = re.finditer(pattern, text_lower, re.IGNORECASE)

                    for match in matches:
                        start, end = match.span()

                        # ìœ„ì¹˜ ì¤‘ë³µ í™•ì¸
                        if any(abs(start - pos[0]) < 5 for pos in found_positions):
                            continue

                        context = get_sentence_context(text_str, start, end)
                        original_text = text_str[start:end]
                        meaning = self.enhanced_korean_definition(
                            idiom, context, is_phrase=True
                        )

                        results.append(
                            {
                                "original": original_text,
                                "base_form": idiom,
                                "meaning": meaning,
                                "context": context,
                                "type": "user_db_idiom",
                                "is_separated": False,
                                "confidence": 0.95,
                                "user_db_match": True,
                                "match_type": "ì‚¬ìš©ìDBìˆ™ì–´",
                            }
                        )
                        found_positions.add((start, end))

                except Exception as e:
                    print(f"         âŒ ìˆ™ì–´ '{idiom}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue  # ì´ì œ for ë£¨í”„ ì•ˆì— ìˆìœ¼ë¯€ë¡œ ì •ìƒ ì‘ë™

        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì DB ìˆ™ì–´ ì¶”ì¶œ ì „ì²´ ì‹¤íŒ¨: {e}")

        return results

    def extract_difficult_words(self, text, easy_words, child_vocab, freq_tiers):
        """ê¸°ë³¸ ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ"""

        try:
            text_str = force_extract_text(text)
            word_candidates = []
            user_db_candidates = []
            seen_lemmas = set()

            # SpaCyë¡œ í† í° ë¶„ì„
            if hasattr(self, "nlp") and self.nlp:
                doc = self.nlp(text_str)
                for token in doc:
                    word = token.text.lower()
                    lemma = token.lemma_.lower()
                    original_word = token.text

                    # ğŸ”¥ ê³ ìœ ëª…ì‚¬ ì œì™¸ (ìƒˆë¡œ ì¶”ê°€)
                    if token.pos_ in ["PROPN"] or token.ent_type_ in [
                        "PERSON",
                        "GPE",
                        "ORG",
                        "NORP",
                    ]:
                        if self.verbose:
                            print(
                                f"         âš ï¸ ê³ ìœ ëª…ì‚¬ ì œì™¸: '{original_word}' ({token.pos_}, {token.ent_type_})"
                            )
                        continue

                    # ğŸ”¥ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ ì œì™¸ (ì¶”ê°€ ì•ˆì „ì¥ì¹˜)
                    if original_word[0].isupper() and len(original_word) > 1:
                        if self.verbose:
                            print(
                                f"         âš ï¸ ëŒ€ë¬¸ì ì‹œì‘ ë‹¨ì–´ ì œì™¸: '{original_word}'"
                            )
                        continue

                    # ê¸°ë³¸ í•„í„°ë§
                    if (
                        len(word) < 3
                        or not word.isalpha()
                        or token.is_stop
                        or token.pos_ in ["PUNCT", "SPACE", "SYM"]
                    ):
                        continue
                    seen_lemmas.add(lemma)

                    # ë¬¸ë§¥ ì¶”ì¶œ
                    context = get_sentence_context(
                        text_str, token.idx, token.idx + len(token.text)
                    )

                    word_info = {
                        "word": original_word,
                        "lemma": lemma,
                        "context": context,
                        "pos": get_simple_pos(token.pos_),
                        "token_info": {
                            "start": token.idx,
                            "end": token.idx + len(token.text),
                            "original": original_word,
                        },
                    }

                    # ì‚¬ìš©ì DB ë‹¨ì–´ ìš°ì„  ë¶„ë¦¬
                    if hasattr(self, "user_single_words") and (
                        lemma in self.user_single_words
                        or word in self.user_single_words
                    ):
                        user_db_candidates.append(word_info)
                        print(
                            f"         âœ… ì‚¬ìš©ì DB ë‹¨ì–´ ë°œê²¬: '{original_word}' (ì›í˜•: {lemma})"
                        )
                    else:
                        # ê°„ë‹¨ í•„í„°ë§
                        if lemma not in easy_words and (
                            not child_vocab or lemma not in child_vocab
                        ):
                            word_candidates.append(word_info)

            final_words = []

            # ì‚¬ìš©ì DB ë‹¨ì–´ë“¤ ì²˜ë¦¬
            print(f"      ğŸ‘¤ ì‚¬ìš©ì DB ë‹¨ì–´ ì²˜ë¦¬: {len(user_db_candidates)}ê°œ")
            for word_info in user_db_candidates:
                korean_meaning = self.enhanced_korean_definition(
                    word_info["lemma"], word_info["context"], is_phrase=False
                )

                word_result = {
                    "original": word_info["token_info"]["original"],
                    "base_form": word_info["lemma"],
                    "lemma": word_info["lemma"],
                    "pos": word_info["pos"],
                    "korean_meaning": korean_meaning,
                    "context": word_info["context"],
                    "difficulty_score": 8.0,
                    "difficulty_level": "user_priority",
                    "confidence": 1.0,
                    "inclusion_reason": "ì‚¬ìš©ìDBìš°ì„ í¬í•¨",
                    "user_db_match": True,
                    "match_type": "ì‚¬ìš©ìDBë‹¨ì–´",
                }
                final_words.append(word_result)

            # ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ ì²˜ë¦¬ (ê°„ë‹¨í•œ í•„í„°ë§)
            print(f"      ğŸ” ì¼ë°˜ ë‹¨ì–´ ì²˜ë¦¬: {len(word_candidates)}ê°œ")
            for word_info in word_candidates[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì²˜ë¦¬
                lemma = word_info["lemma"]

                # ê¸¸ì´ ê¸°ë°˜ ê°„ë‹¨ í•„í„°ë§
                if len(lemma) >= 5:  # 5ê¸€ì ì´ìƒë§Œ í¬í•¨
                    korean_meaning = self.enhanced_korean_definition(
                        lemma, word_info["context"], is_phrase=False
                    )

                    word_result = {
                        "original": word_info["token_info"]["original"],
                        "base_form": lemma,
                        "lemma": lemma,
                        "pos": word_info["pos"],
                        "korean_meaning": korean_meaning,
                        "context": word_info["context"],
                        "difficulty_score": 6.0,
                        "difficulty_level": "intermediate",
                        "confidence": 0.8,
                        "inclusion_reason": "ê¸¸ì´ê¸°ë°˜ì„ íƒ",
                        "user_db_match": False,
                        "match_type": "ì¼ë°˜ë‹¨ì–´",
                    }
                    final_words.append(word_result)

            user_db_count = len(
                [w for w in final_words if w.get("user_db_match", False)]
            )
            other_count = len(
                [w for w in final_words if not w.get("user_db_match", False)]
            )

            print(
                f"      ğŸ“Š ë‹¨ì–´ ì¶”ì¶œ ê²°ê³¼: ì‚¬ìš©ìDB {user_db_count}ê°œ + ê¸°íƒ€ {other_count}ê°œ = ì´ {len(final_words)}ê°œ"
            )

            return final_words

        except Exception as e:
            print(f"âŒ ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def enhanced_korean_definition(
        self, word, sentence, is_phrase=False, pos_hint=None
    ):
        """í•œê¸€ ì˜ë¯¸ ìƒì„± - ë¬¸ë§¥ ê¸°ë°˜ ê°œì„ """

        try:
            # ì§ì ‘ GPT í˜¸ì¶œ
            if not hasattr(self, "client") or not self.client:
                return f"{word}ì˜ ì˜ë¯¸"

            # ğŸ”¥ ë¬¸ë§¥ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°•í™” (ì˜ì–´)
            if sentence and len(sentence.strip()) > 10:
                prompt = f"""Analyze the exact meaning of "{word}" in this sentence.

Context: "{sentence}"

Task: Determine what "{word}" means in THIS specific usage.

Instructions:
1. Read the sentence carefully
2. Understand how "{word}" functions in this context
3. Determine the specific meaning (not all possible meanings)
4. Provide ONLY the Korean equivalent for this specific usage

Response format: Just the Korean meaning (2-4 words)

Example:
- "bank" in "I went to the bank" â†’ "ì€í–‰"
- "bank" in "river bank" â†’ "ê°•ë‘‘"
- "light" in "light meal" â†’ "ê°€ë²¼ìš´"
- "different" in "He's a different kind of person" â†’ "íŠ¹ë³„í•œ"

Korean meaning of "{word}" in this context:"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert English-Korean translator who analyzes context precisely. Provide the exact Korean meaning that fits the specific context, not just general dictionary definitions.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                max_tokens=50,
            )

            answer = response.choices[0].message.content.strip().replace('"', "")

            # ğŸ”¥ ì¶”ê°€ ì •ì œ: ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°
            answer = answer.split("\n")[0]  # ì²« ë²ˆì§¸ ì¤„ë§Œ ì‚¬ìš©
            answer = answer.replace("meaning:", "").replace("translation:", "").strip()

            # GPT í˜¸ì¶œ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            if hasattr(self, "gpt_call_count"):
                self.gpt_call_count += 1

            # í† í° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
            if hasattr(self, "gpt_token_usage") and hasattr(response, "usage"):
                usage = response.usage
                self.gpt_token_usage["prompt_tokens"] += usage.prompt_tokens
                self.gpt_token_usage["completion_tokens"] += usage.completion_tokens
                self.gpt_token_usage["total_tokens"] += usage.total_tokens

            return answer

        except Exception as e:
            print(f"   âŒ ì˜ë¯¸ ìƒì„± ì‹¤íŒ¨ ({word}): {e}")
            return word

    def add_synonyms_antonyms_to_results(self, results):
        """ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ê°€ (ê¸°ë³¸ ë²„ì „)"""

        # ğŸ” ì´ ì¡°ê±´ì—ì„œ ë§‰íˆëŠ”ì§€ í™•ì¸
        if not hasattr(self, "synonym_extractor") or not self.synonym_extractor:
            print("âš ï¸ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤")
            for result in results:
                result["ë™ì˜ì–´"] = ""
                result["ë°˜ì˜ì–´"] = ""
            return results

        print("ğŸ” ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ì¤‘...")

        try:
            # ë‹¨ì–´ ì •ë³´ ì¤€ë¹„
            word_list = []
            for result in results:
                word_info = {
                    "word": result.get("ë‹¨ì–´", ""),
                    "context": result.get("ë¬¸ë§¥", ""),
                    "pos": result.get("í’ˆì‚¬", ""),
                    "meaning": result.get("ëœ»(í•œê¸€)", ""),
                }
                word_list.append(word_info)

            # ë°°ì¹˜ ì¶”ì¶œ
            synonym_results = self.synonym_extractor.batch_extract(word_list)

            # ê²°ê³¼ì— ì¶”ê°€
            for result in results:
                word = result.get("ë‹¨ì–´", "")
                if word in synonym_results:
                    syn_data = synonym_results[word]
                    synonyms = syn_data.get("synonyms", [])[:3]  # ìµœëŒ€ 3ê°œ
                    antonyms = syn_data.get("antonyms", [])[:2]  # ìµœëŒ€ 2ê°œ

                    result["ë™ì˜ì–´"] = ", ".join(synonyms)
                    result["ë°˜ì˜ì–´"] = ", ".join(antonyms)
                else:
                    result["ë™ì˜ì–´"] = ""
                    result["ë°˜ì˜ì–´"] = ""

            return results

        except Exception as e:
            print(f"âŒ ë™ì˜ì–´/ë°˜ì˜ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ì»¬ëŸ¼ ì¶”ê°€
            for result in results:
                result["ë™ì˜ì–´"] = ""
                result["ë°˜ì˜ì–´"] = ""
            return results
